[
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html",
    "title": "C2. Why Most ML Pipelines Are Unsafe by Default",
    "section": "",
    "text": "Many modern machine learning frameworks provide components that can be assembled into leakage-safe workflows.\nYet leakage remains widespread in applied work.\nThis is not a contradiction. It follows from how pipelines are designed and how methodological correctness is treated within those designs.",
    "crumbs": [
      "Concepts",
      "C2. Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#the-paradox-of-modern-ml-tooling",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#the-paradox-of-modern-ml-tooling",
    "title": "C2. Why Most ML Pipelines Are Unsafe by Default",
    "section": "",
    "text": "Many modern machine learning frameworks provide components that can be assembled into leakage-safe workflows.\nYet leakage remains widespread in applied work.\nThis is not a contradiction. It follows from how pipelines are designed and how methodological correctness is treated within those designs.",
    "crumbs": [
      "Concepts",
      "C2. Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#flexibility-without-constraints",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#flexibility-without-constraints",
    "title": "C2. Why Most ML Pipelines Are Unsafe by Default",
    "section": "Flexibility without constraints",
    "text": "Flexibility without constraints\nContemporary ML frameworks emphasize modularity:\n\npreprocessing steps are independent components,\nmodels are interchangeable,\nresampling is optional and configurable,\nevaluation is often treated as a separable stage.\n\nThis flexibility is powerful, but it has consequences.\nIn most frameworks, nothing in the default execution path prevents users from assembling pipelines in orders that violate evaluation assumptions.\nSuch pipelines are typically syntactically valid and computationally successful, even when they are methodologically incorrect.",
    "crumbs": [
      "Concepts",
      "C2. Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#optional-correctness-is-not-correctness",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#optional-correctness-is-not-correctness",
    "title": "C2. Why Most ML Pipelines Are Unsafe by Default",
    "section": "Optional correctness is not correctness",
    "text": "Optional correctness is not correctness\nMany frameworks provide mechanisms intended to reduce leakage risk:\n\nresampling-aware preprocessing tools,\nworkflow abstractions,\nhelper functions that encourage correct usage.\n\nHowever, these mechanisms are optional.\nUsers may still:\n\ncompute transformations using the full dataset,\nreuse or redefine resampling splits inconsistently,\napply preprocessing outside the resampling loop,\ntune models using data that later serve as assessment sets.\n\nIn most cases, the software permits these configurations to execute without intervention.\nAs a result, methodological correctness depends on how components are assembled, not on guarantees provided by the execution model.",
    "crumbs": [
      "Concepts",
      "C2. Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#why-warnings-do-not-solve-the-problem",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#why-warnings-do-not-solve-the-problem",
    "title": "C2. Why Most ML Pipelines Are Unsafe by Default",
    "section": "Why warnings do not solve the problem",
    "text": "Why warnings do not solve the problem\nOne might expect software to detect leakage and issue warnings.\nIn practice, this approach is limited.\nLeakage is often:\n\nsemantically ambiguous,\ndependent on context and intent,\nindistinguishable from valid workflows at runtime.\n\nStatic analysis can detect only narrow classes of problems, and comprehensive runtime checks are difficult without constraining flexibility or expressiveness.\nMost frameworks therefore favor permissiveness and composability over strict enforcement.",
    "crumbs": [
      "Concepts",
      "C2. Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#the-expert-user-fallacy",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#the-expert-user-fallacy",
    "title": "C2. Why Most ML Pipelines Are Unsafe by Default",
    "section": "The expert-user fallacy",
    "text": "The expert-user fallacy\nA common response is to place responsibility on user expertise.\nThis implicitly assumes that:\n\nexperienced users consistently assemble pipelines correctly,\nmistakes are rare or immediately obvious,\ndiscipline scales with workflow complexity.\n\nThese assumptions do not reliably hold.\nAs workflows become more complex, for example nested resampling, grouped splits, or tuning loops, the space for subtle errors expands, even for experienced practitioners.\nExpertise can reduce risk, but it does not eliminate it.",
    "crumbs": [
      "Concepts",
      "C2. Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#consequences-for-evaluation",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#consequences-for-evaluation",
    "title": "C2. Why Most ML Pipelines Are Unsafe by Default",
    "section": "Consequences for evaluation",
    "text": "Consequences for evaluation\nWhen pipelines are unsafe by default:\n\ninvalid evaluations may appear legitimate,\nperformance estimates tend to be optimistic,\nresults are harder to reproduce,\nreviewers and readers have limited visibility into evaluation correctness.\n\nThis is not primarily a failure of individual users.\nIt reflects limitations in pipeline architecture.",
    "crumbs": [
      "Concepts",
      "C2. Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#what-a-safe-default-would-require",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#what-a-safe-default-would-require",
    "title": "C2. Why Most ML Pipelines Are Unsafe by Default",
    "section": "What a safe default would require",
    "text": "What a safe default would require\nA safe default requires more than recommendations or documentation.\nIt requires that, along the default execution path:\n\nresampling encloses all data-dependent learning steps,\npreprocessing is not learned globally,\nevaluation is not detachable from model fitting,\nunsafe configurations are restricted or redirected.\n\nIn other words, correctness must be enforced by construction within the execution model, rather than assumed.",
    "crumbs": [
      "Concepts",
      "C2. Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#summary",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#summary",
    "title": "C2. Why Most ML Pipelines Are Unsafe by Default",
    "section": "Summary",
    "text": "Summary\n\nModern ML frameworks prioritize flexibility and composability.\nMethodological correctness is usually supported but not enforced.\nInvalid pipelines can execute without warnings.\nExpertise alone does not prevent evaluation errors.\nSafer evaluation requires architectural constraints.\n\nThis motivates the core idea behind fastml: Guarded Resampling.\nNext: C3 — Guarded Resampling",
    "crumbs": [
      "Concepts",
      "C2. Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C1-what-is-data-leakage.html",
    "href": "C1-what-is-data-leakage.html",
    "title": "C1. What Is Data Leakage",
    "section": "",
    "text": "Data leakage occurs when information outside the data available at training time influences model fitting or evaluation in a way that would not be available at prediction time.\nThis definition is intentionally structural. Leakage is not about intent, carelessness, or misconduct. It is about when information is used.\nA model can be trained in good faith and still be invalidly evaluated.\nLeakage is a timing error, not a data error.\nMany descriptions frame leakage as a problem of using the test set. This framing is incomplete.\nLeakage occurs whenever a transformation, decision, or parameter is learned outside the resampling procedure, rather than within each resampling split.\nCommon examples include:\n\nscaling predictors using the full dataset prior to cross-validation,\nimputing missing values using global statistics,\nselecting features using all observations,\ntuning hyperparameters outside the resampling loop.\n\nIn each case, the mechanism is the same: information from assessment data is incorporated into training.",
    "crumbs": [
      "Concepts",
      "C1. What Is Data Leakage"
    ]
  },
  {
    "objectID": "C1-what-is-data-leakage.html#the-problem-fastml-is-built-to-address",
    "href": "C1-what-is-data-leakage.html#the-problem-fastml-is-built-to-address",
    "title": "C1. What Is Data Leakage",
    "section": "",
    "text": "Data leakage occurs when information outside the data available at training time influences model fitting or evaluation in a way that would not be available at prediction time.\nThis definition is intentionally structural. Leakage is not about intent, carelessness, or misconduct. It is about when information is used.\nA model can be trained in good faith and still be invalidly evaluated.\nLeakage is a timing error, not a data error.\nMany descriptions frame leakage as a problem of using the test set. This framing is incomplete.\nLeakage occurs whenever a transformation, decision, or parameter is learned outside the resampling procedure, rather than within each resampling split.\nCommon examples include:\n\nscaling predictors using the full dataset prior to cross-validation,\nimputing missing values using global statistics,\nselecting features using all observations,\ntuning hyperparameters outside the resampling loop.\n\nIn each case, the mechanism is the same: information from assessment data is incorporated into training.",
    "crumbs": [
      "Concepts",
      "C1. What Is Data Leakage"
    ]
  },
  {
    "objectID": "C1-what-is-data-leakage.html#why-leakage-is-hard-to-detect",
    "href": "C1-what-is-data-leakage.html#why-leakage-is-hard-to-detect",
    "title": "C1. What Is Data Leakage",
    "section": "Why Leakage Is Hard to Detect",
    "text": "Why Leakage Is Hard to Detect\nLeakage rarely produces errors or warnings.\nPipelines that contain leakage often:\n\nrun without failure,\nproduce stable estimates,\nyield optimistic performance values.\n\nAs a result, invalid pipelines may appear convincing.\nEven experienced practitioners can introduce leakage when workflows are assembled manually from modular components.",
    "crumbs": [
      "Concepts",
      "C1. What Is Data Leakage"
    ]
  },
  {
    "objectID": "C1-what-is-data-leakage.html#a-minimal-example-conceptual",
    "href": "C1-what-is-data-leakage.html#a-minimal-example-conceptual",
    "title": "C1. What Is Data Leakage",
    "section": "A Minimal Example (Conceptual)",
    "text": "A Minimal Example (Conceptual)\nConsider a dataset ( D ) evaluated using 5-fold cross-validation.\nIf a preprocessing step is estimated once using ( D ) and then applied within each fold, the model trained in Fold 1 has already been influenced by data from Folds 2–5.\nThis violates the independence assumption underlying cross-validation.\nThe resulting performance estimate is biased upward by construction.\nNo explicit misuse of a held-out test set is required.",
    "crumbs": [
      "Concepts",
      "C1. What Is Data Leakage"
    ]
  },
  {
    "objectID": "C1-what-is-data-leakage.html#why-best-practices-are-not-enough",
    "href": "C1-what-is-data-leakage.html#why-best-practices-are-not-enough",
    "title": "C1. What Is Data Leakage",
    "section": "Why “Best Practices” Are Not Enough",
    "text": "Why “Best Practices” Are Not Enough\nModern machine learning frameworks provide tools that can help avoid leakage. However, these safeguards are typically optional.\nCorrect evaluation therefore depends on users:\n\nidentifying which steps must be resampling-aware,\nassembling components in the correct order,\navoiding convenience-driven shortcuts.\n\nIncorrect pipelines are not generally prevented from executing.\nAs a result, methodological validity often depends on user discipline rather than on structural guarantees.",
    "crumbs": [
      "Concepts",
      "C1. What Is Data Leakage"
    ]
  },
  {
    "objectID": "C1-what-is-data-leakage.html#what-follows-from-this",
    "href": "C1-what-is-data-leakage.html#what-follows-from-this",
    "title": "C1. What Is Data Leakage",
    "section": "What Follows from This",
    "text": "What Follows from This\nIf leakage is:\n\neasy to introduce,\ndifficult to detect,\nand rarely signaled by software,\n\nthen preventing it cannot rely on guidelines alone.\nIt requires enforcement at the level of workflow design.\nThis motivates Guarded Resampling, introduced in the next concept.",
    "crumbs": [
      "Concepts",
      "C1. What Is Data Leakage"
    ]
  },
  {
    "objectID": "C1-what-is-data-leakage.html#summary",
    "href": "C1-what-is-data-leakage.html#summary",
    "title": "C1. What Is Data Leakage",
    "section": "Summary",
    "text": "Summary\n\nData leakage is a structural timing error.\nIt does not require misuse of a held-out test set.\nIt can produce stable but invalid results.\nOptional safeguards are insufficient.\nPreventing leakage requires architectural constraints, not user vigilance.\n\nNext: C2 — Why Most ML Pipelines Are Unsafe by Default",
    "crumbs": [
      "Concepts",
      "C1. What Is Data Leakage"
    ]
  },
  {
    "objectID": "01-basic-classification.html",
    "href": "01-basic-classification.html",
    "title": "01. Basic Classification",
    "section": "",
    "text": "This tutorial builds on the following conceptual material:\n\nC1. What is Data Leakage\nC2. Why Most ML Pipelines Are Unsafe by Default\nC3. Guarded Resampling\nC4. What fastml Deliberately Does Not Allow\n\nReaders are expected to be familiar with these concepts before proceeding.\nThe workflow demonstrated here uses a deliberately constrained interface. These constraints are intentional and reflect the design philosophy of fastml: to reduce common sources of evaluation error by limiting user-facing degrees of freedom along the default execution path.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#before-you-start",
    "href": "01-basic-classification.html#before-you-start",
    "title": "01. Basic Classification",
    "section": "",
    "text": "This tutorial builds on the following conceptual material:\n\nC1. What is Data Leakage\nC2. Why Most ML Pipelines Are Unsafe by Default\nC3. Guarded Resampling\nC4. What fastml Deliberately Does Not Allow\n\nReaders are expected to be familiar with these concepts before proceeding.\nThe workflow demonstrated here uses a deliberately constrained interface. These constraints are intentional and reflect the design philosophy of fastml: to reduce common sources of evaluation error by limiting user-facing degrees of freedom along the default execution path.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#the-problem-we-are-solving",
    "href": "01-basic-classification.html#the-problem-we-are-solving",
    "title": "01. Basic Classification",
    "section": "The problem we are solving",
    "text": "The problem we are solving\nThe goal of this tutorial is to estimate the out-of-sample performance of a binary classifier.\nThe focus is not on maximizing predictive accuracy, tuning hyperparameters, or examining model internals. Instead, the objective is narrowly defined:\nWhat level of predictive performance can reasonably be expected on new, unseen data, given a fixed modeling specification?\nIn this setting, model fitting itself is straightforward. The primary challenge lies in performance evaluation.\nSpecifically, the difficulty is to obtain an estimate that is not biased by information leakage or other forms of contamination arising from reuse of the data during training and evaluation.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#why-this-is-harder-than-it-sounds",
    "href": "01-basic-classification.html#why-this-is-harder-than-it-sounds",
    "title": "01. Basic Classification",
    "section": "Why this is harder than it sounds",
    "text": "Why this is harder than it sounds\nFrom C1 and C2, recall the following points:\n\nLeakage does not require obvious mistakes.\nPipelines that appear valid can still yield biased performance estimates.\nMost machine learning frameworks allow such pipelines to run without warnings.\n\nFrom C3, recall:\n\nCorrect evaluation requires resampling to enclose preprocessing and model fitting.\nThis enclosure must be structural, not procedural.\n\nFrom C4, recall:\n\nfastml enforces correctness by removing degrees of freedom rather than relying on user discipline.\n\nThis tutorial demonstrates what that enforcement looks like in practice.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#the-data",
    "href": "01-basic-classification.html#the-data",
    "title": "01. Basic Classification",
    "section": "The data",
    "text": "The data\nThis tutorial uses a simple binary classification dataset from the modeldata package.\nThe dataset is characterized by:\n\na binary outcome variable (Class),\na small number of continuous predictors (A and B),\nthe absence of missing values.\n\nThis choice is intentional. The dataset is deliberately low-dimensional and clean, so that attention can remain on the evaluation procedure rather than on feature engineering, preprocessing decisions, or data-quality issues.\n\nlibrary(modeldata)\ndata(two_class_dat)\nhead(two_class_dat)\n\n         A        B  Class\n1 2.069730 1.631647 Class1\n2 2.016415 1.036629 Class1\n3 1.688555 1.366610 Class2\n4 3.434538 1.979776 Class2\n5 2.884596 1.975891 Class1\n6 3.313589 2.405875 Class2\n\n\nThe purpose of this example is to illustrate evaluation mechanics rather than data preprocessing challenges. Although the dataset itself is simple, the evaluation principles demonstrated here extend to more complex datasets and other supported tasks in fastml, subject to their respective assumptions and constraints.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#what-you-do-not-need-to-do-in-fastml",
    "href": "01-basic-classification.html#what-you-do-not-need-to-do-in-fastml",
    "title": "01. Basic Classification",
    "section": "What you do not need to do in fastml",
    "text": "What you do not need to do in fastml\nBefore showing the workflow, it is important to be explicit.\nIn fastml, users are not required to:\n\nmanually assemble train–test splits,\nexplicitly construct preprocessing recipes,\napply scaling or imputation outside the resampling loop,\ncompose workflows from loosely coupled components,\ncontrol when preprocessing is trained relative to resampling,\ndirectly manipulate resampling objects during model execution.\n\nThese steps are common entry points for data leakage.\nBy default, fastml executes preprocessing, model fitting, and evaluation within a single, resampling-aware structure. While advanced users may override specific components, the standard execution path is designed to preserve training–assessment isolation without relying on user discipline.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#declaring-intent",
    "href": "01-basic-classification.html#declaring-intent",
    "title": "01. Basic Classification",
    "section": "Declaring intent",
    "text": "Declaring intent\nIn fastml, the user specifies what is to be evaluated rather than manually assembling the individual components of a modeling pipeline.\nAt a minimum, this specification includes:\n\nthe dataset,\nthe outcome variable,\nthe set of algorithms to be evaluated,\nthe intended resampling strategy.\n\n\nlibrary(fastml)\n\nfit &lt;- fastml(\n  data       = two_class_dat,\n  label      = \"Class\",\n  algorithms = c(\"rand_forest\", \"xgboost\"),\n  resampling = \"cv\",\n  folds      = 5,\n)\n\nThis call defines the full evaluation setup under the default execution path. Model fitting, preprocessing, and performance estimation are carried out internally according to the declared intent and the constraints imposed by fastml.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#what-happens-internally",
    "href": "01-basic-classification.html#what-happens-internally",
    "title": "01. Basic Classification",
    "section": "What happens internally",
    "text": "What happens internally\nThe behavior described below reflects the default execution path in fastml and is not exposed for routine user configuration.\nFor each resampling split:\n\ntraining data are defined according to the resampling specification,\nany preprocessing steps are estimated using the training data only,\nmodels are fitted on the resulting training set,\npredictions are generated for the corresponding assessment set,\nperformance metrics are computed exclusively on assessment data.\n\nAs an additional safety measure, fastml performs internal checks on the resampling structure. If a resample is detected in which the training set coincides with the full dataset, execution is halted and the run is flagged as unsafe.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#why-this-differs-from-typical-workflows",
    "href": "01-basic-classification.html#why-this-differs-from-typical-workflows",
    "title": "01. Basic Classification",
    "section": "Why this differs from typical workflows",
    "text": "Why this differs from typical workflows\nIn many machine learning frameworks:\n\npipelines that violate evaluation assumptions can be constructed,\ncorrect evaluation depends largely on user discipline and correct assembly of components,\nviolations such as preprocessing leakage may execute without explicit warnings.\n\nIn fastml:\n\ncommon classes of incorrect evaluation pipelines are restricted along the default execution path,\nmethodological correctness is less dependent on user assembly decisions,\nkey evaluation invariants are checked and enforced during execution.\n\nThis reflects a deliberate design trade-off: reducing flexibility in pipeline construction in order to lower the risk of undetected evaluation errors.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#inspecting-results",
    "href": "01-basic-classification.html#inspecting-results",
    "title": "01. Basic Classification",
    "section": "Inspecting results",
    "text": "Inspecting results\nOnce execution is complete, performance estimates can be accessed directly from the fitted object.\n\nfit$performance$rand_forest$ranger\n\n# A tibble: 7 × 6\n  .metric   .estimator .estimate .lower .upper .n_boot\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 accuracy  binary         0.818  0.761  0.878     500\n2 kap       binary         0.629  0.511  0.748     500\n3 sens      binary         0.852  0.777  0.922     500\n4 spec      binary         0.775  0.685  0.875     500\n5 precision binary         0.824  0.747  0.902     500\n6 f_meas    binary         0.838  0.777  0.894     500\n7 roc_auc   binary         0.874  0.822  0.928     500\n\nfit$performance$xgboost$xgboost\n\n# A tibble: 7 × 6\n  .metric   .estimator .estimate .lower .upper .n_boot\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 accuracy  binary         0.811  0.748  0.874     500\n2 kap       binary         0.617  0.489  0.742     500\n3 sens      binary         0.841  0.758  0.916     500\n4 spec      binary         0.775  0.683  0.869     500\n5 precision binary         0.822  0.746  0.898     500\n6 f_meas    binary         0.831  0.766  0.891     500\n7 roc_auc   binary         0.864  0.806  0.920     500\n\n\nThe reported metrics summarize performance estimates obtained under 5-fold cross-validation for each model.\nFor both the random forest and xgboost models, these estimates are computed on assessment data that are held out from model fitting within each resampling split. They therefore differ from training-set performance and are not derived from post-hoc adjustments or recalibration.\nIn this example, the random forest model attains slightly higher average performance across most metrics, including accuracy, Cohen’s kappa, F1 score, and ROC AUC. However, the differences relative to xgboost are modest, and both models display a similar balance between sensitivity and specificity. These results indicate comparable overall performance with small differences in error structure rather than a clear dominance of one model over the other.\nAll reported values arise directly from the resampling-based evaluation procedure used by fastml, in which preprocessing, model fitting, and performance estimation are executed within a single, resampling-aware structure. The metrics therefore represent cross-validated performance summaries under the declared evaluation setup, subject to the usual variability and limitations inherent to resampling-based estimates.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#fold-level-variability",
    "href": "01-basic-classification.html#fold-level-variability",
    "title": "01. Basic Classification",
    "section": "Fold-Level variability",
    "text": "Fold-Level variability\n\nfit$resampling_results$`rand_forest (ranger)`$folds\n\n# A tibble: 35 × 4\n   fold  .metric   .estimator .estimate\n   &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n 1 1     accuracy  binary         0.850\n 2 1     kap       binary         0.699\n 3 1     sens      binary         0.843\n 4 1     spec      binary         0.860\n 5 1     precision binary         0.881\n 6 1     f_meas    binary         0.861\n 7 1     roc_auc   binary         0.874\n 8 2     accuracy  binary         0.890\n 9 2     kap       binary         0.779\n10 2     sens      binary         0.871\n# ℹ 25 more rows\n\nfit$resampling_results$`xgboost (xgboost)`$folds\n\n# A tibble: 35 × 4\n   fold  .metric   .estimator .estimate\n   &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n 1 1     accuracy  binary         0.843\n 2 1     kap       binary         0.685\n 3 1     sens      binary         0.814\n 4 1     spec      binary         0.877\n 5 1     precision binary         0.891\n 6 1     f_meas    binary         0.851\n 7 1     roc_auc   binary         0.875\n 8 2     accuracy  binary         0.866\n 9 2     kap       binary         0.730\n10 2     sens      binary         0.871\n# ℹ 25 more rows\n\n\nPerformance estimates vary across resampling folds, across metrics, and across models.\nIn this example, both the random forest and xgboost models exhibit noticeable but moderate fold-to-fold variability. Accuracy, sensitivity, specificity, and agreement-based measures such as Cohen’s kappa fluctuate across folds for both models, reflecting differences in class composition and difficulty across resampling splits.\nAcross folds, the two models show broadly comparable behavior. The random forest model attains slightly higher values in some folds, while the xgboost model matches or closely tracks its performance in others. For both models, sensitivity and specificity remain relatively balanced across folds, and no systematic metric asymmetry or degenerate behavior is observed. Variation in kappa and accuracy remains within a range consistent with expected resampling variability rather than indicating instability.\nThis pattern illustrates that, even when aggregated summaries suggest similar overall performance, fold-level inspection remains necessary to assess stability, class-wise trade-offs, and the influence of individual resampling splits. Such variability is an inherent feature of resampling-based evaluation and cannot be fully characterized by a single averaged estimate.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#model-comparison",
    "href": "01-basic-classification.html#model-comparison",
    "title": "01. Basic Classification",
    "section": "Model comparison",
    "text": "Model comparison\n\nsummary(fit)\n\n\n===== fastml Model Summary =====\nTask: classification \nNumber of Models Trained: 2 \nBest Model(s): rand_forest (ranger) (accuracy: 0.8176101) \n\nPerformance Metrics (Sorted by accuracy):\n\n---------------------------------------------------------------------------------------------- \nModel         Engine   Accuracy  F1 Score  Kappa  Precision  Sensitivity  Specificity  ROC AUC \n---------------------------------------------------------------------------------------------- \nrand_forest*  ranger   0.818     0.838     0.629  0.824      0.852        0.775        0.874   \nxgboost       xgboost  0.811     0.831     0.617  0.822      0.841        0.775        0.864   \n---------------------------------------------------------------------------------------------- \n(*Best model)\n\nBest Model hyperparameters:\n\nModel: rand_forest (ranger) \n  mtry: 1\n  trees: 500\n  min_n: 10\n\n\n===========================\nConfusion Matrices by Model\n===========================\n\nModel: rand_forest (ranger) \n---------------------------\n          Truth\nPrediction Class1 Class2\n    Class1     75     16\n    Class2     13     55\n\n\nThe summary output compares models evaluated under an identical resampling specification.\nIn this example, the random forest model attains slightly higher average performance across all reported metrics, including accuracy, F1 score, Cohen’s kappa, sensitivity, specificity, and ROC AUC. The xgboost model shows closely comparable performance, with modestly lower values across the same metrics. The differences between models are small and reflect incremental variations in error rates rather than qualitatively distinct error profiles.\nBecause all models are evaluated using the same resampling splits, observed performance differences can be attributed to the modeling approaches rather than to variation in data partitioning. This consistency is not merely a convenience; it is a methodological requirement for meaningful model comparison under resampling-based evaluation.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#what-is-guaranteed-here",
    "href": "01-basic-classification.html#what-is-guaranteed-here",
    "title": "01. Basic Classification",
    "section": "What is guaranteed here",
    "text": "What is guaranteed here\nSubject to the constraints accepted along the default execution path, fastml provides the following assurances:\n\npreprocessing steps are estimated separately within each resampling split, preventing information flow across folds,\nmodel training and evaluation are carried out on disjoint data subsets within each split,\nall algorithms are evaluated using an identical resampling structure,\nreported performance metrics correspond to resampling-based estimates of out-of-sample performance.\n\nThese properties arise from the architectural design of fastml rather than from user-enforced conventions. They reflect enforced evaluation invariants under standard usage, not informal best-practice recommendations.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#what-fastml-cannot-guarantee",
    "href": "01-basic-classification.html#what-fastml-cannot-guarantee",
    "title": "01. Basic Classification",
    "section": "What fastml cannot guarantee",
    "text": "What fastml cannot guarantee\nAs emphasized in the accompanying manuscript, fastml does not and cannot guarantee:\n\nthat outcome variables are correctly defined or scientifically meaningful,\nthat the raw features are free from prior leakage, measurement artifacts, or target contamination,\nthat the learning task itself addresses a scientifically relevant question,\nthat the chosen performance metrics are appropriate for the scientific or clinical context.\n\nMethodological safeguards can reduce certain classes of technical error, but they cannot substitute for domain expertise, sound study design, or scientific judgment.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#summary",
    "href": "01-basic-classification.html#summary",
    "title": "01. Basic Classification",
    "section": "Summary",
    "text": "Summary\nThis tutorial did not focus on constructing highly flexible modeling pipelines.\nInstead, it demonstrated how fastml limits common pathways to invalid evaluation by constraining how models are trained, evaluated, and compared along the default execution path.\nThe distinguishing characteristic of fastml is therefore not automation for its own sake, but the enforcement of evaluation invariants intended to reduce methodological errors in performance estimation.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#what-comes-next",
    "href": "01-basic-classification.html#what-comes-next",
    "title": "01. Basic Classification",
    "section": "What comes next",
    "text": "What comes next\n02. Multiple Models and Fair Comparison\nWhy comparing models is statistically invalid without shared resampling.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html",
    "href": "C3-guarded-resampling.html",
    "title": "C3. Guarded Resampling",
    "section": "",
    "text": "C1 established that data leakage is a structural timing error.\nC2 showed that many machine learning pipelines permit such errors by default.\nA direct consequence is that methodological correctness cannot rely on user discipline alone.\nGuarded Resampling is an architectural response to this limitation.",
    "crumbs": [
      "Concepts",
      "C3. Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#from-advice-to-enforcement",
    "href": "C3-guarded-resampling.html#from-advice-to-enforcement",
    "title": "C3. Guarded Resampling",
    "section": "",
    "text": "C1 established that data leakage is a structural timing error.\nC2 showed that many machine learning pipelines permit such errors by default.\nA direct consequence is that methodological correctness cannot rely on user discipline alone.\nGuarded Resampling is an architectural response to this limitation.",
    "crumbs": [
      "Concepts",
      "C3. Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#what-guarded-means",
    "href": "C3-guarded-resampling.html#what-guarded-means",
    "title": "C3. Guarded Resampling",
    "section": "What “Guarded” means",
    "text": "What “Guarded” means\nIn guarded resampling, resampling is not treated as a downstream step in a workflow. Instead, it provides the enclosing structure for all operations that learn parameters from the data.\nUnder this design, any operation whose results depend on estimated quantities is executed separately within each resampling split, including:\n\npreprocessing steps that estimate statistics from the data,\nmodel fitting,\nhyperparameter selection,\nperformance metric computation.\n\nOperations that would require access to information outside the analysis set, or that would induce backward information flow from assessment data, are restricted along the default execution path.\nThis is not a usage convention. It is a structural constraint.",
    "crumbs": [
      "Concepts",
      "C3. Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#resampling-as-the-primary-object",
    "href": "C3-guarded-resampling.html#resampling-as-the-primary-object",
    "title": "C3. Guarded Resampling",
    "section": "Resampling as the primary object",
    "text": "Resampling as the primary object\nIn many applied machine learning workflows, resampling is effectively treated as an external evaluation step, rather than as a structural component of the learning procedure. A common pattern is:\n\nprepare data,\nfit a model,\nevaluate performance using resampling.\n\nAlthough widely used, this organization obscures the fact that valid performance estimation requires strict separation between training and assessment data at all stages of model construction.\nGuarded resampling reverses this relationship. Instead of attaching resampling post hoc, the workflow is defined by the resampling scheme itself:\n\ndefine a resampling plan,\nfor each split:\n\nestimate preprocessing steps,\nfit the model,\ngenerate predictions,\n\naggregate results across splits.\n\nUnder this formulation, resampling defines the experimental structure, and all learning steps operate conditionally within it.",
    "crumbs": [
      "Concepts",
      "C3. Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#why-enclosure-matters",
    "href": "C3-guarded-resampling.html#why-enclosure-matters",
    "title": "C3. Guarded Resampling",
    "section": "Why enclosure matters",
    "text": "Why enclosure matters\nBy enclosing learning within resampling splits, guarded resampling ensures that:\n\nassessment data are isolated from training within each split,\npreprocessing parameters are estimated separately for each training set,\nmodel comparisons are conducted using identical resampling splits,\nperformance estimates correspond to resampling-based out-of-sample evaluation.\n\nThese properties do not depend on remembering rules or following best practices.\nThey arise from how the workflow is constructed.",
    "crumbs": [
      "Concepts",
      "C3. Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#guards-are-not-warnings",
    "href": "C3-guarded-resampling.html#guards-are-not-warnings",
    "title": "C3. Guarded Resampling",
    "section": "Guards are not warnings",
    "text": "Guards are not warnings\nGuarded resampling does not attempt to identify leakage after it has occurred.\nInstead, it restricts the construction of workflows in which leakage-prone configurations would arise along the standard execution path.\nThis distinction is important:\n\nwarnings are advisory and can be ignored,\nstructural constraints limit what can be expressed.\n\nWorkflows that violate the principles of guarded resampling are therefore not part of the default design space.",
    "crumbs": [
      "Concepts",
      "C3. Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#what-this-enables",
    "href": "C3-guarded-resampling.html#what-this-enables",
    "title": "C3. Guarded Resampling",
    "section": "What this enables",
    "text": "What this enables\nBecause resampling is treated as an enclosing structure:\n\npreprocessing is resampling-aware by default,\nresampling splits are applied consistently across models,\nevaluation remains coupled to model fitting,\nreported metrics reflect resampling-based estimates.\n\nThese properties hold independently of model class, algorithmic complexity, or user expertise, subject to the assumptions of the resampling procedure.\nMethodological correctness becomes a property of system design rather than of individual user choices.",
    "crumbs": [
      "Concepts",
      "C3. Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#what-this-does-not-do",
    "href": "C3-guarded-resampling.html#what-this-does-not-do",
    "title": "C3. Guarded Resampling",
    "section": "What this does not do",
    "text": "What this does not do\nGuarded resampling does not:\n\nguarantee strong predictive performance,\nidentify a statistically optimal model,\nreplace careful study design or domain expertise,\neliminate all possible sources of bias.\n\nIt addresses a specific and well-defined failure mode: invalid evaluation arising from data leakage.",
    "crumbs": [
      "Concepts",
      "C3. Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#why-this-is-restrictive-by-design",
    "href": "C3-guarded-resampling.html#why-this-is-restrictive-by-design",
    "title": "C3. Guarded Resampling",
    "section": "Why this is restrictive by design",
    "text": "Why this is restrictive by design\nEnforcing structural guards necessarily reduces flexibility.\nCertain actions that are technically possible in more permissive frameworks are intentionally restricted along the default execution path.\nThis is not a limitation to be worked around.\nIt is a deliberate design choice.\nThe consequences of this choice are addressed in the next concept.",
    "crumbs": [
      "Concepts",
      "C3. Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#summary",
    "href": "C3-guarded-resampling.html#summary",
    "title": "C3. Guarded Resampling",
    "section": "Summary",
    "text": "Summary\n\nGuarded resampling treats resampling as the enclosing structure.\nData-dependent learning occurs within resampling splits.\nLeakage-prone configurations are restricted by design.\nEnforcement replaces advisory guidance.\n\nThis is the core idea behind fastml.\nNext: C4 — What fastml Deliberately Does Not Allow",
    "crumbs": [
      "Concepts",
      "C3. Guarded Resampling"
    ]
  },
  {
    "objectID": "03-metrics-variability-and-uncertainty.html",
    "href": "03-metrics-variability-and-uncertainty.html",
    "title": "03. Metrics, Variability, and Uncertainty",
    "section": "",
    "text": "Machine learning results are often summarized by a single number.\nThis practice is convenient, but it is rarely sufficient.\nPerformance metrics are random variables, not fixed properties of a model.\nThey depend on how data are split, how models are trained, and how evaluation is conducted.\nAs shown in Tutorials 01–02, fastml enforces valid evaluation.\nThis tutorial focuses on how to interpret the resulting metrics responsibly.",
    "crumbs": [
      "Tutorials",
      "03. Metrics, Variability, and Uncertainty"
    ]
  },
  {
    "objectID": "03-metrics-variability-and-uncertainty.html#motivation",
    "href": "03-metrics-variability-and-uncertainty.html#motivation",
    "title": "03. Metrics, Variability, and Uncertainty",
    "section": "",
    "text": "Machine learning results are often summarized by a single number.\nThis practice is convenient, but it is rarely sufficient.\nPerformance metrics are random variables, not fixed properties of a model.\nThey depend on how data are split, how models are trained, and how evaluation is conducted.\nAs shown in Tutorials 01–02, fastml enforces valid evaluation.\nThis tutorial focuses on how to interpret the resulting metrics responsibly.",
    "crumbs": [
      "Tutorials",
      "03. Metrics, Variability, and Uncertainty"
    ]
  },
  {
    "objectID": "03-metrics-variability-and-uncertainty.html#recreating-the-analysis",
    "href": "03-metrics-variability-and-uncertainty.html#recreating-the-analysis",
    "title": "03. Metrics, Variability, and Uncertainty",
    "section": "Recreating the analysis",
    "text": "Recreating the analysis\nEach tutorial in this series is designed to be fully self-contained.\nWe therefore recreate the analysis from Tutorials 01–02 before examining metric behavior.\n\nlibrary(fastml)\nlibrary(mlbench)\nlibrary(dplyr)\n\ndata(BreastCancer)\n\nbreastCancer &lt;- BreastCancer %&gt;%\n  select(-Id) %&gt;%\n  filter(!is.na(Class)) %&gt;%\n  na.omit() %&gt;%\n  mutate(Class = factor(Class, levels = c(\"benign\", \"malignant\")))\n\nfit &lt;- fastml(\n  data       = breastCancer,\n  label      = \"Class\",\n  algorithms = c(\"rand_forest\", \"xgboost\"),\n)",
    "crumbs": [
      "Tutorials",
      "03. Metrics, Variability, and Uncertainty"
    ]
  },
  {
    "objectID": "03-metrics-variability-and-uncertainty.html#metrics-are-estimates-not-truths",
    "href": "03-metrics-variability-and-uncertainty.html#metrics-are-estimates-not-truths",
    "title": "03. Metrics, Variability, and Uncertainty",
    "section": "Metrics Are Estimates, Not Truths",
    "text": "Metrics Are Estimates, Not Truths\nA performance metric such as accuracy or ROC AUC is an estimate of out-of-sample performance rather than a fixed quantity.\nIts value depends on:\n\nthe resampling scheme,\nthe specific data splits,\nrandom elements in model fitting.\n\nReporting only a single point estimate obscures this variability.",
    "crumbs": [
      "Tutorials",
      "03. Metrics, Variability, and Uncertainty"
    ]
  },
  {
    "objectID": "03-metrics-variability-and-uncertainty.html#aggregated-metrics",
    "href": "03-metrics-variability-and-uncertainty.html#aggregated-metrics",
    "title": "03. Metrics, Variability, and Uncertainty",
    "section": "Aggregated Metrics",
    "text": "Aggregated Metrics\nfastml reports metrics aggregated across resampling folds.\n\nfit$resampling_results$`rand_forest (ranger)`$aggregated\n\n# A tibble: 7 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.962\n2 f_meas    binary         0.970\n3 kap       binary         0.916\n4 precision binary         0.975\n5 roc_auc   binary         0.991\n6 sens      binary         0.966\n7 spec      binary         0.953\n\n\nThese values summarize performance under guarded cross-validation.\nThey answer the question:\nWhat is the average performance of this model under this evaluation design?\nThey do not answer:\nHow stable is this performance?",
    "crumbs": [
      "Tutorials",
      "03. Metrics, Variability, and Uncertainty"
    ]
  },
  {
    "objectID": "03-metrics-variability-and-uncertainty.html#fold-level-variability",
    "href": "03-metrics-variability-and-uncertainty.html#fold-level-variability",
    "title": "03. Metrics, Variability, and Uncertainty",
    "section": "Fold-Level Variability",
    "text": "Fold-Level Variability\nTo assess performance stability, fold-level results must be examined.\n\nfit$resampling_results$`rand_forest (ranger)`$folds\n\n# A tibble: 70 × 4\n   fold  .metric   .estimator .estimate\n   &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n 1 1     accuracy  binary         0.929\n 2 1     kap       binary         0.844\n 3 1     sens      binary         0.944\n 4 1     spec      binary         0.9  \n 5 1     precision binary         0.944\n 6 1     f_meas    binary         0.944\n 7 1     roc_auc   binary         0.968\n 8 2     accuracy  binary         0.964\n 9 2     kap       binary         0.922\n10 2     sens      binary         0.944\n# ℹ 60 more rows\n\n\nFold-level metrics reveal:\n\nsensitivity to data partitioning,\nvariability across assessment sets,\noverlap between competing models.\n\nLarge variability indicates that small differences in average performance are unlikely to be meaningful.",
    "crumbs": [
      "Tutorials",
      "03. Metrics, Variability, and Uncertainty"
    ]
  },
  {
    "objectID": "03-metrics-variability-and-uncertainty.html#variability-and-model-comparison",
    "href": "03-metrics-variability-and-uncertainty.html#variability-and-model-comparison",
    "title": "03. Metrics, Variability, and Uncertainty",
    "section": "Variability and Model Comparison",
    "text": "Variability and Model Comparison\nIn Tutorial 02, models were compared using shared resampling.\nEven under fair comparison, it is common to observe:\n\noverlapping fold-level performance,\ndifferent rankings across folds,\ninstability in “best model” selection.\n\nThis is not a flaw.\nIt reflects finite-sample uncertainty.",
    "crumbs": [
      "Tutorials",
      "03. Metrics, Variability, and Uncertainty"
    ]
  },
  {
    "objectID": "03-metrics-variability-and-uncertainty.html#beyond-point-estimates",
    "href": "03-metrics-variability-and-uncertainty.html#beyond-point-estimates",
    "title": "03. Metrics, Variability, and Uncertainty",
    "section": "Beyond Point Estimates",
    "text": "Beyond Point Estimates\nPoint estimates encourage overinterpretation.\nTwo models with accuracies of 0.82 and 0.84 may appear different, but without information on variability, this difference is not interpretable.\nFor this reason, fastml supports uncertainty-aware performance summaries.",
    "crumbs": [
      "Tutorials",
      "03. Metrics, Variability, and Uncertainty"
    ]
  },
  {
    "objectID": "03-metrics-variability-and-uncertainty.html#bootstrap-based-metric-uncertainty",
    "href": "03-metrics-variability-and-uncertainty.html#bootstrap-based-metric-uncertainty",
    "title": "03. Metrics, Variability, and Uncertainty",
    "section": "Bootstrap-Based Metric Uncertainty",
    "text": "Bootstrap-Based Metric Uncertainty\nWhen enabled, fastml computes bootstrap distributions of evaluation metrics.\n\nfit$performance$rand_forest\n\n$ranger\n# A tibble: 7 × 6\n  .metric   .estimator .estimate .lower .upper .n_boot\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 accuracy  binary         0.978  0.949      1     500\n2 kap       binary         0.953  0.893      1     500\n3 sens      binary         0.966  0.920      1     500\n4 spec      binary         1      1          1     500\n5 precision binary         1      1          1     500\n6 f_meas    binary         0.983  0.958      1     500\n7 roc_auc   binary         0.996  0.990      1     500\n\nfit$performance$xgboost\n\n$xgboost\n# A tibble: 7 × 6\n  .metric   .estimator .estimate .lower .upper .n_boot\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 accuracy  binary         0.978  0.949      1     500\n2 kap       binary         0.952  0.888      1     500\n3 sens      binary         0.978  0.942      1     500\n4 spec      binary         0.979  0.929      1     500\n5 precision binary         0.989  0.963      1     500\n6 f_meas    binary         0.983  0.959      1     500\n7 roc_auc   binary         0.997  0.992      1     500\n\n\nBootstrapping provides:\n\nempirical uncertainty estimates,\ninsight into metric dispersion,\na basis for interval-based reporting.\n\nThese intervals describe evaluation uncertainty rather than population-level inference.",
    "crumbs": [
      "Tutorials",
      "03. Metrics, Variability, and Uncertainty"
    ]
  },
  {
    "objectID": "03-metrics-variability-and-uncertainty.html#what-fastml-does-not-claim",
    "href": "03-metrics-variability-and-uncertainty.html#what-fastml-does-not-claim",
    "title": "03. Metrics, Variability, and Uncertainty",
    "section": "What fastml Does Not Claim",
    "text": "What fastml Does Not Claim\nfastml does not:\n\nperform hypothesis testing between models,\nassign statistical significance to metric differences,\nprovide p-values for model superiority.\n\nSuch procedures require additional assumptions and lie outside the scope of guarded resampling.",
    "crumbs": [
      "Tutorials",
      "03. Metrics, Variability, and Uncertainty"
    ]
  },
  {
    "objectID": "03-metrics-variability-and-uncertainty.html#responsible-reporting",
    "href": "03-metrics-variability-and-uncertainty.html#responsible-reporting",
    "title": "03. Metrics, Variability, and Uncertainty",
    "section": "Responsible Reporting",
    "text": "Responsible Reporting\nA defensible performance report should include:\n\nthe primary evaluation metric,\nthe resampling scheme,\naggregated performance estimates,\nfold-level variability or uncertainty intervals.\n\nSingle-number summaries are insufficient.",
    "crumbs": [
      "Tutorials",
      "03. Metrics, Variability, and Uncertainty"
    ]
  },
  {
    "objectID": "03-metrics-variability-and-uncertainty.html#summary",
    "href": "03-metrics-variability-and-uncertainty.html#summary",
    "title": "03. Metrics, Variability, and Uncertainty",
    "section": "Summary",
    "text": "Summary\nMetrics are random variables.\nPoint estimates hide variability.\nFold-level results reveal stability and overlap.\nBootstrap methods provide uncertainty estimates.\nfastml emphasizes defensible interpretation over ranking.",
    "crumbs": [
      "Tutorials",
      "03. Metrics, Variability, and Uncertainty"
    ]
  },
  {
    "objectID": "03-metrics-variability-and-uncertainty.html#what-comes-next",
    "href": "03-metrics-variability-and-uncertainty.html#what-comes-next",
    "title": "03. Metrics, Variability, and Uncertainty",
    "section": "What comes next",
    "text": "What comes next\n04. Missing Data and Guarded Imputation",
    "crumbs": [
      "Tutorials",
      "03. Metrics, Variability, and Uncertainty"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fastml Tutorials",
    "section": "",
    "text": "This site provides a structured introduction to fastml, an R package for training, evaluating, and comparing machine learning models under architecturally constrained, leakage-aware resampling.\nThe emphasis is not on developing new modeling techniques or maximizing predictive performance. Instead, the focus is on methodologically sound performance evaluation under clearly defined assumptions.",
    "crumbs": [
      "fastml Tutorials"
    ]
  },
  {
    "objectID": "index.html#what-this-site-is-about",
    "href": "index.html#what-this-site-is-about",
    "title": "fastml Tutorials",
    "section": "",
    "text": "This site provides a structured introduction to fastml, an R package for training, evaluating, and comparing machine learning models under architecturally constrained, leakage-aware resampling.\nThe emphasis is not on developing new modeling techniques or maximizing predictive performance. Instead, the focus is on methodologically sound performance evaluation under clearly defined assumptions.",
    "crumbs": [
      "fastml Tutorials"
    ]
  },
  {
    "objectID": "index.html#why-this-matters",
    "href": "index.html#why-this-matters",
    "title": "fastml Tutorials",
    "section": "Why this matters",
    "text": "Why this matters\nIn applied machine learning, reported performance estimates are frequently optimistic due to subtle forms of data leakage and unsafe evaluation workflows.\nfastml is designed to reduce this risk by treating Guarded Resampling as a core design principle rather than as an optional recommendation.",
    "crumbs": [
      "fastml Tutorials"
    ]
  },
  {
    "objectID": "index.html#how-to-read-this-site",
    "href": "index.html#how-to-read-this-site",
    "title": "fastml Tutorials",
    "section": "How to read this site",
    "text": "How to read this site\nThe material is organized into four sections.\n\nConcepts\nThese sections introduce the ideas that motivate fastml, including:\n\nwhat data leakage is,\nwhy many ML pipelines are unsafe by default,\nwhat guarded resampling entails,\nwhich classes of workflow configurations fastml deliberately restricts.\n\nThe concepts establish the assumptions required to interpret the tutorials correctly.\n\n\nTutorials\nThe tutorials apply the conceptual framework to concrete modeling tasks.\nThey assume familiarity with the Concepts section and focus on demonstrating evaluation workflows rather than reintroducing theoretical material.\n\n\nAdvanced\nAdvanced sections extend the framework to more complex settings, such as:\n\nhandling missing data,\nsurvival analysis,\nmodel interpretation and diagnostics.\n\nThese sections build on the same evaluation principles under additional modeling constraints.\n\n\nComparisons\nComparative sections discuss fastml in relation to other frameworks and common workflows, highlighting differences in design philosophy and evaluation guarantees.",
    "crumbs": [
      "fastml Tutorials"
    ]
  },
  {
    "objectID": "index.html#where-to-start",
    "href": "index.html#where-to-start",
    "title": "fastml Tutorials",
    "section": "Where to start",
    "text": "Where to start\nReaders should begin with the Concepts section (C1–C4).\nThe first hands-on example is Tutorial 01: Basic Cl",
    "crumbs": [
      "fastml Tutorials"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html",
    "href": "C4-what-fastml-does-not-allow.html",
    "title": "C4. What fastml Deliberately Does Not Allow",
    "section": "",
    "text": "fastml adopts a deliberately constrained design.\nThese constraints are not the result of missing functionality or incomplete implementation.\nThey are a direct consequence of the guarantees described in C1–C3.\nTo support guarded resampling along the default execution path, certain classes of actions are intentionally restricted.",
    "crumbs": [
      "Concepts",
      "C4. What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#constraints-as-a-design-decision",
    "href": "C4-what-fastml-does-not-allow.html#constraints-as-a-design-decision",
    "title": "C4. What fastml Deliberately Does Not Allow",
    "section": "",
    "text": "fastml adopts a deliberately constrained design.\nThese constraints are not the result of missing functionality or incomplete implementation.\nThey are a direct consequence of the guarantees described in C1–C3.\nTo support guarded resampling along the default execution path, certain classes of actions are intentionally restricted.",
    "crumbs": [
      "Concepts",
      "C4. What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#no-global-preprocessing",
    "href": "C4-what-fastml-does-not-allow.html#no-global-preprocessing",
    "title": "C4. What fastml Deliberately Does Not Allow",
    "section": "No global preprocessing",
    "text": "No global preprocessing\nAlong the default execution path, fastml does not permit preprocessing steps to be estimated using the full dataset prior to resampling.\nUsers are not required—and are not encouraged—to:\n\nscale predictors globally,\nimpute missing values using global statistics,\nperform feature selection outside resampling,\nreuse preprocessing parameters across folds.\n\nInstead, preprocessing steps are estimated separately within each resampling split.\nThis design prevents backward information flow from assessment data into training by construction.",
    "crumbs": [
      "Concepts",
      "C4. What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#no-detached-evaluation",
    "href": "C4-what-fastml-does-not-allow.html#no-detached-evaluation",
    "title": "C4. What fastml Deliberately Does Not Allow",
    "section": "No detached evaluation",
    "text": "No detached evaluation\nIn fastml, evaluation is not treated as a post hoc operation.\nUnder the standard execution model, users do not:\n\nfit a model once and later evaluate it using resampling,\nreuse a trained model across new or inconsistent folds,\ncompute performance metrics outside a resampling-aware context.\n\nModel fitting and evaluation are coupled within a single resampling-based procedure.",
    "crumbs": [
      "Concepts",
      "C4. What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#no-manual-fold-reuse-across-models",
    "href": "C4-what-fastml-does-not-allow.html#no-manual-fold-reuse-across-models",
    "title": "C4. What fastml Deliberately Does Not Allow",
    "section": "No manual fold reuse across models",
    "text": "No manual fold reuse across models\nModel comparison in fastml is conducted using a shared resampling specification.\nUsers do not manually:\n\ndefine different folds for different models within the same comparison,\nselectively reuse folds,\nmix incompatible resampling schemes across models.\n\nThis ensures that observed performance differences arise from modeling choices rather than from differences in data partitioning.",
    "crumbs": [
      "Concepts",
      "C4. What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#no-implicit-tuning-loops",
    "href": "C4-what-fastml-does-not-allow.html#no-implicit-tuning-loops",
    "title": "C4. What fastml Deliberately Does Not Allow",
    "section": "No implicit tuning loops",
    "text": "No implicit tuning loops\nWhen hyperparameter tuning is enabled, it is carried out within the resampling structure.\nUnder the default design, users do not:\n\ntune models on the full dataset and then evaluate them using resampling,\ntune once and reuse tuning results across folds,\ndecouple tuning from evaluation.\n\nThis restricts subtle leakage pathways that can arise when tuning is performed outside resampling.",
    "crumbs": [
      "Concepts",
      "C4. What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#no-partial-control-over-pipeline-order",
    "href": "C4-what-fastml-does-not-allow.html#no-partial-control-over-pipeline-order",
    "title": "C4. What fastml Deliberately Does Not Allow",
    "section": "No partial control over pipeline order",
    "text": "No partial control over pipeline order\nfastml does not expose low-level hooks for rearranging the order of:\n\npreprocessing,\nmodel fitting,\nevaluation.\n\nThis ordering is fixed along the default execution path.\nUsers specify what is to be evaluated, not how the internal pipeline is assembled.",
    "crumbs": [
      "Concepts",
      "C4. What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#why-these-restrictions-exist",
    "href": "C4-what-fastml-does-not-allow.html#why-these-restrictions-exist",
    "title": "C4. What fastml Deliberately Does Not Allow",
    "section": "Why these restrictions exist",
    "text": "Why these restrictions exist\nEach restriction removes a class of evaluation errors that are:\n\neasy to introduce,\ndifficult to detect,\nassociated with optimistic performance estimates,\nrarely flagged by software.\n\nReducing flexibility is the cost of enforcing methodological validity.",
    "crumbs": [
      "Concepts",
      "C4. What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#what-fastml-optimizes-for",
    "href": "C4-what-fastml-does-not-allow.html#what-fastml-optimizes-for",
    "title": "C4. What fastml Deliberately Does Not Allow",
    "section": "What fastml optimizes for",
    "text": "What fastml optimizes for\nfastml is not designed to optimize for:\n\nmaximal procedural flexibility,\nunrestricted pipeline experimentation,\ninteractive trial-and-error workflow assembly.\n\nIt is designed to prioritize:\n\nevaluation correctness,\nreproducibility,\ncomparability across models,\ndefensible performance estimation.",
    "crumbs": [
      "Concepts",
      "C4. What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#when-fastml-may-not-be-appropriate",
    "href": "C4-what-fastml-does-not-allow.html#when-fastml-may-not-be-appropriate",
    "title": "C4. What fastml Deliberately Does Not Allow",
    "section": "When fastml may not be appropriate",
    "text": "When fastml may not be appropriate\nfastml may not be well suited when:\n\nexploratory pipeline experimentation is the primary objective,\nhighly nonstandard evaluation schemes are required,\nfine-grained procedural control is essential.\n\nIn such settings, lower-level or more permissive frameworks may be more appropriate.",
    "crumbs": [
      "Concepts",
      "C4. What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#summary",
    "href": "C4-what-fastml-does-not-allow.html#summary",
    "title": "C4. What fastml Deliberately Does Not Allow",
    "section": "Summary",
    "text": "Summary\n\nfastml is restrictive by design.\nRestrictions follow directly from guarded resampling principles.\nFlexibility is traded for methodological validity.\nIf a workflow feels constrained, the guard is functioning as intended.",
    "crumbs": [
      "Concepts",
      "C4. What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "02-multiple-models-fair-comparison.html",
    "href": "02-multiple-models-fair-comparison.html",
    "title": "02. Multiple Models and Fair Comparison",
    "section": "",
    "text": "Comparing machine learning models is a central goal of applied analysis.\nHowever, many model comparisons are invalid—not because the models are wrong, but because they are evaluated under different data partitions, different preprocessing, or different sources of randomness.\nAs discussed in C3 — Guarded Resampling, fair comparison requires more than using the same metric.",
    "crumbs": [
      "Tutorials",
      "02. Multiple Models and Fair Comparison"
    ]
  },
  {
    "objectID": "02-multiple-models-fair-comparison.html#motivation",
    "href": "02-multiple-models-fair-comparison.html#motivation",
    "title": "02. Multiple Models and Fair Comparison",
    "section": "",
    "text": "Comparing machine learning models is a central goal of applied analysis.\nHowever, many model comparisons are invalid—not because the models are wrong, but because they are evaluated under different data partitions, different preprocessing, or different sources of randomness.\nAs discussed in C3 — Guarded Resampling, fair comparison requires more than using the same metric.",
    "crumbs": [
      "Tutorials",
      "02. Multiple Models and Fair Comparison"
    ]
  },
  {
    "objectID": "02-multiple-models-fair-comparison.html#what-fair-comparison-means",
    "href": "02-multiple-models-fair-comparison.html#what-fair-comparison-means",
    "title": "02. Multiple Models and Fair Comparison",
    "section": "What “fair comparison” means",
    "text": "What “fair comparison” means\nA comparison is fair only if:\n\nall models see the same resampling splits\npreprocessing is learned independently within each split\nmetrics are computed on identical assessment sets\ndifferences reflect models, not data partitioning\n\nfastml enforces these conditions along its guarded resampling execution path, ensuring fair comparison when models are evaluated within a single fastml call.",
    "crumbs": [
      "Tutorials",
      "02. Multiple Models and Fair Comparison"
    ]
  },
  {
    "objectID": "02-multiple-models-fair-comparison.html#data",
    "href": "02-multiple-models-fair-comparison.html#data",
    "title": "02. Multiple Models and Fair Comparison",
    "section": "Data",
    "text": "Data\nWe use a complete-case version of the BreastCancer dataset after removing the identifier column and excluding observations with missing values. This complete-case strategy is used for simplicity in this tutorial; in real applications, missingness should be handled explicitly (e.g., via guarded imputation) when it is nontrivial or informative.\n\nlibrary(fastml)\nlibrary(mlbench)\nlibrary(dplyr)\n\ndata(BreastCancer)\n\nbreastCancer &lt;- BreastCancer %&gt;%\n  select(-Id) %&gt;%\n  filter(!is.na(Class)) %&gt;%\n  na.omit() %&gt;%\n  mutate(Class = factor(Class, levels = c(\"benign\", \"malignant\")))\n  \nhead(breastCancer)\n\n  Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size Bare.nuclei\n1            5         1          1             1            2           1\n2            5         4          4             5            7          10\n3            3         1          1             1            2           2\n4            6         8          8             1            3           4\n5            4         1          1             3            2           1\n6            8        10         10             8            7          10\n  Bl.cromatin Normal.nucleoli Mitoses     Class\n1           3               1       1    benign\n2           3               2       1    benign\n3           3               1       1    benign\n4           3               7       1    benign\n5           3               1       1    benign\n6           9               7       1 malignant",
    "crumbs": [
      "Tutorials",
      "02. Multiple Models and Fair Comparison"
    ]
  },
  {
    "objectID": "02-multiple-models-fair-comparison.html#defining-multiple-models",
    "href": "02-multiple-models-fair-comparison.html#defining-multiple-models",
    "title": "02. Multiple Models and Fair Comparison",
    "section": "Defining Multiple Models",
    "text": "Defining Multiple Models\nWe compare three structurally different models:\n\nLogistic regression (linear)\nRandom forest (tree ensemble)\nGradient boosted trees\n\nNo hyperparameter tuning is performed.\nDefault engine settings are used intentionally. This isolates differences due to model structure rather than tuning effort, which is essential for a fair baseline comparison.\n\nfit &lt;- fastml(\n  data       = breastCancer,\n  label      = \"Class\",\n  algorithms = c(\"logistic_reg\", \"rand_forest\", \"xgboost\")\n)",
    "crumbs": [
      "Tutorials",
      "02. Multiple Models and Fair Comparison"
    ]
  },
  {
    "objectID": "02-multiple-models-fair-comparison.html#shared-resampling-plan",
    "href": "02-multiple-models-fair-comparison.html#shared-resampling-plan",
    "title": "02. Multiple Models and Fair Comparison",
    "section": "Shared resampling plan",
    "text": "Shared resampling plan\nAll models are evaluated under a single, shared resampling plan.\n\nfit$resampling_plan\n\n$splits\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [490/56]&gt; Fold01\n 2 &lt;split [491/55]&gt; Fold02\n 3 &lt;split [491/55]&gt; Fold03\n 4 &lt;split [491/55]&gt; Fold04\n 5 &lt;split [491/55]&gt; Fold05\n 6 &lt;split [492/54]&gt; Fold06\n 7 &lt;split [492/54]&gt; Fold07\n 8 &lt;split [492/54]&gt; Fold08\n 9 &lt;split [492/54]&gt; Fold09\n10 &lt;split [492/54]&gt; Fold10\n\n$metadata\n$metadata$method\n[1] \"cv\"\n\n$metadata$params\n$metadata$params$v\n[1] 10\n\n$metadata$params$repeats\n[1] 1\n\n$metadata$params$strata\n[1] \"Class\"\n\n\n\nattr(,\"class\")\n[1] \"fastml_resample_plan\" \"list\"                \n\n\nThis guarantees that:\n\nevery model is trained and evaluated on the same splits\ndifferences in performance are attributable to the model, not the data",
    "crumbs": [
      "Tutorials",
      "02. Multiple Models and Fair Comparison"
    ]
  },
  {
    "objectID": "02-multiple-models-fair-comparison.html#aggregated-performance",
    "href": "02-multiple-models-fair-comparison.html#aggregated-performance",
    "title": "02. Multiple Models and Fair Comparison",
    "section": "Aggregated performance",
    "text": "Aggregated performance\nWe first examine performance aggregated across folds.\n\nfit$resampling_results$`logistic_reg (glm)`$aggregated\n\n# A tibble: 7 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.914\n2 f_meas    binary         0.936\n3 kap       binary         0.804\n4 precision binary         0.907\n5 roc_auc   binary         0.923\n6 sens      binary         0.969\n7 spec      binary         0.812\n\nfit$resampling_results$`rand_forest (ranger)`$aggregated\n\n# A tibble: 7 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.962\n2 f_meas    binary         0.970\n3 kap       binary         0.916\n4 precision binary         0.975\n5 roc_auc   binary         0.991\n6 sens      binary         0.966\n7 spec      binary         0.953\n\nfit$resampling_results$`xgboost (xgboost)`$aggregated\n\n# A tibble: 7 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.949\n2 f_meas    binary         0.961\n3 kap       binary         0.887\n4 precision binary         0.957\n5 roc_auc   binary         0.987\n6 sens      binary         0.966\n7 spec      binary         0.917\n\n\nThese summaries provide point estimates under guarded cross-validation.",
    "crumbs": [
      "Tutorials",
      "02. Multiple Models and Fair Comparison"
    ]
  },
  {
    "objectID": "02-multiple-models-fair-comparison.html#fold-level-variability",
    "href": "02-multiple-models-fair-comparison.html#fold-level-variability",
    "title": "02. Multiple Models and Fair Comparison",
    "section": "Fold-level variability",
    "text": "Fold-level variability\nFair comparison also requires inspecting variability across folds.\n\nfit$resampling_results$`rand_forest (ranger)`$folds\n\n# A tibble: 70 × 4\n   fold  .metric   .estimator .estimate\n   &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n 1 1     accuracy  binary         0.929\n 2 1     kap       binary         0.844\n 3 1     sens      binary         0.944\n 4 1     spec      binary         0.9  \n 5 1     precision binary         0.944\n 6 1     f_meas    binary         0.944\n 7 1     roc_auc   binary         0.968\n 8 2     accuracy  binary         0.964\n 9 2     kap       binary         0.922\n10 2     sens      binary         0.944\n# ℹ 60 more rows\n\n\nFold-level results reveal:\n\ninstability\nsensitivity to data partitioning\noverlap between models’ performance distributions\n\nSingle numbers are rarely sufficient.",
    "crumbs": [
      "Tutorials",
      "02. Multiple Models and Fair Comparison"
    ]
  },
  {
    "objectID": "02-multiple-models-fair-comparison.html#best-model-selection",
    "href": "02-multiple-models-fair-comparison.html#best-model-selection",
    "title": "02. Multiple Models and Fair Comparison",
    "section": "Best model selection",
    "text": "Best model selection\nfastml identifies a “best” model based on the primary metric.\n\nfit$best_model_name\n\nrand_forest     xgboost \n   \"ranger\"   \"xgboost\" \n\n\nThis selection is descriptive, not inferential.\nIt indicates which model performed best under the chosen metric, resampling scheme, and random seed, not which model is universally superior.",
    "crumbs": [
      "Tutorials",
      "02. Multiple Models and Fair Comparison"
    ]
  },
  {
    "objectID": "02-multiple-models-fair-comparison.html#why-this-comparison-is-valid",
    "href": "02-multiple-models-fair-comparison.html#why-this-comparison-is-valid",
    "title": "02. Multiple Models and Fair Comparison",
    "section": "Why this comparison is valid",
    "text": "Why this comparison is valid\nThis comparison is valid because:\n\npreprocessing is guarded\nresampling is shared\nevaluation is inseparable from fitting\nno model receives information unavailable to others\n\nThese properties hold regardless of model complexity.",
    "crumbs": [
      "Tutorials",
      "02. Multiple Models and Fair Comparison"
    ]
  },
  {
    "objectID": "02-multiple-models-fair-comparison.html#what-fastml-does-not-allow-here",
    "href": "02-multiple-models-fair-comparison.html#what-fastml-does-not-allow-here",
    "title": "02. Multiple Models and Fair Comparison",
    "section": "What fastml does not allow here",
    "text": "What fastml does not allow here\nConsistent with C4 — What fastml Deliberately Does Not Allow, within a single fastml benchmarking call users cannot:\n\nevaluate models on different folds\nreuse preprocessing across models\ntune one model more aggressively than another\ndetach evaluation from resampling\n\nThese restrictions are necessary for fairness.",
    "crumbs": [
      "Tutorials",
      "02. Multiple Models and Fair Comparison"
    ]
  },
  {
    "objectID": "02-multiple-models-fair-comparison.html#summary",
    "href": "02-multiple-models-fair-comparison.html#summary",
    "title": "02. Multiple Models and Fair Comparison",
    "section": "Summary",
    "text": "Summary\n\nFair model comparison requires shared resampling.\nfastml enforces this structurally.\nDifferences in performance reflect models, not evaluation artifacts.\n“Best model” selection is contextual and metric-dependent.",
    "crumbs": [
      "Tutorials",
      "02. Multiple Models and Fair Comparison"
    ]
  },
  {
    "objectID": "02-multiple-models-fair-comparison.html#what-comes-next",
    "href": "02-multiple-models-fair-comparison.html#what-comes-next",
    "title": "02. Multiple Models and Fair Comparison",
    "section": "What comes next",
    "text": "What comes next\n03. Metrics, Variability, and Uncertainty\nFold variability, bootstrap metrics and over-interpretation of small differences.",
    "crumbs": [
      "Tutorials",
      "02. Multiple Models and Fair Comparison"
    ]
  }
]