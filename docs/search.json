[
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html",
    "title": "C2 — Why Most ML Pipelines Are Unsafe by Default",
    "section": "",
    "text": "Many modern machine learning frameworks provide components that can be assembled into leakage-safe workflows.\nYet leakage remains widespread in applied work.\nThis is not a contradiction. It follows from how pipelines are designed and how methodological correctness is treated within those designs.",
    "crumbs": [
      "Concepts",
      "C2 — Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#the-paradox-of-modern-ml-tooling",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#the-paradox-of-modern-ml-tooling",
    "title": "C2 — Why Most ML Pipelines Are Unsafe by Default",
    "section": "",
    "text": "Many modern machine learning frameworks provide components that can be assembled into leakage-safe workflows.\nYet leakage remains widespread in applied work.\nThis is not a contradiction. It follows from how pipelines are designed and how methodological correctness is treated within those designs.",
    "crumbs": [
      "Concepts",
      "C2 — Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#flexibility-without-constraints",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#flexibility-without-constraints",
    "title": "C2 — Why Most ML Pipelines Are Unsafe by Default",
    "section": "Flexibility without constraints",
    "text": "Flexibility without constraints\nContemporary ML frameworks emphasize modularity:\n\npreprocessing steps are independent components,\nmodels are interchangeable,\nresampling is optional and configurable,\nevaluation is often treated as a separable stage.\n\nThis flexibility is powerful, but it has consequences.\nNothing in the default design prevents users from assembling pipelines in orders that violate evaluation assumptions.\nSuch pipelines are typically syntactically valid and computationally successful, even when they are methodologically incorrect.",
    "crumbs": [
      "Concepts",
      "C2 — Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#optional-correctness-is-not-correctness",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#optional-correctness-is-not-correctness",
    "title": "C2 — Why Most ML Pipelines Are Unsafe by Default",
    "section": "Optional correctness is not correctness",
    "text": "Optional correctness is not correctness\nMany frameworks provide mechanisms intended to reduce leakage risk:\n\nresampling-aware preprocessing tools,\nworkflow abstractions,\nhelper functions that encourage correct usage.\n\nHowever, these mechanisms are optional.\nUsers may still:\n\ncompute transformations using the full dataset,\nreuse or redefine resampling splits inconsistently,\napply preprocessing outside the resampling loop,\ntune models using data that later serve as assessment sets.\n\nThe software generally permits these configurations to run without intervention.\nAs a result, methodological correctness is advisory rather than enforced.",
    "crumbs": [
      "Concepts",
      "C2 — Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#why-warnings-do-not-solve-the-problem",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#why-warnings-do-not-solve-the-problem",
    "title": "C2 — Why Most ML Pipelines Are Unsafe by Default",
    "section": "Why warnings do not solve the problem",
    "text": "Why warnings do not solve the problem\nOne might expect software to detect leakage and issue warnings.\nIn practice, this approach is limited.\nLeakage is often:\n\nsemantically ambiguous,\ndependent on context and intent,\nindistinguishable from valid workflows at runtime.\n\nStatic analysis can detect only narrow classes of problems, and comprehensive runtime checks are difficult without restricting flexibility.\nMost frameworks therefore favor permissiveness over enforcement.",
    "crumbs": [
      "Concepts",
      "C2 — Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#the-expert-user-fallacy",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#the-expert-user-fallacy",
    "title": "C2 — Why Most ML Pipelines Are Unsafe by Default",
    "section": "The expert-user fallacy",
    "text": "The expert-user fallacy\nA common response is to place responsibility on user expertise.\nThis implicitly assumes that:\n\nexperienced users consistently assemble pipelines correctly,\nmistakes are rare or immediately obvious,\ndiscipline scales with workflow complexity.\n\nThese assumptions do not reliably hold.\nAs workflows become more complex—nested resampling, grouped splits, tuning loops—the space for subtle errors expands, even for experienced practitioners.\nExpertise can reduce risk, but it does not eliminate it.",
    "crumbs": [
      "Concepts",
      "C2 — Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#consequences-for-evaluation",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#consequences-for-evaluation",
    "title": "C2 — Why Most ML Pipelines Are Unsafe by Default",
    "section": "Consequences for evaluation",
    "text": "Consequences for evaluation\nWhen pipelines are unsafe by default:\n\ninvalid evaluations may appear legitimate,\nperformance estimates tend to be optimistic,\nresults are harder to reproduce,\nreviewers and readers have limited visibility into evaluation correctness.\n\nThis is not primarily a failure of individual users.\nIt reflects limitations in pipeline architecture.",
    "crumbs": [
      "Concepts",
      "C2 — Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#what-a-safe-default-would-require",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#what-a-safe-default-would-require",
    "title": "C2 — Why Most ML Pipelines Are Unsafe by Default",
    "section": "What a safe default would require",
    "text": "What a safe default would require\nA safe default requires more than recommendations or documentation.\nIt requires that:\n\nresampling encloses all data-dependent learning steps,\npreprocessing cannot be learned globally,\nevaluation cannot be detached from model fitting,\nunsafe configurations are restricted along the default execution path.\n\nIn other words, correctness must be enforced by construction rather than assumed.",
    "crumbs": [
      "Concepts",
      "C2 — Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#summary",
    "href": "C2-why-most-ml-pipelines-are-unsafe-by-default.html#summary",
    "title": "C2 — Why Most ML Pipelines Are Unsafe by Default",
    "section": "Summary",
    "text": "Summary\n\nModern ML frameworks prioritize flexibility.\nMethodological correctness is typically optional.\nInvalid pipelines are often allowed to execute silently.\nExpertise alone does not prevent evaluation errors.\nSafe evaluation requires architectural constraints.\n\nThis motivates the core idea behind fastml: Guarded Resampling.\nNext: C3 — Guarded Resampling",
    "crumbs": [
      "Concepts",
      "C2 — Why Most ML Pipelines Are Unsafe by Default"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html",
    "href": "C4-what-fastml-does-not-allow.html",
    "title": "C4 — What fastml Deliberately Does Not Allow",
    "section": "",
    "text": "fastml adopts a deliberately constrained design.\nThese constraints are not the result of missing functionality or incomplete implementation.\nThey are a direct consequence of the guarantees described in C1–C3.\nTo support guarded resampling along the default execution path, certain classes of actions are intentionally restricted.",
    "crumbs": [
      "Concepts",
      "C4 — What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#constraints-as-a-design-decision",
    "href": "C4-what-fastml-does-not-allow.html#constraints-as-a-design-decision",
    "title": "C4 — What fastml Deliberately Does Not Allow",
    "section": "",
    "text": "fastml adopts a deliberately constrained design.\nThese constraints are not the result of missing functionality or incomplete implementation.\nThey are a direct consequence of the guarantees described in C1–C3.\nTo support guarded resampling along the default execution path, certain classes of actions are intentionally restricted.",
    "crumbs": [
      "Concepts",
      "C4 — What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#no-global-preprocessing",
    "href": "C4-what-fastml-does-not-allow.html#no-global-preprocessing",
    "title": "C4 — What fastml Deliberately Does Not Allow",
    "section": "No global preprocessing",
    "text": "No global preprocessing\nAlong the default execution path, fastml does not permit preprocessing steps to be estimated using the full dataset prior to resampling.\nUsers are not required—and are not encouraged—to:\n\nscale predictors globally,\nimpute missing values using global statistics,\nperform feature selection outside resampling,\nreuse preprocessing parameters across folds.\n\nInstead, preprocessing steps are estimated separately within each resampling split.\nThis design prevents backward information flow from assessment data into training by construction.",
    "crumbs": [
      "Concepts",
      "C4 — What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#no-detached-evaluation",
    "href": "C4-what-fastml-does-not-allow.html#no-detached-evaluation",
    "title": "C4 — What fastml Deliberately Does Not Allow",
    "section": "No detached evaluation",
    "text": "No detached evaluation\nIn fastml, evaluation is not treated as a post hoc operation.\nUnder the standard execution model, users do not:\n\nfit a model once and later evaluate it using resampling,\nreuse a trained model across new or inconsistent folds,\ncompute performance metrics outside a resampling-aware context.\n\nModel fitting and evaluation are coupled within a single resampling-based procedure.",
    "crumbs": [
      "Concepts",
      "C4 — What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#no-manual-fold-reuse-across-models",
    "href": "C4-what-fastml-does-not-allow.html#no-manual-fold-reuse-across-models",
    "title": "C4 — What fastml Deliberately Does Not Allow",
    "section": "No manual fold reuse across models",
    "text": "No manual fold reuse across models\nModel comparison in fastml is conducted using a shared resampling specification.\nUsers do not manually:\n\ndefine different folds for different models within the same comparison,\nselectively reuse folds,\nmix incompatible resampling schemes across models.\n\nThis ensures that observed performance differences arise from modeling choices rather than from differences in data partitioning.",
    "crumbs": [
      "Concepts",
      "C4 — What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#no-implicit-tuning-loops",
    "href": "C4-what-fastml-does-not-allow.html#no-implicit-tuning-loops",
    "title": "C4 — What fastml Deliberately Does Not Allow",
    "section": "No implicit tuning loops",
    "text": "No implicit tuning loops\nWhen hyperparameter tuning is enabled, it is carried out within the resampling structure.\nUnder the default design, users do not:\n\ntune models on the full dataset and then evaluate them using resampling,\ntune once and reuse tuning results across folds,\ndecouple tuning from evaluation.\n\nThis restricts subtle leakage pathways that can arise when tuning is performed outside resampling.",
    "crumbs": [
      "Concepts",
      "C4 — What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#no-partial-control-over-pipeline-order",
    "href": "C4-what-fastml-does-not-allow.html#no-partial-control-over-pipeline-order",
    "title": "C4 — What fastml Deliberately Does Not Allow",
    "section": "No partial control over pipeline order",
    "text": "No partial control over pipeline order\nfastml does not expose low-level hooks for rearranging the order of:\n\npreprocessing,\nmodel fitting,\nevaluation.\n\nThis ordering is fixed along the default execution path.\nUsers specify what is to be evaluated, not how the internal pipeline is assembled.",
    "crumbs": [
      "Concepts",
      "C4 — What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#why-these-restrictions-exist",
    "href": "C4-what-fastml-does-not-allow.html#why-these-restrictions-exist",
    "title": "C4 — What fastml Deliberately Does Not Allow",
    "section": "Why these restrictions exist",
    "text": "Why these restrictions exist\nEach restriction removes a class of evaluation errors that are:\n\neasy to introduce,\ndifficult to detect,\nassociated with optimistic performance estimates,\nrarely flagged by software.\n\nReducing flexibility is the cost of enforcing methodological validity.",
    "crumbs": [
      "Concepts",
      "C4 — What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#what-fastml-optimizes-for",
    "href": "C4-what-fastml-does-not-allow.html#what-fastml-optimizes-for",
    "title": "C4 — What fastml Deliberately Does Not Allow",
    "section": "What fastml optimizes for",
    "text": "What fastml optimizes for\nfastml is not designed to optimize for:\n\nmaximal procedural flexibility,\nunrestricted pipeline experimentation,\ninteractive trial-and-error workflow assembly.\n\nIt is designed to prioritize:\n\nevaluation correctness,\nreproducibility,\ncomparability across models,\ndefensible performance estimation.",
    "crumbs": [
      "Concepts",
      "C4 — What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#when-fastml-may-not-be-appropriate",
    "href": "C4-what-fastml-does-not-allow.html#when-fastml-may-not-be-appropriate",
    "title": "C4 — What fastml Deliberately Does Not Allow",
    "section": "When fastml may not be appropriate",
    "text": "When fastml may not be appropriate\nfastml may not be well suited when:\n\nexploratory pipeline experimentation is the primary objective,\nhighly nonstandard evaluation schemes are required,\nfine-grained procedural control is essential.\n\nIn such settings, lower-level or more permissive frameworks may be more appropriate.",
    "crumbs": [
      "Concepts",
      "C4 — What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "C4-what-fastml-does-not-allow.html#summary",
    "href": "C4-what-fastml-does-not-allow.html#summary",
    "title": "C4 — What fastml Deliberately Does Not Allow",
    "section": "Summary",
    "text": "Summary\n\nfastml is restrictive by design.\nRestrictions follow directly from guarded resampling principles.\nFlexibility is traded for methodological validity.\nIf a workflow feels constrained, the guard is functioning as intended.",
    "crumbs": [
      "Concepts",
      "C4 — What fastml Deliberately Does Not Allow"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fastml Tutorials",
    "section": "",
    "text": "This site provides a structured introduction to fastml, an R package for training, evaluating, and comparing machine learning models under architecturally constrained, leakage-aware resampling.\nThe emphasis is not on developing new modeling techniques or maximizing predictive performance. Instead, the focus is on methodologically sound performance evaluation under clearly defined assumptions.",
    "crumbs": [
      "fastml Tutorials"
    ]
  },
  {
    "objectID": "index.html#what-this-site-is-about",
    "href": "index.html#what-this-site-is-about",
    "title": "fastml Tutorials",
    "section": "",
    "text": "This site provides a structured introduction to fastml, an R package for training, evaluating, and comparing machine learning models under architecturally constrained, leakage-aware resampling.\nThe emphasis is not on developing new modeling techniques or maximizing predictive performance. Instead, the focus is on methodologically sound performance evaluation under clearly defined assumptions.",
    "crumbs": [
      "fastml Tutorials"
    ]
  },
  {
    "objectID": "index.html#why-this-matters",
    "href": "index.html#why-this-matters",
    "title": "fastml Tutorials",
    "section": "Why this matters",
    "text": "Why this matters\nIn applied machine learning, reported performance estimates are frequently optimistic due to subtle forms of data leakage and unsafe evaluation workflows.\nfastml is designed to reduce this risk by treating Guarded Resampling as a core design principle rather than as an optional recommendation.",
    "crumbs": [
      "fastml Tutorials"
    ]
  },
  {
    "objectID": "index.html#how-to-read-this-site",
    "href": "index.html#how-to-read-this-site",
    "title": "fastml Tutorials",
    "section": "How to read this site",
    "text": "How to read this site\nThe material is organized into four sections.\n\nConcepts\nThese sections introduce the ideas that motivate fastml, including:\n\nwhat data leakage is,\nwhy many ML pipelines are unsafe by default,\nwhat guarded resampling entails,\nwhich classes of workflow configurations fastml deliberately restricts.\n\nThe concepts establish the assumptions required to interpret the tutorials correctly.\n\n\nTutorials\nThe tutorials apply the conceptual framework to concrete modeling tasks.\nThey assume familiarity with the Concepts section and focus on demonstrating evaluation workflows rather than reintroducing theoretical material.\n\n\nAdvanced\nAdvanced sections extend the framework to more complex settings, such as:\n\nhandling missing data,\nsurvival analysis,\nmodel interpretation and diagnostics.\n\nThese sections build on the same evaluation principles under additional modeling constraints.\n\n\nComparisons\nComparative sections discuss fastml in relation to other frameworks and common workflows, highlighting differences in design philosophy and evaluation guarantees.",
    "crumbs": [
      "fastml Tutorials"
    ]
  },
  {
    "objectID": "index.html#where-to-start",
    "href": "index.html#where-to-start",
    "title": "fastml Tutorials",
    "section": "Where to start",
    "text": "Where to start\nReaders should begin with the Concepts section (C1–C4).\nThe first hands-on example is Tutorial 01: Basic Cl",
    "crumbs": [
      "fastml Tutorials"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html",
    "href": "C3-guarded-resampling.html",
    "title": "C3 — Guarded Resampling",
    "section": "",
    "text": "C1 established that data leakage is a structural timing error.\nC2 showed that many machine learning pipelines permit such errors by default.\nA direct consequence is that methodological correctness cannot rely on user discipline alone.\nGuarded Resampling is an architectural response to this limitation.",
    "crumbs": [
      "Concepts",
      "C3 — Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#from-advice-to-enforcement",
    "href": "C3-guarded-resampling.html#from-advice-to-enforcement",
    "title": "C3 — Guarded Resampling",
    "section": "",
    "text": "C1 established that data leakage is a structural timing error.\nC2 showed that many machine learning pipelines permit such errors by default.\nA direct consequence is that methodological correctness cannot rely on user discipline alone.\nGuarded Resampling is an architectural response to this limitation.",
    "crumbs": [
      "Concepts",
      "C3 — Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#what-guarded-means",
    "href": "C3-guarded-resampling.html#what-guarded-means",
    "title": "C3 — Guarded Resampling",
    "section": "What “Guarded” means",
    "text": "What “Guarded” means\nIn guarded resampling, resampling is not treated as a downstream step in a workflow. Instead, it provides the enclosing structure for all operations that learn parameters from the data.\nUnder this design, any operation whose results depend on estimated quantities is executed separately within each resampling split, including:\n\npreprocessing steps that estimate statistics from the data,\nmodel fitting,\nhyperparameter selection,\nperformance metric computation.\n\nOperations that would require access to information outside the analysis set, or that would induce backward information flow from assessment data, are restricted along the default execution path.\nThis is not a usage convention. It is a structural constraint.",
    "crumbs": [
      "Concepts",
      "C3 — Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#resampling-as-the-primary-object",
    "href": "C3-guarded-resampling.html#resampling-as-the-primary-object",
    "title": "C3 — Guarded Resampling",
    "section": "Resampling as the primary object",
    "text": "Resampling as the primary object\nIn many applied machine learning workflows, resampling is effectively treated as an external evaluation step, rather than as a structural component of the learning procedure. A common pattern is:\n\nprepare data,\nfit a model,\nevaluate performance using resampling.\n\nAlthough widely used, this organization obscures the fact that valid performance estimation requires strict separation between training and assessment data at all stages of model construction.\nGuarded resampling reverses this relationship. Instead of attaching resampling post hoc, the workflow is defined by the resampling scheme itself:\n\ndefine a resampling plan,\nfor each split:\n\nestimate preprocessing steps,\nfit the model,\ngenerate predictions,\n\naggregate results across splits.\n\nUnder this formulation, resampling defines the experimental structure, and all learning steps operate conditionally within it.",
    "crumbs": [
      "Concepts",
      "C3 — Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#why-enclosure-matters",
    "href": "C3-guarded-resampling.html#why-enclosure-matters",
    "title": "C3 — Guarded Resampling",
    "section": "Why enclosure matters",
    "text": "Why enclosure matters\nBy enclosing learning within resampling splits, guarded resampling ensures that:\n\nassessment data are isolated from training within each split,\npreprocessing parameters are estimated separately for each training set,\nmodel comparisons are conducted using identical resampling splits,\nperformance estimates correspond to resampling-based out-of-sample evaluation.\n\nThese properties do not depend on remembering rules or following best practices.\nThey arise from how the workflow is constructed.",
    "crumbs": [
      "Concepts",
      "C3 — Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#guards-are-not-warnings",
    "href": "C3-guarded-resampling.html#guards-are-not-warnings",
    "title": "C3 — Guarded Resampling",
    "section": "Guards are not warnings",
    "text": "Guards are not warnings\nGuarded resampling does not attempt to identify leakage after it has occurred.\nInstead, it restricts the construction of workflows in which leakage-prone configurations would arise along the standard execution path.\nThis distinction is important:\n\nwarnings are advisory and can be ignored,\nstructural constraints limit what can be expressed.\n\nWorkflows that violate the principles of guarded resampling are therefore not part of the default design space.",
    "crumbs": [
      "Concepts",
      "C3 — Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#what-this-enables",
    "href": "C3-guarded-resampling.html#what-this-enables",
    "title": "C3 — Guarded Resampling",
    "section": "What this enables",
    "text": "What this enables\nBecause resampling is treated as an enclosing structure:\n\npreprocessing is resampling-aware by default,\nresampling splits are applied consistently across models,\nevaluation remains coupled to model fitting,\nreported metrics reflect resampling-based estimates.\n\nThese properties hold independently of model class, algorithmic complexity, or user expertise, subject to the assumptions of the resampling procedure.\nMethodological correctness becomes a property of system design rather than of individual user choices.",
    "crumbs": [
      "Concepts",
      "C3 — Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#what-this-does-not-do",
    "href": "C3-guarded-resampling.html#what-this-does-not-do",
    "title": "C3 — Guarded Resampling",
    "section": "What this does not do",
    "text": "What this does not do\nGuarded resampling does not:\n\nguarantee strong predictive performance,\nidentify a statistically optimal model,\nreplace careful study design or domain expertise,\neliminate all possible sources of bias.\n\nIt addresses a specific and well-defined failure mode: invalid evaluation arising from data leakage.",
    "crumbs": [
      "Concepts",
      "C3 — Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#why-this-is-restrictive-by-design",
    "href": "C3-guarded-resampling.html#why-this-is-restrictive-by-design",
    "title": "C3 — Guarded Resampling",
    "section": "Why this is restrictive by design",
    "text": "Why this is restrictive by design\nEnforcing structural guards necessarily reduces flexibility.\nCertain actions that are technically possible in more permissive frameworks are intentionally restricted along the default execution path.\nThis is not a limitation to be worked around.\nIt is a deliberate design choice.\nThe consequences of this choice are addressed in the next concept.",
    "crumbs": [
      "Concepts",
      "C3 — Guarded Resampling"
    ]
  },
  {
    "objectID": "C3-guarded-resampling.html#summary",
    "href": "C3-guarded-resampling.html#summary",
    "title": "C3 — Guarded Resampling",
    "section": "Summary",
    "text": "Summary\n\nGuarded resampling treats resampling as the enclosing structure.\nData-dependent learning occurs within resampling splits.\nLeakage-prone configurations are restricted by design.\nEnforcement replaces advisory guidance.\n\nThis is the core idea behind fastml.\nNext: C4 — What fastml Deliberately Does Not Allow",
    "crumbs": [
      "Concepts",
      "C3 — Guarded Resampling"
    ]
  },
  {
    "objectID": "01-basic-classification.html",
    "href": "01-basic-classification.html",
    "title": "01. Basic Classification",
    "section": "",
    "text": "This tutorial builds on the following conceptual material:\n\nC1 — What is Data Leakage\nC2 — Why Most ML Pipelines Are Unsafe by Default\nC3 — Guarded Resampling\nC4 — What fastml Deliberately Does Not Allow\n\nReaders are expected to be familiar with these concepts before proceeding.\nThe workflow demonstrated here uses a deliberately constrained interface. These constraints are intentional and reflect the design philosophy of fastml: to reduce common sources of evaluation error by limiting user-facing degrees of freedom along the default execution path.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#before-you-start",
    "href": "01-basic-classification.html#before-you-start",
    "title": "01. Basic Classification",
    "section": "",
    "text": "This tutorial builds on the following conceptual material:\n\nC1 — What is Data Leakage\nC2 — Why Most ML Pipelines Are Unsafe by Default\nC3 — Guarded Resampling\nC4 — What fastml Deliberately Does Not Allow\n\nReaders are expected to be familiar with these concepts before proceeding.\nThe workflow demonstrated here uses a deliberately constrained interface. These constraints are intentional and reflect the design philosophy of fastml: to reduce common sources of evaluation error by limiting user-facing degrees of freedom along the default execution path.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#the-problem-we-are-solving",
    "href": "01-basic-classification.html#the-problem-we-are-solving",
    "title": "01. Basic Classification",
    "section": "The problem we are solving",
    "text": "The problem we are solving\nThe goal of this tutorial is to estimate the out-of-sample performance of a binary classifier.\nThe focus is not on maximizing predictive accuracy, tuning hyperparameters, or examining model internals. Instead, the objective is narrowly defined:\nWhat level of predictive performance can reasonably be expected on new, unseen data, given a fixed modeling specification?\nIn this setting, model fitting itself is straightforward. The primary challenge lies in performance evaluation.\nSpecifically, the difficulty is to obtain an estimate that is not biased by information leakage or other forms of contamination arising from reuse of the data during training and evaluation.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#why-this-is-harder-than-it-sounds",
    "href": "01-basic-classification.html#why-this-is-harder-than-it-sounds",
    "title": "01. Basic Classification",
    "section": "Why this is harder than it sounds",
    "text": "Why this is harder than it sounds\nFrom C1 and C2, recall the following points:\n\nLeakage does not require obvious mistakes.\nPipelines that appear valid can still yield biased performance estimates.\nMost machine learning frameworks allow such pipelines to run without warnings.\n\nFrom C3, recall:\n\nCorrect evaluation requires resampling to enclose preprocessing and model fitting.\nThis enclosure must be structural, not procedural.\n\nFrom C4, recall:\n\nfastml enforces correctness by removing degrees of freedom rather than relying on user discipline.\n\nThis tutorial demonstrates what that enforcement looks like in practice.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#the-data",
    "href": "01-basic-classification.html#the-data",
    "title": "01. Basic Classification",
    "section": "The data",
    "text": "The data\nThis tutorial uses a simple binary classification dataset from the modeldata package.\nThe dataset is characterized by:\n\na binary outcome variable (Class),\na small number of continuous predictors (A and B),\nthe absence of missing values.\n\nThis choice is intentional. The dataset is deliberately low-dimensional and clean, so that attention can remain on the evaluation procedure rather than on feature engineering, preprocessing decisions, or data-quality issues.\n\nlibrary(modeldata)\ndata(two_class_dat)\ntwo_class_dat\n\n            A         B  Class\n1   2.0697297 1.6316467 Class1\n2   2.0164150 1.0366289 Class1\n3   1.6885554 1.3666097 Class2\n4   3.4345378 1.9797759 Class2\n5   2.8845957 1.9758911 Class1\n6   3.3135895 2.4058755 Class2\n7   2.4955833 1.5596673 Class2\n8   1.9782978 1.5502284 Class2\n9   2.8759583 0.5797836 Class1\n10  3.7402188 2.7431176 Class2\n11  1.4433105 1.6776070 Class1\n12  2.3427624 2.3230457 Class2\n13  2.6469801 1.8830934 Class2\n14  0.8486771 0.8129134 Class1\n15  3.2529647 0.8692317 Class1\n16  1.0516171 0.8450980 Class1\n17  0.8862019 0.4885507 Class1\n18  2.9069784 1.5378191 Class1\n19  3.1365968 2.0626572 Class2\n20  1.0387123 0.8864907 Class2\n21  2.1357587 2.3263359 Class2\n22  1.5648698 1.1003705 Class2\n23  3.6546557 2.4699692 Class2\n24  1.1324521 0.8976271 Class1\n25  2.1411035 1.8825245 Class1\n26  1.4251777 1.1849752 Class2\n27  0.9617529 0.7489629 Class1\n28  1.5055126 0.9684829 Class1\n29  3.9320717 1.7431961 Class1\n30  1.6410639 0.5024271 Class1\n31  2.9537896 3.0028569 Class2\n32  0.9892504 0.6127839 Class2\n33  2.2643581 1.5671440 Class2\n34  1.0098809 0.6989700 Class1\n35  1.8817118 1.9577988 Class1\n36  1.6086655 1.1430148 Class1\n37  1.4185320 0.8195439 Class1\n38  3.2408479 1.9705328 Class1\n39  0.8818781 0.4983106 Class1\n40  1.1986673 0.9138139 Class1\n41  2.0509957 1.6473830 Class2\n42  3.7312721 1.6519561 Class2\n43  1.0986309 0.7543483 Class1\n44  1.0771790 1.2750809 Class1\n45  3.3142770 2.6234457 Class2\n46  1.7454073 1.4487063 Class1\n47  1.6375912 1.5340261 Class2\n48  0.7405329 0.1986571 Class1\n49  1.7613585 0.8195439 Class1\n50  3.5785367 1.9978231 Class1\n51  1.9807671 1.8069935 Class2\n52  3.0635699 2.8069258 Class2\n53  0.8709567 0.6201361 Class1\n54  2.7894378 1.9884252 Class2\n55  1.1929973 1.0374265 Class1\n56  1.6971850 1.9608512 Class2\n57  1.1329441 1.3521825 Class1\n58  1.1210380 0.3010300 Class1\n59  1.4959114 0.4014005 Class1\n60  2.4443374 2.1000257 Class2\n61  2.1653559 2.3988077 Class2\n62  1.9948128 1.7347998 Class1\n63  2.9673959 2.8054329 Class2\n64  1.8083332 0.7708520 Class1\n65  1.4243459 0.6981005 Class1\n66  2.2958749 1.9218425 Class2\n67  3.9190304 2.6030361 Class1\n68  2.9963335 2.5742628 Class2\n69  3.9247944 3.3042923 Class2\n70  1.2389557 0.6901961 Class1\n71  1.3487062 0.8819550 Class1\n72  1.6831528 1.3304138 Class1\n73  2.9836169 1.9992611 Class2\n74  1.5811148 1.0421816 Class1\n75  1.3579220 1.4183013 Class1\n76  3.1485419 1.4312029 Class1\n77  0.7373215 0.7481880 Class1\n78  1.5243478 1.2172207 Class1\n79  2.3111901 2.1055102 Class2\n80  1.9847841 1.9329808 Class2\n81  3.2748186 3.1664301 Class2\n82  2.8389339 2.0994044 Class2\n83  1.0641919 0.6627578 Class1\n84  2.8063707 1.9554472 Class2\n85  3.5825135 1.4616486 Class1\n86  2.1041960 1.8007171 Class2\n87  2.9007813 1.9890492 Class2\n88  1.6020883 1.7708520 Class2\n89  2.2487514 1.9350032 Class1\n90  1.2333820 1.9982593 Class2\n91  2.5728610 1.0530784 Class1\n92  1.7762621 0.7767012 Class1\n93  1.8303619 1.2073650 Class2\n94  1.9244405 0.4623980 Class1\n95  2.6838117 2.3153405 Class2\n96  1.0340391 0.2405492 Class1\n97  3.2157011 2.5890555 Class2\n98  2.0392915 1.9484130 Class2\n99  3.9320717 2.1419825 Class1\n100 1.4929386 1.4166405 Class2\n101 1.2013638 0.8808136 Class1\n102 1.2248285 1.3138672 Class1\n103 1.8965881 1.9813655 Class2\n104 0.9873975 0.5728716 Class1\n105 1.5960533 1.4668676 Class1\n106 1.2023312 0.6434527 Class2\n107 2.6810283 0.8615344 Class1\n108 1.0056867 1.2405492 Class1\n109 2.4456415 0.9242793 Class1\n110 1.1303729 0.8808136 Class1\n111 1.2032458 0.6324573 Class1\n112 2.2004313 2.2049335 Class2\n113 1.5313184 1.6170003 Class2\n114 1.5040268 0.7767012 Class1\n115 1.4135101 0.8808136 Class1\n116 1.7117700 0.9542425 Class1\n117 1.2996303 1.0884905 Class1\n118 0.8959855 0.6928469 Class1\n119 1.5879767 0.5774918 Class1\n120 3.5637939 2.6303262 Class2\n121 1.5793687 0.5314789 Class1\n122 1.2078368 0.8082110 Class1\n123 1.3394318 1.5705429 Class1\n124 2.2858644 1.8192806 Class1\n125 1.4668087 1.2035768 Class2\n126 3.1297283 2.7296507 Class2\n127 1.0116499 1.0863598 Class1\n128 1.3483032 0.3424227 Class1\n129 1.5171731 0.2504200 Class2\n130 2.5619709 1.2940251 Class1\n131 1.2074461 0.8234742 Class1\n132 2.7654546 1.9147133 Class1\n133 1.8640556 1.6117233 Class2\n134 1.4773936 1.2764618 Class1\n135 1.1518518 0.7993405 Class1\n136 1.1840035 0.5314789 Class1\n137 2.7902697 1.1772478 Class2\n138 3.3445838 2.3581253 Class2\n139 2.7110593 1.6812412 Class1\n140 3.2158060 1.7168377 Class1\n141 1.4569120 0.6294096 Class1\n142 3.3067498 1.3673559 Class1\n143 1.0723317 0.8651040 Class1\n144 3.8208835 3.1964802 Class2\n145 1.3532269 0.9956352 Class1\n146 3.4248252 2.8880109 Class2\n147 2.4776203 1.8018837 Class2\n148 0.8392408 0.8195439 Class1\n149 1.8353843 1.0499929 Class1\n150 2.4975812 2.1532049 Class2\n151 1.3238427 1.2718416 Class2\n152 2.8313033 1.7958800 Class1\n153 1.7589877 0.1461280 Class1\n154 3.7999698 2.6101276 Class2\n155 1.2562615 0.9493900 Class1\n156 0.4443868 0.5314789 Class1\n157 2.3662909 2.8748875 Class2\n158 1.1656436 1.2469907 Class1\n159 3.3114858 2.1467480 Class1\n160 1.9640393 1.0000000 Class1\n161 0.4400599 0.3802112 Class1\n162 2.2292064 1.6009729 Class1\n163 1.1173990 1.3149201 Class1\n164 0.9544488 0.5622929 Class1\n165 1.7352566 1.7058637 Class2\n166 1.0326433 1.6605809 Class2\n167 3.7757727 3.2428171 Class2\n168 1.5158263 1.2405492 Class1\n169 2.0401250 0.9138139 Class1\n170 0.9117613 0.6901961 Class1\n171 3.8704353 2.5016069 Class1\n172 0.9807156 0.9344985 Class1\n173 3.6351337 2.5363059 Class2\n174 3.2025169 1.2169572 Class1\n175 1.0243016 0.9637878 Class1\n176 2.6667111 2.4279727 Class2\n177 1.7100831 0.9637878 Class1\n178 2.0708183 1.6512780 Class1\n179 2.5294512 2.2657374 Class1\n180 1.5484864 1.8438554 Class1\n181 1.6662424 1.1728947 Class1\n182 2.5343203 2.0011277 Class2\n183 1.9123481 1.6138418 Class2\n184 0.4477367 0.1461280 Class1\n185 1.1055698 0.5575072 Class1\n186 2.3865400 0.8567289 Class1\n187 3.6460259 3.2071495 Class2\n188 3.6346569 2.9005637 Class2\n189 1.1149407 1.2111205 Class1\n190 1.1477690 1.1673173 Class1\n191 2.3495452 2.0312872 Class1\n192 3.0649622 2.4375920 Class2\n193 1.3765981 1.2073650 Class1\n194 0.8837234 0.8987252 Class1\n195 2.9515553 1.0128372 Class1\n196 0.9772291 1.3096302 Class1\n197 0.9899970 0.8920946 Class1\n198 1.4076970 0.7993405 Class1\n199 3.0710733 3.3367398 Class1\n200 3.8137607 2.6920534 Class2\n201 3.1479818 2.6895752 Class2\n202 1.9192259 1.8280151 Class1\n203 1.7501617 2.0599419 Class2\n204 3.8090487 3.3140127 Class2\n205 0.9341354 0.8195439 Class1\n206 0.8759789 0.6127839 Class1\n207 2.8916416 2.0761305 Class1\n208 1.0786219 1.0253059 Class1\n209 2.6253589 1.6776070 Class1\n210 1.6432847 1.5733358 Class1\n211 1.5938699 1.0827854 Class1\n212 3.4458242 2.4774107 Class1\n213 1.3396227 0.9395193 Class1\n214 2.9548306 1.3974185 Class1\n215 1.0839858 0.4623980 Class1\n216 1.2995813 1.8161087 Class2\n217 2.2041739 1.2185355 Class2\n218 3.4746941 2.5680843 Class2\n219 0.6247626 0.6589648 Class1\n220 0.5653138 0.6857417 Class1\n221 1.4914606 1.1105897 Class1\n222 1.4682915 0.8627275 Class1\n223 2.6417854 1.8426092 Class1\n224 0.9306435 0.9052560 Class1\n225 1.1572432 1.1139434 Class1\n226 3.6991888 2.4360990 Class2\n227 2.2327158 1.5428254 Class1\n228 1.9802622 1.6935511 Class2\n229 0.7220149 0.5453071 Class1\n230 2.2386974 1.6273659 Class2\n231 1.6136453 1.7067178 Class2\n232 3.4603894 1.1442628 Class1\n233 1.2728221 1.7218106 Class1\n234 2.7989636 2.7087607 Class2\n235 3.0944786 2.6738500 Class2\n236 0.7696885 1.0842187 Class1\n237 1.2781279 0.8325089 Class1\n238 1.0069283 0.7923917 Class2\n239 3.3884220 2.0484806 Class1\n240 1.7526197 1.2528530 Class2\n241 0.8741253 1.0681859 Class1\n242 1.9827702 1.9469433 Class2\n243 2.4919443 2.3070680 Class2\n244 1.1187232 1.1430148 Class2\n245 3.2448652 1.4440448 Class1\n246 1.2765356 0.6532125 Class1\n247 1.8188421 1.1789769 Class1\n248 2.7632284 1.9674544 Class2\n249 2.5694675 2.4068807 Class2\n250 1.6301542 1.5751878 Class1\n251 1.2845048 1.2405492 Class2\n252 3.9139629 1.7617775 Class1\n253 1.5313808 1.3372595 Class2\n254 1.8873971 2.2030329 Class2\n255 1.7873598 1.9566486 Class2\n256 3.8157835 2.1628331 Class2\n257 3.8906656 3.6849620 Class2\n258 1.7206383 1.7944880 Class2\n259 1.0349981 0.6232493 Class1\n260 1.1221941 1.0827854 Class1\n261 2.6117717 1.5611014 Class1\n262 2.2002561 0.9772662 Class1\n263 1.8207007 1.9811388 Class2\n264 2.1018028 2.2245331 Class2\n265 1.4235361 1.6776070 Class2\n266 2.0655373 1.7885219 Class2\n267 2.4438545 0.6444386 Class1\n268 3.4423555 2.8584530 Class2\n269 1.6983763 1.2405492 Class1\n270 3.6905673 1.4225898 Class1\n271 2.0345106 1.4899585 Class1\n272 1.1289762 0.8853612 Class1\n273 2.3082328 2.1126720 Class2\n274 2.2067191 1.0000000 Class1\n275 0.8827373 0.7634280 Class1\n276 1.8034647 2.2350232 Class2\n277 1.9306878 1.2851070 Class2\n278 1.5519759 1.3180633 Class2\n279 1.2343039 0.1461280 Class1\n280 3.4090302 2.6605809 Class2\n281 1.4626134 1.1058507 Class1\n282 1.2101792 0.8082110 Class1\n283 1.0385384 0.9731279 Class1\n284 2.9874710 0.8698182 Class1\n285 1.3341740 1.1205739 Class1\n286 2.5084662 1.1743506 Class1\n287 2.9365921 2.7404416 Class2\n288 2.9779156 2.3144992 Class2\n289 2.2311212 0.8651040 Class1\n290 1.1475253 1.2671717 Class1\n291 1.7589179 0.8388491 Class1\n292 2.8584869 2.4299137 Class2\n293 1.4377749 0.5740313 Class1\n294 1.5202406 0.8129134 Class1\n295 2.7572733 1.0863598 Class1\n296 1.9206117 0.9633155 Class1\n297 2.3561981 1.9719713 Class2\n298 1.3714267 1.7094396 Class2\n299 1.3240837 1.2648178 Class2\n300 2.4370653 2.3481101 Class2\n301 2.5494506 2.4065402 Class2\n302 1.9495364 1.4733410 Class1\n303 1.1402788 0.7067178 Class1\n304 2.6770085 0.6222140 Class2\n305 1.7839966 1.3626709 Class2\n306 2.4688200 1.9572241 Class2\n307 1.5899308 1.3424227 Class1\n308 1.4813502 0.6655810 Class1\n309 0.6430136 0.3560259 Class1\n310 1.7154803 1.7831171 Class2\n311 2.9486505 1.3827373 Class1\n312 3.0607210 2.4769620 Class2\n313 2.3472500 2.1590255 Class1\n314 1.9268194 1.4623980 Class2\n315 1.9427961 1.0453230 Class1\n316 3.1060838 2.2846563 Class2\n317 3.0123071 2.2004674 Class2\n318 1.2735705 2.2891428 Class1\n319 2.5034291 1.9263424 Class2\n320 2.8000989 0.9623693 Class1\n321 0.9232960 1.1038037 Class1\n322 1.2999734 1.0362295 Class1\n323 2.5927205 2.2384727 Class2\n324 1.5817765 1.5774918 Class2\n325 1.5068485 1.1105897 Class1\n326 1.2367082 1.2741578 Class1\n327 2.0840434 1.2695129 Class1\n328 1.9305520 1.3664230 Class1\n329 1.7461657 0.8859263 Class1\n330 2.6688133 2.3059959 Class2\n331 2.9372107 2.8773713 Class2\n332 3.2031760 2.6675463 Class2\n333 1.7646803 1.9548212 Class2\n334 2.0160939 2.3669830 Class2\n335 1.1441641 1.1172713 Class1\n336 3.1245045 2.5582284 Class1\n337 3.0491795 1.9393195 Class1\n338 1.4306734 0.4313638 Class2\n339 1.7606442 1.4377506 Class1\n340 2.2230057 1.8668778 Class1\n341 2.7163400 0.9439889 Class1\n342 1.1529316 1.2552725 Class1\n343 2.6668456 1.4842998 Class1\n344 3.4436123 2.6664618 Class2\n345 1.5543314 1.3921691 Class2\n346 0.9739765 0.6730209 Class1\n347 2.1440744 2.3609719 Class2\n348 1.1102621 1.5954962 Class1\n349 1.4063539 0.7075702 Class2\n350 0.5744726 0.4983106 Class1\n351 3.0095308 2.4134674 Class2\n352 1.9719623 1.4927604 Class1\n353 3.7031329 1.0476642 Class1\n354 3.1211439 2.3508293 Class2\n355 0.4421235 0.8864907 Class1\n356 2.2974063 1.6538876 Class2\n357 3.1647721 2.5340261 Class2\n358 0.7510934 0.6384893 Class1\n359 1.7370163 1.4454485 Class2\n360 1.7175749 2.1687920 Class2\n361 1.6757704 0.7664128 Class1\n362 3.1772186 2.3554324 Class2\n363 1.6421173 1.1172713 Class2\n364 3.5534894 2.6337713 Class2\n365 1.8465831 1.7767012 Class2\n366 1.5235944 1.4983106 Class2\n367 1.0262596 0.8469553 Class1\n368 1.5307562 1.9210619 Class2\n369 1.2026187 0.6794279 Class1\n370 1.2698996 1.7932314 Class2\n371 2.7118170 0.8267225 Class1\n372 1.2655930 0.9527924 Class1\n373 3.3122066 3.0301138 Class2\n374 1.2200040 0.9030900 Class1\n375 0.4368185 0.1461280 Class1\n376 0.9770787 1.1205739 Class1\n377 2.9116540 2.7617024 Class2\n378 1.7858415 1.6364879 Class2\n379 3.6407692 1.4139700 Class1\n380 1.0348238 0.9804579 Class2\n381 2.2473613 0.9623693 Class2\n382 1.0251494 0.9493900 Class1\n383 2.7671557 1.3673559 Class1\n384 1.1320146 0.6954817 Class1\n385 3.4851491 2.5354207 Class2\n386 2.2261395 1.7032914 Class2\n387 3.1636151 2.9929068 Class2\n388 1.2837618 0.6127839 Class1\n389 0.7120883 0.9190781 Class1\n390 0.9254695 1.2430380 Class2\n391 0.9593218 0.7543483 Class1\n392 0.7656108 1.2253093 Class2\n393 0.4431342 1.4623980 Class1\n394 1.0663019 1.0310043 Class2\n395 3.6922255 1.7096939 Class1\n396 0.7437041 0.4149733 Class1\n397 3.7896577 2.9782718 Class2\n398 1.7968861 0.9344985 Class1\n399 2.9556390 1.7820424 Class1\n400 2.2355367 2.3803922 Class2\n401 2.4006036 1.9724805 Class1\n402 1.9207639 1.8457180 Class2\n403 2.8095029 1.6512780 Class1\n404 2.0770362 2.4432630 Class2\n405 2.2297733 2.6151080 Class2\n406 1.0593626 1.0207755 Class1\n407 1.6447375 1.2610248 Class1\n408 2.4493286 1.0655797 Class1\n409 0.9647263 0.2787536 Class1\n410 1.3120778 0.7853298 Class1\n411 3.0337673 2.4232459 Class2\n412 2.2346349 2.2753114 Class2\n413 2.1748831 0.7611758 Class1\n414 2.7453766 1.8836614 Class1\n415 1.7169794 1.3443923 Class1\n416 2.4098578 2.2340108 Class1\n417 1.7338149 1.9641181 Class2\n418 1.0501499 1.5258220 Class2\n419 1.4465241 1.2552725 Class1\n420 0.9507237 1.2095150 Class2\n421 1.4407566 0.9395193 Class1\n422 1.2913722 1.5132176 Class2\n423 1.2808606 1.3706981 Class2\n424 1.9929599 1.2988531 Class1\n425 2.4965744 2.4075608 Class2\n426 0.8145640 0.3617278 Class1\n427 1.8652219 1.6444386 Class1\n428 1.6480387 1.1583625 Class1\n429 2.2235762 1.8375884 Class2\n430 1.6783601 0.9513375 Class1\n431 1.6849686 1.4313638 Class2\n432 0.4438212 0.8195439 Class2\n433 2.8361585 2.5352941 Class1\n434 2.2665855 1.9643539 Class1\n435 1.5507043 0.8864907 Class1\n436 2.6107782 1.9237620 Class2\n437 0.9766273 0.6532125 Class1\n438 1.8663711 1.5831988 Class1\n439 0.6754108 0.5943926 Class1\n440 1.2785506 1.5786392 Class2\n441 1.3915648 0.8555192 Class1\n442 1.0394078 0.5797836 Class1\n443 1.6962861 0.7481880 Class1\n444 1.3264444 1.7632782 Class2\n445 3.3167452 2.7347998 Class2\n446 2.7957240 1.6334685 Class1\n447 3.3741792 2.7171127 Class2\n448 1.1408772 0.9867717 Class1\n449 1.5658440 1.6464037 Class2\n450 1.7420507 0.1731863 Class1\n451 2.7161581 2.5374413 Class2\n452 1.9713370 2.4007106 Class2\n453 2.9399550 2.4258601 Class2\n454 2.4612058 1.6560023 Class1\n455 1.5668580 0.6334685 Class1\n456 2.4236520 0.5440680 Class1\n457 2.7866363 2.5680843 Class2\n458 2.8193566 2.4723175 Class2\n459 3.6469615 2.7241939 Class2\n460 1.8007701 0.6334685 Class1\n461 1.0163580 0.7160033 Class1\n462 1.1122016 1.8407332 Class2\n463 3.5142229 1.3802112 Class1\n464 1.6486103 0.8864907 Class1\n465 1.4758771 1.5670264 Class2\n466 2.8087281 2.7722483 Class2\n467 2.9095826 1.9988694 Class2\n468 2.8771530 1.9726656 Class2\n469 1.2975701 1.4608978 Class2\n470 0.6545513 0.1731863 Class1\n471 3.2722012 2.4058584 Class2\n472 1.1914449 1.7619278 Class1\n473 1.5961723 2.0371475 Class2\n474 1.1215887 0.8129134 Class1\n475 1.8731789 1.8303962 Class2\n476 1.2461863 1.3767594 Class2\n477 0.8856938 0.5078559 Class1\n478 3.7120081 2.9551584 Class2\n479 2.8770649 2.3469395 Class2\n480 1.2385729 0.6560982 Class2\n481 0.6896951 0.1731863 Class1\n482 1.0189710 0.8692317 Class1\n483 2.0606855 1.3096302 Class1\n484 3.1033639 1.8299467 Class1\n485 1.1783590 1.6232493 Class2\n486 2.5659206 1.2455127 Class1\n487 3.0398046 2.7007037 Class2\n488 1.1645994 0.9395193 Class1\n489 0.6562144 0.6989700 Class1\n490 2.4449003 1.2405492 Class1\n491 3.7469087 2.9916690 Class2\n492 1.7844752 1.5682017 Class2\n493 3.3217884 1.8407332 Class1\n494 1.7176110 0.9960737 Class1\n495 3.5491143 2.6717281 Class1\n496 2.3416068 1.1367206 Class1\n497 1.3158886 1.2201081 Class1\n498 1.9203682 1.2164298 Class1\n499 3.5074215 1.2008505 Class1\n500 3.1655559 2.5902844 Class2\n501 3.7378928 2.6141270 Class2\n502 0.9026027 0.6627578 Class1\n503 1.4365269 0.7403627 Class1\n504 2.4153317 0.9138139 Class1\n505 1.2728221 1.3424227 Class2\n506 3.1412655 2.8816699 Class2\n507 3.8447065 2.6632296 Class2\n508 1.5749832 1.7745170 Class1\n509 2.6123802 1.3463530 Class1\n510 2.0414353 1.5976952 Class2\n511 2.4777664 0.9938769 Class1\n512 2.5284270 2.1981070 Class2\n513 1.4056477 1.7831887 Class2\n514 1.5154051 1.0374265 Class1\n515 1.3043531 1.2174839 Class2\n516 0.9014072 1.0253059 Class1\n517 0.8420445 1.0572856 Class1\n518 2.5092682 2.1131743 Class2\n519 2.6776116 1.7781513 Class1\n520 2.2960229 1.9084850 Class2\n521 2.4467476 2.1021935 Class2\n522 2.3887742 1.9395193 Class1\n523 3.0999104 2.5742628 Class2\n524 0.5689145 0.4913617 Class1\n525 1.2897693 1.6042261 Class2\n526 1.4533698 1.2041200 Class1\n527 1.9623834 1.4756712 Class2\n528 2.2526772 1.1205739 Class2\n529 1.1542263 0.5797836 Class1\n530 1.0794700 0.7853298 Class1\n531 2.9865909 1.9871745 Class1\n532 1.1987721 1.5118834 Class2\n533 2.2694059 0.8512583 Class1\n534 2.1437035 2.0378248 Class2\n535 1.6107264 1.8970770 Class2\n536 2.0382112 1.7143298 Class2\n537 2.2435589 2.1713754 Class2\n538 1.1292502 0.7993405 Class1\n539 1.7533211 2.0955180 Class2\n540 2.1905697 1.5976952 Class2\n541 2.4070611 2.4308809 Class2\n542 3.4307631 2.8145805 Class2\n543 2.1702332 1.6857417 Class2\n544 1.8239794 1.8318698 Class2\n545 2.6655196 2.4109459 Class2\n546 3.1685599 2.9716005 Class1\n547 0.8773200 1.1492191 Class1\n548 1.4640570 1.0417873 Class1\n549 2.2226404 1.9106244 Class2\n550 2.5183574 2.2725378 Class2\n551 1.3700258 0.9030900 Class1\n552 2.6315881 2.4080703 Class2\n553 1.0853944 1.1003705 Class1\n554 1.3291376 1.5224442 Class2\n555 0.4255132 0.1461280 Class1\n556 1.1095689 1.4712917 Class1\n557 0.8023440 0.5932861 Class1\n558 2.9890903 2.4850112 Class2\n559 1.3966185 1.1634596 Class1\n560 2.4754957 1.9732202 Class2\n561 2.5677805 1.6304279 Class2\n562 1.9592847 1.7708520 Class2\n563 0.9203419 0.6159501 Class1\n564 1.2867068 1.2624511 Class1\n565 0.9330855 0.7427251 Class1\n566 2.2071948 1.8585372 Class1\n567 1.3642680 1.7881684 Class2\n568 0.4363319 0.8750613 Class1\n569 1.1013655 1.8662873 Class2\n570 3.3036787 3.0177051 Class2\n571 2.4088334 2.0170333 Class2\n572 1.4648437 0.9912261 Class2\n573 3.7316795 2.6496269 Class2\n574 1.6388586 1.8876173 Class2\n575 1.6904557 0.2430380 Class1\n576 1.2862864 0.6627578 Class1\n577 0.9962551 0.7015680 Class1\n578 1.8521994 2.1053398 Class2\n579 2.3297452 1.2273724 Class1\n580 1.4263459 1.5550944 Class2\n581 1.5873780 2.0191163 Class2\n582 1.6186063 0.8450980 Class1\n583 1.1764995 1.4377506 Class1\n584 1.3273106 0.9642596 Class2\n585 1.1939963 1.3802112 Class2\n586 1.8525707 2.2837534 Class2\n587 2.8422565 0.8129134 Class1\n588 0.8182333 1.0606978 Class2\n589 1.2833902 0.9324738 Class1\n590 3.5642941 2.5509618 Class2\n591 1.8771529 1.4199557 Class2\n592 1.9041941 1.8585372 Class2\n593 1.3121750 0.9211661 Class1\n594 1.7899993 1.9662356 Class2\n595 2.3085457 2.3163898 Class2\n596 1.6925555 0.9680157 Class1\n597 2.4987457 1.9746959 Class2\n598 2.9599731 2.3628593 Class2\n599 2.5184391 1.3384565 Class1\n600 3.3066292 3.0450099 Class2\n601 1.5301313 1.4082400 Class1\n602 2.9770939 1.7134905 Class1\n603 1.6340514 1.5998831 Class1\n604 1.3451461 1.2278867 Class2\n605 3.9249666 2.3502480 Class1\n606 1.2717487 0.9222063 Class1\n607 0.8029770 1.5415792 Class2\n608 1.4940515 1.2878017 Class2\n609 1.3976959 1.1931246 Class2\n610 3.3105643 2.2861644 Class1\n611 2.7294204 0.1760913 Class1\n612 3.6146231 1.7902852 Class1\n613 1.5147731 1.8744818 Class2\n614 2.1594082 1.9595660 Class2\n615 1.9850140 1.9705793 Class2\n616 2.5692683 1.4563660 Class1\n617 2.7866081 1.4881275 Class1\n618 0.4122137 0.6232493 Class1\n619 2.9262528 1.9592322 Class1\n620 1.6656410 0.9242793 Class1\n621 2.0766404 2.1516762 Class2\n622 2.5903238 1.0211893 Class1\n623 2.9572096 2.4563660 Class2\n624 3.0421990 1.9628427 Class1\n625 1.2533825 1.6493349 Class2\n626 1.2753154 1.6190933 Class1\n627 1.8707816 1.8285955 Class1\n628 2.4577289 1.9237620 Class2\n629 2.0693843 1.9602329 Class2\n630 1.4321950 0.8188854 Class1\n631 1.3626254 0.6334685 Class1\n632 1.2182200 1.1818436 Class1\n633 2.9024715 1.2624511 Class2\n634 1.2663200 0.8260748 Class1\n635 1.2004480 0.7558749 Class1\n636 1.1560048 0.9758911 Class1\n637 1.9657951 0.5440680 Class1\n638 2.9641222 1.9815015 Class1\n639 2.5327327 1.7730547 Class1\n640 1.9826263 1.6711728 Class2\n641 3.7731220 2.5622572 Class2\n642 0.4457194 0.8808136 Class1\n643 3.3004269 1.7291648 Class1\n644 3.0243532 1.8305887 Class1\n645 0.8916230 0.8463371 Class1\n646 3.2565459 3.0462219 Class2\n647 1.1570548 0.8299467 Class1\n648 1.8361375 1.2866810 Class1\n649 1.4212841 1.6009729 Class2\n650 1.3833721 1.0863598 Class1\n651 3.7863123 2.0145205 Class1\n652 1.2059864 0.8061800 Class1\n653 2.2118935 2.3912880 Class2\n654 2.3947421 2.3430145 Class2\n655 1.7190535 0.8692317 Class1\n656 2.8112836 2.7752463 Class2\n657 2.8901596 0.8363241 Class1\n658 1.2140233 1.5390761 Class1\n659 2.2733987 1.4358444 Class1\n660 2.2736586 1.5428254 Class1\n661 3.4391079 2.8031839 Class2\n662 1.2221988 0.8293038 Class1\n663 3.4232949 2.3590572 Class2\n664 3.0887466 2.5621739 Class2\n665 1.9501297 1.6732974 Class1\n666 3.8317600 2.8646890 Class1\n667 3.8665085 3.8097078 Class2\n668 1.3782249 1.6972293 Class2\n669 1.5006033 0.4313638 Class1\n670 0.8579010 0.8082110 Class1\n671 1.0435167 1.0827854 Class1\n672 2.5812045 2.3727279 Class2\n673 2.0056392 1.9579423 Class2\n674 0.9709290 1.1818436 Class2\n675 2.0844888 1.6866363 Class2\n676 2.3269393 2.1783149 Class2\n677 1.1593946 0.4471580 Class1\n678 2.6127277 2.5819497 Class2\n679 3.6185069 2.3203540 Class2\n680 0.7949644 0.9493900 Class2\n681 1.3738517 0.8260748 Class1\n682 1.2210373 0.7853298 Class1\n683 2.0519417 2.2281436 Class2\n684 2.2482957 1.8195439 Class2\n685 3.8956997 2.7062055 Class2\n686 1.9037308 1.7528164 Class1\n687 1.1163498 0.7986506 Class1\n688 0.8314253 0.9299296 Class1\n689 1.2562615 1.2600714 Class1\n690 2.0867522 1.6785184 Class2\n691 1.2847771 0.9365137 Class2\n692 2.2968363 2.4586378 Class1\n693 2.0432617 2.2523675 Class2\n694 3.5598808 2.6760531 Class2\n695 1.4792112 0.9344985 Class1\n696 3.0207918 2.4667194 Class1\n697 2.2930166 2.3664230 Class2\n698 1.4695768 1.7986506 Class2\n699 2.3279811 1.1430148 Class1\n700 3.7552408 2.8531504 Class2\n701 1.6927764 1.9881128 Class2\n702 0.5330984 0.4842998 Class1\n703 1.7679412 1.9484130 Class2\n704 1.1555200 0.5440680 Class1\n705 1.8394065 0.9965117 Class1\n706 1.0564122 0.6989700 Class1\n707 3.6926171 2.6739420 Class2\n708 1.2386750 1.1072100 Class1\n709 0.9313548 0.4471580 Class1\n710 3.8649648 3.2504688 Class2\n711 3.2868462 2.7579272 Class2\n712 1.2297918 0.8195439 Class1\n713 2.2673595 1.7846173 Class2\n714 2.5461431 2.2440296 Class2\n715 0.6064050 0.3710679 Class1\n716 2.3450891 1.6910815 Class2\n717 0.8664658 0.7242759 Class1\n718 1.9561922 0.1731863 Class1\n719 2.8283457 1.9439889 Class1\n720 1.1181164 0.7853298 Class1\n721 0.9005572 0.4424798 Class1\n722 2.3931306 2.2232363 Class2\n723 1.1564895 1.0453230 Class1\n724 1.7285682 1.3342526 Class2\n725 0.8355827 1.4189638 Class2\n726 1.5791477 1.0644580 Class1\n727 2.4432192 2.0919130 Class2\n728 1.1373926 1.0492180 Class2\n729 0.9314476 0.3159703 Class1\n730 3.2838530 2.1105897 Class1\n731 0.4358857 0.1461280 Class1\n732 1.9953680 1.6414741 Class2\n733 1.5132551 1.0685569 Class1\n734 3.4703838 2.2046625 Class2\n735 2.0151578 1.5965971 Class2\n736 3.8437696 2.8734020 Class2\n737 3.0406076 2.5281451 Class2\n738 3.2526139 2.6863681 Class2\n739 3.2653139 2.5808110 Class2\n740 3.1180518 2.6266483 Class2\n741 1.6224410 0.6627578 Class1\n742 3.9162166 2.7304593 Class2\n743 0.7709778 0.8926510 Class1\n744 3.5669606 2.4896773 Class1\n745 3.6145237 3.7008335 Class2\n746 1.6996947 0.6875290 Class1\n747 1.7269218 2.0909631 Class2\n748 3.3957607 1.4727564 Class1\n749 0.4284474 0.7708520 Class1\n750 3.4720916 2.5848173 Class2\n751 0.6256078 0.5185139 Class1\n752 1.4979607 1.2124540 Class1\n753 2.8440403 1.8698182 Class1\n754 3.5183215 2.6952189 Class2\n755 3.2775229 1.2060159 Class1\n756 1.9068159 2.2523675 Class2\n757 1.1876231 0.8512583 Class1\n758 2.0840041 1.7774268 Class2\n759 1.3394080 1.1303338 Class1\n760 1.5133395 0.9637878 Class1\n761 3.6674849 3.5408048 Class2\n762 1.7230837 1.2833012 Class1\n763 2.8020106 1.1795518 Class1\n764 1.8614153 1.5910646 Class2\n765 1.7975625 1.3010300 Class1\n766 2.6351574 0.5453071 Class1\n767 1.2377302 1.3710679 Class2\n768 1.3129525 0.9912261 Class1\n769 1.6956070 0.9057959 Class1\n770 3.3672385 2.4913617 Class2\n771 2.7038385 1.9834007 Class2\n772 3.1805539 2.6905505 Class2\n773 0.9610847 0.9637878 Class1\n774 3.2942366 1.9787282 Class2\n775 2.3255588 2.3290926 Class2\n776 1.5399711 1.0740847 Class1\n777 2.2155005 2.3053514 Class2\n778 1.2841086 1.5959369 Class2\n779 1.8658924 2.0827854 Class2\n780 3.7907533 2.3803922 Class2\n781 2.5775445 1.8591383 Class1\n782 2.7607717 2.2655253 Class1\n783 0.7912798 0.6127839 Class1\n784 2.2768787 1.8536982 Class2\n785 3.9145490 3.0401354 Class1\n786 2.8016574 2.5282738 Class2\n787 1.7915633 1.0923697 Class1\n788 3.9320717 2.4726102 Class1\n789 3.8705828 2.6044421 Class2\n790 2.9787360 1.0413927 Class2\n791 1.9074935 0.6532125 Class1\n\n\nThe purpose of this example is to illustrate evaluation mechanics rather than data preprocessing challenges. Although the dataset itself is simple, the evaluation principles demonstrated here extend to more complex datasets and other supported tasks in fastml, subject to their respective assumptions and constraints.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#what-you-do-not-need-to-do-in-fastml",
    "href": "01-basic-classification.html#what-you-do-not-need-to-do-in-fastml",
    "title": "01. Basic Classification",
    "section": "What you do not need to do in fastml",
    "text": "What you do not need to do in fastml\nBefore showing the workflow, it is important to be explicit.\nIn fastml, users are not required to:\n\nmanually assemble train–test splits,\nexplicitly construct preprocessing recipes,\napply scaling or imputation outside the resampling loop,\ncompose workflows from loosely coupled components,\ncontrol when preprocessing is trained relative to resampling,\ndirectly manipulate resampling objects during model execution.\n\nThese steps are common entry points for data leakage.\nBy default, fastml executes preprocessing, model fitting, and evaluation within a single, resampling-aware structure. While advanced users may override specific components, the standard execution path is designed to preserve training–assessment isolation without relying on user discipline.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#declaring-intent",
    "href": "01-basic-classification.html#declaring-intent",
    "title": "01. Basic Classification",
    "section": "Declaring intent",
    "text": "Declaring intent\nIn fastml, the user specifies what is to be evaluated rather than manually assembling the individual components of a modeling pipeline.\nAt a minimum, this specification includes:\n\nthe dataset,\nthe outcome variable,\nthe set of algorithms to be evaluated,\nthe intended resampling strategy.\n\n\nlibrary(fastml)\n\nfit &lt;- fastml(\n  data       = two_class_dat,\n  label      = \"Class\",\n  algorithms = c(\"rand_forest\", \"xgboost\"),\n  resampling = \"cv\",\n  folds      = 5,\n)\n\nThis call defines the full evaluation setup under the default execution path. Model fitting, preprocessing, and performance estimation are carried out internally according to the declared intent and the constraints imposed by fastml.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#what-happens-internally",
    "href": "01-basic-classification.html#what-happens-internally",
    "title": "01. Basic Classification",
    "section": "What happens internally",
    "text": "What happens internally\nThe behavior described below reflects the default execution path in fastml and is not exposed for routine user configuration.\nFor each resampling split:\n\ntraining data are defined according to the resampling specification,\nany preprocessing steps are estimated using the training data only,\nmodels are fitted on the resulting training set,\npredictions are generated for the corresponding assessment set,\nperformance metrics are computed exclusively on assessment data.\n\nAs an additional safety measure, fastml performs internal checks on the resampling structure. If a resample is detected in which the training set coincides with the full dataset, execution is halted and the run is flagged as unsafe.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#why-this-differs-from-typical-workflows",
    "href": "01-basic-classification.html#why-this-differs-from-typical-workflows",
    "title": "01. Basic Classification",
    "section": "Why this differs from typical workflows",
    "text": "Why this differs from typical workflows\nIn many machine learning frameworks:\n\npipelines that violate evaluation assumptions can be constructed,\ncorrect evaluation depends largely on user discipline and correct assembly of components,\nviolations such as preprocessing leakage may execute without explicit warnings.\n\nIn fastml:\n\ncommon classes of incorrect evaluation pipelines are restricted along the default execution path,\nmethodological correctness is less dependent on user assembly decisions,\nkey evaluation invariants are checked and enforced during execution.\n\nThis reflects a deliberate design trade-off: reducing flexibility in pipeline construction in order to lower the risk of undetected evaluation errors.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#inspecting-results",
    "href": "01-basic-classification.html#inspecting-results",
    "title": "01. Basic Classification",
    "section": "Inspecting results",
    "text": "Inspecting results\nOnce execution is complete, performance estimates can be accessed directly from the fitted object.\n\nfit$performance$rand_forest$ranger\n\n# A tibble: 7 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.818\n2 kap       binary         0.629\n3 sens      binary         0.852\n4 spec      binary         0.775\n5 precision binary         0.824\n6 f_meas    binary         0.838\n7 roc_auc   binary         0.874\n\nfit$performance$xgboost$xgboost\n\n# A tibble: 7 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.811\n2 kap       binary         0.617\n3 sens      binary         0.841\n4 spec      binary         0.775\n5 precision binary         0.822\n6 f_meas    binary         0.831\n7 roc_auc   binary         0.864\n\n\nThe reported metrics summarize performance estimates obtained under 5-fold cross-validation for each model.\nFor both the random forest and xgboost models, these estimates are computed on assessment data that are held out from model fitting within each resampling split. They therefore differ from training-set performance and are not derived from post-hoc adjustments or recalibration.\nIn this example, the random forest model attains slightly higher average performance across most metrics, including accuracy, Cohen’s kappa, F1 score, and ROC AUC. However, the differences relative to xgboost are modest, and both models display a similar balance between sensitivity and specificity. These results indicate comparable overall performance with small differences in error structure rather than a clear dominance of one model over the other.\nAll reported values arise directly from the resampling-based evaluation procedure used by fastml, in which preprocessing, model fitting, and performance estimation are executed within a single, resampling-aware structure. The metrics therefore represent cross-validated performance summaries under the declared evaluation setup, subject to the usual variability and limitations inherent to resampling-based estimates.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#fold-level-variability",
    "href": "01-basic-classification.html#fold-level-variability",
    "title": "01. Basic Classification",
    "section": "Fold-Level variability",
    "text": "Fold-Level variability\n\nfit$resampling_results$`rand_forest (ranger)`$folds\n\n# A tibble: 35 × 4\n   fold  .metric   .estimator .estimate\n   &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n 1 1     accuracy  binary         0.850\n 2 1     kap       binary         0.699\n 3 1     sens      binary         0.843\n 4 1     spec      binary         0.860\n 5 1     precision binary         0.881\n 6 1     f_meas    binary         0.861\n 7 1     roc_auc   binary         0.874\n 8 2     accuracy  binary         0.890\n 9 2     kap       binary         0.779\n10 2     sens      binary         0.871\n# ℹ 25 more rows\n\nfit$resampling_results$`xgboost (xgboost)`$folds\n\n# A tibble: 35 × 4\n   fold  .metric   .estimator .estimate\n   &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n 1 1     accuracy  binary         0.843\n 2 1     kap       binary         0.685\n 3 1     sens      binary         0.814\n 4 1     spec      binary         0.877\n 5 1     precision binary         0.891\n 6 1     f_meas    binary         0.851\n 7 1     roc_auc   binary         0.875\n 8 2     accuracy  binary         0.866\n 9 2     kap       binary         0.730\n10 2     sens      binary         0.871\n# ℹ 25 more rows\n\n\nPerformance estimates vary across resampling folds, across metrics, and across models.\nIn this example, both the random forest and xgboost models exhibit noticeable but moderate fold-to-fold variability. Accuracy, sensitivity, specificity, and agreement-based measures such as Cohen’s kappa fluctuate across folds for both models, reflecting differences in class composition and difficulty across resampling splits.\nAcross folds, the two models show broadly comparable behavior. The random forest model attains slightly higher values in some folds, while the xgboost model matches or closely tracks its performance in others. For both models, sensitivity and specificity remain relatively balanced across folds, and no systematic metric asymmetry or degenerate behavior is observed. Variation in kappa and accuracy remains within a range consistent with expected resampling variability rather than indicating instability.\nThis pattern illustrates that, even when aggregated summaries suggest similar overall performance, fold-level inspection remains necessary to assess stability, class-wise trade-offs, and the influence of individual resampling splits. Such variability is an inherent feature of resampling-based evaluation and cannot be fully characterized by a single averaged estimate.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#model-comparison",
    "href": "01-basic-classification.html#model-comparison",
    "title": "01. Basic Classification",
    "section": "Model comparison",
    "text": "Model comparison\n\nsummary(fit)\n\n\n===== fastml Model Summary =====\nTask: classification \nNumber of Models Trained: 2 \nBest Model(s): rand_forest (ranger) (accuracy: 0.8176101) \n\nPerformance Metrics (Sorted by accuracy):\n\n---------------------------------------------------------------------------------------------- \nModel         Engine   Accuracy  F1 Score  Kappa  Precision  Sensitivity  Specificity  ROC AUC \n---------------------------------------------------------------------------------------------- \nrand_forest*  ranger   0.818     0.838     0.629  0.824      0.852        0.775        0.874   \nxgboost       xgboost  0.811     0.831     0.617  0.822      0.841        0.775        0.864   \n---------------------------------------------------------------------------------------------- \n(*Best model)\n\nBest Model hyperparameters:\n\nModel: rand_forest (ranger) \n  mtry: 1\n  trees: 500\n  min_n: 10\n\n\n===========================\nConfusion Matrices by Model\n===========================\n\nModel: rand_forest (ranger) \n---------------------------\n          Truth\nPrediction Class1 Class2\n    Class1     75     16\n    Class2     13     55\n\n\nThe summary output compares models evaluated under an identical resampling specification.\nIn this example, the random forest model attains slightly higher average performance across all reported metrics, including accuracy, F1 score, Cohen’s kappa, sensitivity, specificity, and ROC AUC. The xgboost model shows closely comparable performance, with modestly lower values across the same metrics. The differences between models are small and reflect incremental variations in error rates rather than qualitatively distinct error profiles.\nBecause all models are evaluated using the same resampling splits, observed performance differences can be attributed to the modeling approaches rather than to variation in data partitioning. This consistency is not merely a convenience; it is a methodological requirement for meaningful model comparison under resampling-based evaluation.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#what-is-guaranteed-here",
    "href": "01-basic-classification.html#what-is-guaranteed-here",
    "title": "01. Basic Classification",
    "section": "What is guaranteed here",
    "text": "What is guaranteed here\nSubject to the constraints accepted along the default execution path, fastml provides the following assurances:\n\npreprocessing steps are estimated separately within each resampling split, preventing information flow across folds,\nmodel training and evaluation are carried out on disjoint data subsets within each split,\nall algorithms are evaluated using an identical resampling structure,\nreported performance metrics correspond to resampling-based estimates of out-of-sample performance.\n\nThese properties arise from the architectural design of fastml rather than from user-enforced conventions. They reflect enforced evaluation invariants under standard usage, not informal best-practice recommendations.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#what-fastml-cannot-guarantee",
    "href": "01-basic-classification.html#what-fastml-cannot-guarantee",
    "title": "01. Basic Classification",
    "section": "What fastml cannot guarantee",
    "text": "What fastml cannot guarantee\nAs emphasized in the accompanying manuscript, fastml does not and cannot guarantee:\n\nthat outcome variables are correctly defined or scientifically meaningful,\nthat the raw features are free from prior leakage, measurement artifacts, or target contamination,\nthat the learning task itself addresses a scientifically relevant question,\nthat the chosen performance metrics are appropriate for the scientific or clinical context.\n\nMethodological safeguards can reduce certain classes of technical error, but they cannot substitute for domain expertise, sound study design, or scientific judgment.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#summary",
    "href": "01-basic-classification.html#summary",
    "title": "01. Basic Classification",
    "section": "Summary",
    "text": "Summary\nThis tutorial did not focus on constructing highly flexible modeling pipelines.\nInstead, it demonstrated how fastml limits common pathways to invalid evaluation by constraining how models are trained, evaluated, and compared along the default execution path.\nThe distinguishing characteristic of fastml is therefore not automation for its own sake, but the enforcement of evaluation invariants intended to reduce methodological errors in performance estimation.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "01-basic-classification.html#what-comes-next",
    "href": "01-basic-classification.html#what-comes-next",
    "title": "01. Basic Classification",
    "section": "What comes next",
    "text": "What comes next\n02. Multiple Models and Fair Comparison\nWhy comparing models is statistically invalid without shared resampling.\nConcept: Guarded Resampling (Revisited)\nA deeper look at how fastml enforces evaluation invariants.",
    "crumbs": [
      "Tutorials",
      "01. Basic Classification"
    ]
  },
  {
    "objectID": "C1-what-is-data-leakage.html",
    "href": "C1-what-is-data-leakage.html",
    "title": "C1 — What Is Data Leakage",
    "section": "",
    "text": "Data leakage occurs when information outside the data available at training time influences model fitting or evaluation in a way that would not be available at prediction time.\nThis definition is intentionally structural. Leakage is not about intent, carelessness, or misconduct. It is about when information is used.\nA model can be trained in good faith and still be invalidly evaluated.\nLeakage is a timing error, not a data error.\nMany descriptions frame leakage as a problem of using the test set. This framing is incomplete.\nLeakage occurs whenever a transformation, decision, or parameter is learned outside the resampling procedure, rather than within each resampling split.\nCommon examples include:\n\nscaling predictors using the full dataset prior to cross-validation,\nimputing missing values using global statistics,\nselecting features using all observations,\ntuning hyperparameters outside the resampling loop.\n\nIn each case, the mechanism is the same: information from assessment data is incorporated into training.",
    "crumbs": [
      "Concepts",
      "C1 — What Is Data Leakage"
    ]
  },
  {
    "objectID": "C1-what-is-data-leakage.html#the-problem-fastml-is-built-to-address",
    "href": "C1-what-is-data-leakage.html#the-problem-fastml-is-built-to-address",
    "title": "C1 — What Is Data Leakage",
    "section": "",
    "text": "Data leakage occurs when information outside the data available at training time influences model fitting or evaluation in a way that would not be available at prediction time.\nThis definition is intentionally structural. Leakage is not about intent, carelessness, or misconduct. It is about when information is used.\nA model can be trained in good faith and still be invalidly evaluated.\nLeakage is a timing error, not a data error.\nMany descriptions frame leakage as a problem of using the test set. This framing is incomplete.\nLeakage occurs whenever a transformation, decision, or parameter is learned outside the resampling procedure, rather than within each resampling split.\nCommon examples include:\n\nscaling predictors using the full dataset prior to cross-validation,\nimputing missing values using global statistics,\nselecting features using all observations,\ntuning hyperparameters outside the resampling loop.\n\nIn each case, the mechanism is the same: information from assessment data is incorporated into training.",
    "crumbs": [
      "Concepts",
      "C1 — What Is Data Leakage"
    ]
  },
  {
    "objectID": "C1-what-is-data-leakage.html#why-leakage-is-hard-to-detect",
    "href": "C1-what-is-data-leakage.html#why-leakage-is-hard-to-detect",
    "title": "C1 — What Is Data Leakage",
    "section": "Why Leakage Is Hard to Detect",
    "text": "Why Leakage Is Hard to Detect\nLeakage rarely produces errors or warnings.\nPipelines that contain leakage often:\n\nrun without failure,\nproduce stable estimates,\nyield optimistic performance values.\n\nAs a result, invalid pipelines may appear convincing.\nEven experienced practitioners can introduce leakage when workflows are assembled manually from modular components.",
    "crumbs": [
      "Concepts",
      "C1 — What Is Data Leakage"
    ]
  },
  {
    "objectID": "C1-what-is-data-leakage.html#a-minimal-example-conceptual",
    "href": "C1-what-is-data-leakage.html#a-minimal-example-conceptual",
    "title": "C1 — What Is Data Leakage",
    "section": "A Minimal Example (Conceptual)",
    "text": "A Minimal Example (Conceptual)\nConsider a dataset ( D ) evaluated using 5-fold cross-validation.\nIf a preprocessing step is estimated once using ( D ) and then applied within each fold, the model trained in Fold 1 has already been influenced by data from Folds 2–5.\nThis violates the independence assumption underlying cross-validation.\nThe resulting performance estimate is biased upward by construction.\nNo explicit misuse of a held-out test set is required.",
    "crumbs": [
      "Concepts",
      "C1 — What Is Data Leakage"
    ]
  },
  {
    "objectID": "C1-what-is-data-leakage.html#why-best-practices-are-not-enough",
    "href": "C1-what-is-data-leakage.html#why-best-practices-are-not-enough",
    "title": "C1 — What Is Data Leakage",
    "section": "Why “Best Practices” Are Not Enough",
    "text": "Why “Best Practices” Are Not Enough\nModern machine learning frameworks provide tools that can help avoid leakage. However, these safeguards are typically optional.\nCorrect evaluation therefore depends on users:\n\nidentifying which steps must be resampling-aware,\nassembling components in the correct order,\navoiding convenience-driven shortcuts.\n\nIncorrect pipelines are not generally prevented from executing.\nAs a result, methodological validity often depends on user discipline rather than on structural guarantees.",
    "crumbs": [
      "Concepts",
      "C1 — What Is Data Leakage"
    ]
  },
  {
    "objectID": "C1-what-is-data-leakage.html#what-follows-from-this",
    "href": "C1-what-is-data-leakage.html#what-follows-from-this",
    "title": "C1 — What Is Data Leakage",
    "section": "What Follows from This",
    "text": "What Follows from This\nIf leakage is:\n\neasy to introduce,\ndifficult to detect,\nand rarely signaled by software,\n\nthen preventing it cannot rely on guidelines alone.\nIt requires enforcement at the level of workflow design.\nThis motivates Guarded Resampling, introduced in the next concept.",
    "crumbs": [
      "Concepts",
      "C1 — What Is Data Leakage"
    ]
  },
  {
    "objectID": "C1-what-is-data-leakage.html#summary",
    "href": "C1-what-is-data-leakage.html#summary",
    "title": "C1 — What Is Data Leakage",
    "section": "Summary",
    "text": "Summary\n\nData leakage is a structural timing error.\nIt does not require misuse of a held-out test set.\nIt can produce stable but invalid results.\nOptional safeguards are insufficient.\nPreventing leakage requires architectural constraints, not user vigilance.\n\nNext: C2 — Why Most ML Pipelines Are Unsafe by Default",
    "crumbs": [
      "Concepts",
      "C1 — What Is Data Leakage"
    ]
  }
]