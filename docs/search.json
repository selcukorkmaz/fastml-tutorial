[
  {
    "objectID": "tutorials/fastml-regression.html",
    "href": "tutorials/fastml-regression.html",
    "title": "Predicting Continuous Outcomes",
    "section": "",
    "text": "In this tutorial, we‚Äôll use fastml to tackle a regression problem‚Äîspecifically, predicting house sale prices. Just like in classification, the fastml() function streamlines the entire modeling workflow for continuous targets with minimal setup.\nYou‚Äôll see how it:\nWhether you‚Äôre forecasting prices, estimating risk scores, or predicting any numeric outcome, fastml() makes regression modeling just as effortless and reproducible as classification. Let‚Äôs walk through a real-world example step by step."
  },
  {
    "objectID": "tutorials/fastml-regression.html#load-packages-and-data",
    "href": "tutorials/fastml-regression.html#load-packages-and-data",
    "title": "Predicting Continuous Outcomes",
    "section": "1. Load Packages and Data",
    "text": "1. Load Packages and Data\nIn this regression example, we‚Äôll use a medical dataset to predict a continuous outcome‚Äîspecifically, Body Mass Index (BMI) based on various clinical features. This simulates a common scenario in healthcare: estimating a patient‚Äôs health metric using routine measurements.\nWe‚Äôll use the Pima Indians Diabetes dataset, available via the mlbench package. It contains health data from adult female patients of Pima Indian heritage.\n\nlibrary(fastml)\nlibrary(dplyr)\nlibrary(mlbench)\n\ndata(PimaIndiansDiabetes)\n\n# Prepare dataset: rename 'mass' to 'BMI' and drop 'diabetes'\npima_reg &lt;- PimaIndiansDiabetes %&gt;%\n  rename(BMI = mass) %&gt;%\n  select(-diabetes) %&gt;%\n  filter(BMI &gt; 0)  # remove invalid BMI values (e.g., 0)\n\nhead(pima_reg)\n\n  pregnant glucose pressure triceps insulin  BMI pedigree age\n1        6     148       72      35       0 33.6    0.627  50\n2        1      85       66      29       0 26.6    0.351  31\n3        8     183       64       0       0 23.3    0.672  32\n4        1      89       66      23      94 28.1    0.167  21\n5        0     137       40      35     168 43.1    2.288  33\n6        5     116       74       0       0 25.6    0.201  30\n\n\nThe resulting dataset includes 768 rows and 8 predictor variables:\n\npregnant: Number of times pregnant,\nglucose: Plasma glucose concentration,\npressure: Diastolic blood pressure (mm Hg),\ntriceps: Triceps skin fold thickness (mm),\ninsulin: 2-hour serum insulin (ŒºU/mL),\nBMI: Body mass index (target variable here),\npedigree: Diabetes pedigree function\nage: Age in years\n\nIn the next step, we‚Äôll explore the dataset and launch a full regression workflow using fastml()."
  },
  {
    "objectID": "tutorials/fastml-regression.html#train-several-regression-models",
    "href": "tutorials/fastml-regression.html#train-several-regression-models",
    "title": "Predicting Continuous Outcomes",
    "section": "2. Train Several Regression Models",
    "text": "2. Train Several Regression Models\nWith the dataset prepared, we can now use fastml() to train and evaluate multiple regression models in a single step. Just like in classification, the function takes care of splitting the data, preprocessing, resampling, and hyperparameter tuning.\nIn this example, we‚Äôll train four common regression algorithms to predict BMI:\n\nLinear regression (linear_reg)\nSupport Vector Machine with radial kernel (svm_rbf)\nRandom Forest (rand_forest)\nLightGBM (lightgbm)\n\nLet‚Äôs run the full pipeline:\n\nresult &lt;- fastml(\n  data       = pima_reg,\n  label      = \"BMI\",\n  algorithms = c(\"linear_reg\", \"svm_rbf\", \"rand_forest\", \"lightgbm\")\n)\n\nWhat happens under the hood?\n\nRecipe: median‚Äëimputes NAs, one‚Äëhot‚Äëencodes categoricals, centres & scales numerics.\nResampling: 10‚Äëfold CV within the train split.\nTuning grids: automatically generated per algorithm.\nFinalisation: the best hyper‚Äëparameters are selected and the workflow refit on the full training data.\n\nYou don‚Äôt need to worry about creating resampling objects or preprocessing steps manually‚Äîeverything is handled internally, while remaining fully customizable via optional arguments.\nIn the next section, we‚Äôll examine and compare the performance of the trained regression models using summary metrics like RMSE and R-squared."
  },
  {
    "objectID": "tutorials/fastml-regression.html#compare-model-performance",
    "href": "tutorials/fastml-regression.html#compare-model-performance",
    "title": "Predicting Continuous Outcomes",
    "section": "3. Compare Model Performance",
    "text": "3. Compare Model Performance\nAfter training, you can evaluate how each model performed using the summary() function. This provides a comprehensive overview of model performance based on regression metrics such as:\n\nRMSE (Root Mean Squared Error): Measures average prediction error magnitude.\nR-squared (Coefficient of Determination): Indicates how well the model explains variability in the outcome.\nMAE (Mean Absolute Error): Average absolute difference between predicted and actual values.\n\nTo view the results:\n\nsummary(result, type = \"metrics\")\n\n\n===== fastml Model Summary =====\nTask: regression \nNumber of Models Trained: 4 \nBest Model(s): lightgbm (lightgbm) (rmse: 5.5434100) \n\nPerformance Metrics (Sorted by rmse ):\n\n------------------------------------------------------ \nModel        Engine    RMSE       R-squared  MAE       \n------------------------------------------------------ \nlightgbm*    lightgbm  5.5434100  0.4006765  4.0925296 \nrand_forest  ranger    5.7083442  0.3679268  4.2930432 \nsvm_rbf      kernlab   5.8889871  0.3302711  4.3937137 \nlinear_reg   lm        6.5682857  0.1621334  5.1909364 \n------------------------------------------------------ \n(*Best model)\n\n\nFrom this output, we see that the lightgbm model achieved the lowest RMSE, indicating the most accurate predictions overall. It also has the highest R-squared, suggesting it explains a greater portion of variance in BMI compared to other models.\nYou can also plot the performance of all models across metrics using:\n\nplot(result, type = \"bar\")\n\n\n\n\n\n\n\n\nThis generates a faceted bar plot, making it easy to compare RMSE, R-squared, and MAE visually.\nIn the next step, we‚Äôll take a closer look at the best model‚Äôs tuned hyperparameters."
  },
  {
    "objectID": "tutorials/fastml-regression.html#inspect-the-best-model",
    "href": "tutorials/fastml-regression.html#inspect-the-best-model",
    "title": "Predicting Continuous Outcomes",
    "section": "4. Inspect the Best Model",
    "text": "4. Inspect the Best Model\nOnce the models have been trained and evaluated, fastml() automatically identifies the best-performing model based on the optimization metric you specified (e.g., RMSE). You can inspect which model was selected and view its internal details using:\n\nresult$best_model_name\n\n  lightgbm \n\"lightgbm\" \n\n\nThis tells us that the best-performing model is a LightGBM gradient boosting machine.\nTo view the trained model‚Äôs workflow‚Äîincluding the preprocessing steps, model specification, and tuning results‚Äîuse:\n\nresult$best_model\n\n$`lightgbm (lightgbm)`\n‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: boost_tree()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n4 Recipe Steps\n\n‚Ä¢ step_zv()\n‚Ä¢ step_dummy()\n‚Ä¢ step_center()\n‚Ä¢ step_scale()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLightGBM Model (100 trees)\nObjective: regression\nFitted to dataset with 7 columns\n\n\nThis reveals:\n\nThe full tidymodels workflow, including the recipe and the fitted model.\nThe model engine and its finalized hyperparameters.\nA summary of the training process and resampling results.\n\nThis confirms that:\n\nPreprocessing included zero-variance filtering, dummy encoding (if applicable), centering, and scaling.\nThe model is a boosted tree trained with LightGBM, using 100 trees.\n\nYou can also retrieve the exact hyperparameters selected during tuning using:\n\nsummary(result, type = \"params\")\n\nBest Model hyperparameters:\n\nModel: lightgbm (lightgbm) \n  mtry: 2\n  trees: 100\n  min_n: 5\n  tree_depth: 3\n  learn_rate: 0.1\n  loss_reduction: 1\n  sample_size: 0.5\n\n\nThis level of detail is helpful for understanding model complexity, reproducibility, and for future deployment.\nIn the next section, we‚Äôll use the selected model to make predictions on new, unseen data."
  },
  {
    "objectID": "tutorials/fastml-regression.html#predict-on-new-observations",
    "href": "tutorials/fastml-regression.html#predict-on-new-observations",
    "title": "Predicting Continuous Outcomes",
    "section": "5. Predict on New Observations",
    "text": "5. Predict on New Observations\nOnce you‚Äôve identified and reviewed the best model, you can use it to make predictions on new, unseen data.\nStart by sampling a few observations from the dataset (or use an external dataset if available):\n\n# Sample 5 new observations\nnew_obs &lt;- pima_reg %&gt;% \n  slice_sample(n = 5) %&gt;% \n  dplyr::select(-BMI) \n\nnew_obs\n\n    pregnant glucose pressure triceps insulin pedigree age\n430        1      95       82      25     180    0.233  43\n76         1       0       48      20       0    0.140  22\n714        0     134       58      20     291    0.352  21\n389        5     144       82      26     285    0.452  58\n646        2     157       74      35     440    0.134  30\n\n\nUse the predict() function to generate predicted BMI values:\n\npredict(result, new_obs)\n\n[1] 22.82529 22.82529 22.82529 22.82529 22.82529"
  },
  {
    "objectID": "tutorials/fastml-regression.html#variable-importance-shap-values",
    "href": "tutorials/fastml-regression.html#variable-importance-shap-values",
    "title": "Predicting Continuous Outcomes",
    "section": "6. Variable Importance & SHAP Values",
    "text": "6. Variable Importance & SHAP Values\nIn medical and clinical applications, understanding how a model arrives at its prediction is often just as crucial as the prediction itself. For this reason, fastml provides SHAP (Shapley Additive Explanations) support via the fastexplain() function‚Äîallowing you to interpret individual-level predictions from your best model.\nTo compute and visualize SHAP values:\n\nfastexplain(result)\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  lightgbm \n  -&gt; data              :  605  rows  7  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  605  values \n  -&gt; predict function  :  predict_function \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package , ver. , task regression \n  -&gt; predicted values  :  numerical, min =  21.50534 , mean =  32.58835 , max =  50.39042  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -13.91933 , mean =  0.002388819 , max =  15.90049  \n  A new explainer has been created!  \n\n=== DALEX Variable Importance (with Boxplots) ===\n\n\n\n\n\n\n\n\n\n\n=== DALEX Shapley Values (SHAP) ===\n\n\n\n\n\n\n\n\n\nThis produces two key plots:\nFeature Importance (Permutation-Based)\n\nThis plot shows how much the model‚Äôs RMSE increases when each feature is permuted (i.e., randomly shuffled).\nThe larger the increase in error, the more important the feature is to the model‚Äôs performance.\nTriceps skinfold thickness is by far the most important feature, followed by glucose, blood pressure, and age.\n\nSHAP Values (Local Explainability)\n\nSHAP values break down each individual prediction into feature-level contributions.\nPositive SHAP values increase the predicted BMI; negative ones decrease it.\nIn this example:\n\nTriceps and pressure have strong positive contributions.\nPregnancy count and age tend to lower BMI predictions for certain individuals.\nGlucose and pedigree show more mixed or subtle effects.\n\n\nInterpretation Highlights\n\nTriceps consistently emerges as the most influential variable, both globally and locally.\nSome variables (like glucose) may be important for prediction accuracy (as seen in the permutation plot) but may not contribute uniformly across all patients (as reflected in SHAP values).\nSHAP plots also help identify non-linear or interaction effects‚Äîfor example, how pregnant affects predictions differently depending on other inputs."
  },
  {
    "objectID": "tutorials/fastml-advanced.html",
    "href": "tutorials/fastml-advanced.html",
    "title": "Advanced Workflows with fastml()",
    "section": "",
    "text": "# Why an Advanced Tutorial?\nfastml() handles 90‚ÄØ% of day‚Äëto‚Äëday modelling out‚Äëof‚Äëthe‚Äëbox, but real projects often need more control. This guide dives into the power‚Äëuser levers:\n\nCustom recipes (feature engineering, text / time features)\nAlternative hyper‚Äëparameter search strategies (Bayesian, adaptive racing)\nParallel and distributed engines (multi‚Äëcore, sparklyr, h2o)\nStacking / ensembling multiple fastml runs\nLearning curves and automated model monitoring\n\nWe assume you‚Äôve worked through the classification and regression tutorials.\n\n## 1 ¬∑ Set‚Äëup\nlibrary(fastml)\nlibrary(tidymodels)\nlibrary(dplyr)\n\ndata(\"credit_data\", package = \"modeldata\")\ncredit_data &lt;- credit_data %&gt;%\nmutate(Status = factor(Status, levels = c(\"good\", \"bad\")))\nWe‚Äôll predict credit status (binary classification). The dataset contains wide ordinals, categoricals, and numeric predictors ‚Äî perfect for advanced preprocessing.\n\n## 2 ¬∑ Custom Recipe\nBelow we:\n\nImpute missing numerics with KNN; categoricals with the mode.\nCreate interaction terms (Income √ó Limit).\nNormalize numeric predictors.\nCollapse infrequent factor levels (&lt;‚ÄØ3‚ÄØ%).\n\ncredit_rec &lt;- recipe(Status ~ ., data = credit_data) %&gt;%\n  step_impute_knn(all_numeric_predictors()) %&gt;%\n  step_impute_mode(all_nominal_predictors()) %&gt;%\n  step_interact(terms = ~ Income:Limit) %&gt;%\n  step_other(all_nominal_predictors(), threshold = 0.03) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nIf you supply recipe = credit_rec to fastml() internal preprocessing (imputation/encoding/scaling) is skipped, preventing conflicts.\n\n\n## 3 ¬∑ Tuning Strategies\n### 3.1 Bayesian Optimisation\nbayes_res &lt;- fastml(\ndata       = credit_data,\nlabel      = \"Status\",\nalgorithms = c(\"rand_forest\", \"xgboost\"),\nmetric     = \"roc_auc\",\nresampling_method = \"cv\",\nfolds      = 5,\nrecipe     = credit_rec,\ntuning_strategy   = \"bayes\",\ntuning_iterations = 40,      # iterations after the 10‚Äëpoint space‚Äëfilling start\nearly_stopping    = TRUE,    # stop when no improvement for 5 iters\nn_cores           = 6,\nseed              = 2025)\nBayesian search explores the hyper‚Äëparameter space efficiently, especially when evaluation is costly.\n\n3.2 Adaptive Racing (ANOVA)\nrace_res &lt;- fastml(\n  data       = credit_data,\n  label      = \"Status\",\n  algorithms = c(\"rand_forest\", \"xgboost\"),\n  recipe     = credit_rec,\n  metric     = \"roc_auc\",\n  adaptive   = TRUE,           # enables `finetune::tune_race_anova()`\n  tuning_strategy = \"grid\",   # initial grid; racing drops losers\n  folds      = 10,\n  n_cores    = 6,\n  seed       = 2025)\nRacing aggressively prunes poor combos early; handy for large grids.\n\n## 4 ¬∑ Parallel & Distributed Engines\n### 4.1 Multi‚Äëcore (doFuture)\nfastml() automatically parallelises when n_cores &gt; 1:\nlibrary(doFuture)\nregisterDoFuture()\n\nplan(multisession, workers = 8)  # or multicore on Linux/macOS\n\n\n4.2 Spark Cluster\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"spark://my‚Äëcluster:7077\")\n\nspark_res &lt;- fastml(\n  data       = credit_data,\n  label      = \"Status\",\n  algorithms = c(\"rand_forest\", \"linear_reg\"),\n  algorithm_engines = list(rand_forest = \"spark\",\n                           linear_reg  = \"spark\"),\n  metric     = \"roc_auc\",\n  n_cores    = 4)  # number of executor cores per worker\n\nSpark engines (sparklyr wrappers) offload model fitting to the cluster while fastml coordinates.\n\n\n\n4.3 H2O AutoML + fastml API\nlibrary(h2o)\nh2o.init(nthreads = -1)\n\nh2o_res &lt;- fastml(\n  data       = credit_data,\n  label      = \"Status\",\n  algorithms = c(\"rand_forest\", \"logistic_reg\"),\n  algorithm_engines = list(rand_forest  = \"h2o\",\n                           logistic_reg = \"h2o\"),\n  metric     = \"roc_auc\",\n  impute_method = \"h2o\",   # optional custom imputation function\n  n_cores    = 4)\n\n## 5 ¬∑ Learning Curves & Monitoring\nlc_res &lt;- fastml(\ndata           = credit_data,\nlabel          = \"Status\",\nalgorithms     = c(\"xgboost\"),\nlearning_curve = TRUE,\nfolds          = 5)\nThe built‚Äëin learning curve shows ROC AUC vs training fraction, helping spot under‚Äë/over‚Äëfitting.\n\n## 6 ¬∑ Ensembling / Stacking\nWhile fastml() returns individual models, you can ensemble their predictions easily.\npred_dfs &lt;- bayes_res$predictions  # nested list\n\n# Extract probabilities for the positive class across models\nprobs &lt;- purrr::map_df(pred_dfs, ~ .x[[1]][, c(\".pred_bad\")], .id = \"Model\") %&gt;%\n  bind_cols(truth = rep(credit_data$Status, times = length(pred_dfs)))\n\n# Simple average ensemble\nprobs$avg &lt;- probs %&gt;%\n  select(starts_with(\".pred_\")) %&gt;%\n  rowMeans()\n\nlibrary(yardstick)\nroc_auc(probs, truth = truth, avg)\nFor a tidy stacking pipeline use the stacks package and supply fastml workflows as candidates.\n\n## 7 ¬∑ Custom Metrics\nPass any yardstick-compatible summariser via summaryFunction.\ngeometric_mean &lt;- function(data, ...) {\n  yardstick::mcc(data, ...)  # Matthew‚Äôs correlation coefficient as example\n}\n\ncustom_res &lt;- fastml(\n  data      = credit_data,\n  label     = \"Status\",\n  algorithms = c(\"rand_forest\", \"xgboost\"),\n  summaryFunction = geometric_mean,\n  metric    = \"mcc\")\n\n## 8 ¬∑ Save & Reload Best Model\nbest_wf &lt;- bayes_res$best_model[[1]]\n\ntidymodels::write_rds(best_wf, \"models/best_credit_model.rds\")\n\n# later...\nloaded_wf &lt;- readr::read_rds(\"models/best_credit_model.rds\")\npredict(loaded_wf, new_data = credit_data[1:3, ])\n\n## 9 ¬∑ Frequently Asked Questions\n\n\n\n\n\n\n\nQuestion\nAnswer\n\n\n\n\n‚ÄúCan I tune across multiple engines for the same algorithm?‚Äù\nYes ‚Äì supply algorithm_engines = list(rand_forest = c(\"ranger\", \"partykit\")). Each engine is tuned independently.\n\n\n‚ÄúWhy does tuning seem slow?‚Äù\nCheck n_cores, reduce folds, or switch to adaptive racing.\n\n\n‚ÄúHow do I use a grouped time‚Äëseries CV?‚Äù\nPrepare rsample::vfold_cv object yourself and pass via resampling_method = \"none\", then supply resamples (coming in next release).\n\n\n\n\n## 10 ¬∑ Where to Next?\n\nExplainability: revisit fastexplain() with SHAP & PDPs on your best model.\nProduction: convert workflow to a vetiver model for API deployment.\nAutomated monitoring: schedule fastml re‚Äëtraining with GitHub Actions or cronR.\n\nHappy modelling!\n\n### Session Info\nsessionInfo()"
  },
  {
    "objectID": "tutorials/fastexplain.html",
    "href": "tutorials/fastexplain.html",
    "title": "Explaining Models",
    "section": "",
    "text": "Building accurate models is only part of the machine learning journey. In many real-world applications‚Äîespecially in healthcare, finance, and policy‚Äîmodel transparency and interpretability are equally critical.\nAfter training your models using the fastml() function, you can gain deeper insights into how your models make decisions using the fastexplain() function. This function offers a suite of visual and quantitative tools to help you:\n\nInterpret model predictions on a per-observation basis,\nAssess global feature importance,\nUnderstand the direction and magnitude of each feature‚Äôs influence,\nIdentify potential biases or instability in the model‚Äôs logic.\n\nUnlike black-box models that offer little explanation, fastexplain() provides interpretable outputs that are especially valuable in high-stakes domains where accountability matters.\nUnder the hood, fastexplain() is powered by the excellent DALEX package, enabling consistent, model-agnostic explanations across a wide range of algorithms‚Äîincluding tree-based models, linear models, boosting methods, and support vector machines.\nIn this tutorial, you‚Äôll learn how to:\n\nCreate explainers from trained fastml objects,\nVisualize feature importance using permutation methods,\nInterpret SHAP values to understand individual predictions,\nExplore optional parameters to fine-tune explanations.\n\n\nPurpose\nfastexplain() helps to answer questions such as:\n\nWhich features are the most important for model prediction?\nHow do features affect the predicted outcome?\nAre model predictions stable or sensitive to small changes in input?\n\n\n\nUsage\nexplanation &lt;- fastexplain(result, type = \"full\")\n\nKey Arguments:\n\nobject: The output object from fastml().\ntype: Type of explanation. Options are:\n\n\"importance\": Show variable importance only\n\n\"shap\": Show SHAP values only\n\"profiles\": Show model profile plots\n\"full\": Display all available explanation types\nmodel_index: Optional index if you want to explain a model other than the best one\n\n\n\n\nExample\nWe‚Äôll continue using the result object from the fastml() example in the previous tutorial:\nexplanation &lt;- fastexplain(result, type = \"full\")\nThis call produces:\n\nA variable importance plot (permutation-based)\nA SHAP summary plot\nModel profile plots (how individual features influence predictions)\n\n\n\nVisual Outputs\nEach of the outputs helps you understand different aspects:\n\nVariable Importance: Shows which variables contribute most to the model‚Äôs predictive performance.\nSHAP Summary: Breaks down individual predictions by showing how much each feature contributed.\nModel Profiles: Depicts the functional form of how features affect predictions (analogous to partial dependence plots).\n\n\n\nCustomization\nYou can pass additional arguments to customize behavior. For example:\nfastexplain(result, type = \"importance\", top_n = 10)\nLimits the explanation to the top 10 most important features.\n\n\nNotes\nMake sure that your data contains enough features and variation to compute reliable explanations. For very small datasets, some explanation techniques may produce unstable results.\n\n\nNext Steps\nIn the next tutorial, we‚Äôll explore how to perform model stacking and ensembling in fastml to further improve predictive performance."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fastml 0.6.1",
    "section": "",
    "text": "Welcome to fastml, your one-stop R package for lightning-fast model development, evaluation, and explainability‚Äîall in a single workflow."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "fastml 0.6.1",
    "section": "üíø Installation",
    "text": "üíø Installation\ninstall.packages(\"fastml\")"
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "fastml 0.6.1",
    "section": "üöÄ Get Started",
    "text": "üöÄ Get Started\nIf you‚Äôre new to fastml, begin with our Getting Started Guide"
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Getting Started with fastml",
    "section": "",
    "text": "Introduction\nThe fastml package provides a streamlined, user-friendly interface for training, evaluating, and comparing multiple machine learning models for classification or regression tasks. Built on top of the tidymodels ecosystem, it automates common steps like resampling, preprocessing, model tuning, and performance reporting, so you can focus on insights rather than infrastructure.\nThis tutorial will help you get started with fastml, explaining how to install the package, prepare your data, train multiple models in one command, and inspect their performance.\n\n\nInstallation\nInstall the latest stable version from CRAN:\ninstall.packages(\"fastml\")\nYou can install the development version from GitHub:\n# install.packages(\"devtools\")\ndevtools::install_github(\"selcukkorkmaz/fastml\")\nOnce installed, load the package:\nlibrary(fastml)\n\n\nExample Dataset\nTo illustrate how fastml works, we‚Äôll use the built-in iris dataset. We‚Äôll convert it into a binary classification task for simplicity:\nlibrary(dplyr)\ndata &lt;- iris %&gt;%\n  filter(Species != \"setosa\") %&gt;%\n  mutate(Species = factor(Species))\n\n\nRunning fastml()\nThe main function in this package is fastml(). It takes in a dataset and automates model training for a selected list of algorithms.\nresult &lt;- fastml(\n  data = data,\n  label = \"Species\",\n  task = \"classification\",\n  metric = \"accuracy\",\n  algorithms = c(\"logistic_reg\", \"rand_forest\", \"svm_linear\"),\n  nfolds = 5,\n  tune = TRUE,\n  seed = 123\n)\n\nKey Arguments Explained:\n\ndata: A data frame containing predictors and the outcome.\nlabel: The name of the outcome column (character).\ntask: Either ‚Äúclassification‚Äù or ‚Äúregression‚Äù.\nmetric: The performance metric to optimize (e.g., ‚Äúaccuracy‚Äù, ‚Äúrmse‚Äù).\nalgorithms: A vector of algorithm names. Use availableMethods(\"classification\") to see all.\nnfolds: Number of folds for cross-validation.\ntune: Whether to perform hyperparameter tuning (TRUE/FALSE).\nseed: Reproducibility seed.\n\n\n\n\nOutput\nThe result is a list with the following key elements:\n\nmodels: Nested list of fitted model workflows.\nperformance: Performance metrics for each model-engine pair.\npredictions: Predictions and class probabilities.\nbest_model_name: Best engine for each algorithm.\nbest_model: Finalized best model workflows.\n\n\n\nSummary of Results\nYou can generate a comprehensive summary with:\nsummary(result)\nThis command prints model-wise performance metrics, highlights the best model, and produces comparison plots.\n\n\nNext Steps\nOnce you‚Äôve trained models using fastml(), you can:\n\nUse summary() with plot = TRUE to visualize ROC curves, confusion matrices, and more.\nRun fastexplain() to compute variable importance and SHAP values (explained in another tutorial).\n\nIn the next tutorial, we will explain in detail how fastml handles model training, including resampling, engine selection, and hyperparameter tuning."
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Reference",
    "section": "",
    "text": "How to use this reference\n\n\n\n\n\n\nClick any function name to see its complete help page.\nSections are grouped so you can quickly locate functions for each step of the workflow.\n\nThese pages are generated from roxygen2 docs, so they stay in sync with each release."
  },
  {
    "objectID": "reference/index.html#core-workflow-verbs",
    "href": "reference/index.html#core-workflow-verbs",
    "title": "Reference",
    "section": "Core workflow verbs",
    "text": "Core workflow verbs\n\n\n\nFunction\nPurpose\n\n\n\n\nfastml()\nTrain, tune & compare many models in one call.\n\n\nfastexplore()\nOne-shot EDA: missings, outliers, correlations, plots.\n\n\nfastexplain()\nPermutation VI, SHAP values, profiles & calibration."
  },
  {
    "objectID": "reference/index.html#model-training-helpers",
    "href": "reference/index.html#model-training-helpers",
    "title": "Reference",
    "section": "Model-training helpers",
    "text": "Model-training helpers\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\ntrain_models()\nInternal engine powering fastml (multi-engine, tuning).\n\n\nevaluate_models()\nComputes metrics, collects predictions, highlights best.\n\n\navailableMethods()\nReturns keys for all supported algorithms."
  },
  {
    "objectID": "reference/index.html#specification-builders",
    "href": "reference/index.html#specification-builders",
    "title": "Reference",
    "section": "Specification builders",
    "text": "Specification builders\nAll exported so you can use them directly:\ndefine_rand_forest_spec() define_lightgbm_spec() define_xgboost_spec()\ndefine_decision_tree_spec() define_svm_linear_spec() define_svm_rbf_spec()\n‚Ä¶and the rest."
  },
  {
    "objectID": "reference/index.html#utility-helpers",
    "href": "reference/index.html#utility-helpers",
    "title": "Reference",
    "section": "Utility helpers",
    "text": "Utility helpers\n\n\n\n\n\n\n\nHelper\nWhat it does\n\n\n\n\nsanitize()\nClean column names / vectors (spaces ‚Üí _, remove /, etc.).\n\n\nget_default_engine()\nMap algorithm ‚Üí default engine.\n\n\nget_default_params()\nSensible defaults when you skip tuning.\n\n\nget_default_tune_params()\nLightweight grids when tune = TRUE.\n\n\nget_engine_names()\nExtract engine labels from nested workflows.\n\n\n\n\n\nLooking for step-by-step walkthroughs?\nVisit the Tutorials section for narrative, hands-on guides that combine many of the functions listed above."
  },
  {
    "objectID": "tutorials/fastexplore.html",
    "href": "tutorials/fastexplore.html",
    "title": "Exploring Data with fastexplore()",
    "section": "",
    "text": "# Introduction\nBefore building predictive models, it‚Äôs essential to understand your dataset. The fastexplore() function in the fastml package provides a quick and informative overview of your data. It computes descriptive statistics, detects missing values, highlights class imbalance, and gives early insights into feature distributions and correlations.\nIn this tutorial, we‚Äôll explore how to use fastexplore() to inspect datasets before model training.\n\nLoading the Package and Data\nWe‚Äôll use the classic PimaIndiansDiabetes dataset from the mlbench package to demonstrate.\nlibrary(fastml)\nlibrary(mlbench)\ndata(PimaIndiansDiabetes)\n\n# View structure\nstr(PimaIndiansDiabetes)\n\n\nUsing fastexplore()\nThe fastexplore() function expects a data frame and the name of the outcome column. It works for both classification and regression problems.\nexplore &lt;- fastexplore(\n  data = PimaIndiansDiabetes,\n  label = \"diabetes\"\n)\n\n\nOutput Summary\nThe result is a list with structured summaries:\n\nbasic: General info like number of rows, columns, and missing values.\ntarget: Outcome distribution.\nfeatures: Summary statistics for each feature.\ncorrelation: Feature correlation matrix (for numeric predictors).\n\nYou can inspect these components individually:\nexplore$basic\nexplore$target\nexplore$features\nexplore$correlation\n\n\nVisualizing Exploration Results\nfastexplore() also provides an optional argument plot = TRUE to visualize results.\nfastexplore(\n  data = PimaIndiansDiabetes,\n  label = \"diabetes\",\n  plot = TRUE\n)\nThis generates:\n\nA bar plot of outcome class distribution.\nHistograms of numerical features.\nA heatmap of pairwise correlations.\n\n\n\nWhen to Use fastexplore()\nUse fastexplore():\n\nRight after loading your dataset.\nBefore splitting the data.\nTo detect class imbalance or outliers.\nTo guide feature engineering decisions.\n\n\n\nConclusion\nThe fastexplore() function helps you quickly assess your dataset‚Äôs structure and quality. Understanding your data is the first step toward building a robust machine learning model. In the next tutorial, we‚Äôll demonstrate how to train and compare models using fastml()."
  },
  {
    "objectID": "tutorials/fastml-classification.html",
    "href": "tutorials/fastml-classification.html",
    "title": "Training Classification Models",
    "section": "",
    "text": "The fastml() function lies at the heart of the fastml package, providing a unified pipeline for training and evaluating classification models with minimal code. It handles everything from data preprocessing to hyperparameter tuning, cross-validation, and model comparison ‚Äî all in a single step.\nIn this tutorial, we‚Äôll walk through a complete binary classification workflow using the iris dataset. You‚Äôll learn how to prepare your data, train multiple models at once, evaluate their performance, and interpret the results. Whether you‚Äôre a beginner or an experienced user, this example will show you how to streamline your classification tasks using fastml."
  },
  {
    "objectID": "tutorials/fastml-classification.html#load-packages-and-data",
    "href": "tutorials/fastml-classification.html#load-packages-and-data",
    "title": "Training Classification Models",
    "section": "1. Load Packages and Data",
    "text": "1. Load Packages and Data\nWe begin by loading the necessary packages: fastml for model training and evaluation, and dplyr for data manipulation.\n\nlibrary(fastml)\nlibrary(dplyr)\n\nTo keep things simple for this introductory classification tutorial, we‚Äôll use a modified version of the classic iris dataset. Specifically, we‚Äôll filter out the ‚Äúsetosa‚Äù species to turn the problem into a binary classification task ‚Äî predicting whether a sample belongs to versicolor or virginica.\n\niris_bin &lt;- iris %&gt;%\n  filter(Species != \"setosa\") %&gt;%\n  mutate(Species = factor(Species))\n\nhead(iris_bin)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1          7.0         3.2          4.7         1.4 versicolor\n2          6.4         3.2          4.5         1.5 versicolor\n3          6.9         3.1          4.9         1.5 versicolor\n4          5.5         2.3          4.0         1.3 versicolor\n5          6.5         2.8          4.6         1.5 versicolor\n6          5.7         2.8          4.5         1.3 versicolor\n\n\nAlthough fastml() supports multi-class classification, we use a binary task here to simplify the initial visualization and performance interpretation. This approach makes it easier to follow the modeling process while still demonstrating the core capabilities of the fastml package."
  },
  {
    "objectID": "tutorials/fastml-classification.html#train-many-models-in-one-shot",
    "href": "tutorials/fastml-classification.html#train-many-models-in-one-shot",
    "title": "Training Classification Models",
    "section": "2. Train Many Models in One Shot",
    "text": "2. Train Many Models in One Shot\nOnce your data is ready, training and comparing multiple classification models is as simple as one function call with fastml(). Specify the dataset, the target label, and the algorithms you want to evaluate:\n\nresult &lt;- fastml(\ndata        = iris_bin,\nlabel       = \"Species\",\nalgorithms  = c(\"logistic_reg\",         \n                \"svm_rbf\",             \n                \"rand_forest\",        \n                \"xgboost\")              \n)\n\nAll major components of a typical machine learning pipeline‚Äîdata splitting, preprocessing, cross-validation, and hyperparameter tuning‚Äîare handled automatically, yet remain fully customizable via function arguments.\nWhat just happened?\n\n\n\n\n\n\n\nStep\nAutomated task\n\n\n\n\n1\nStratified train/test split based on test_size = 0.2 (unless train_data/test_data are provided).\n\n\n2\nAutomatic preprocessing recipe, including dummy-encoding for categorical variables, centering and scaling of numeric predictors, and removal of zero-variance features.\n\n\n3\n10-fold cross-validation within the training set usingresampling_method = \"cv\" and folds = 10.\n\n\n4\nModel-specific hyperparameter tuning via grid search (or Bayesian optimization if tuning_strategy = \"bayes\")\n\n\n5\nMetric collection and model selection, finalizing the best workflow for each algorithm.\n\n\n\n\nEach component can be adjusted through the arguments of fastml() ‚Äî for instance, you can specify your own recipe, choose from multiple resampling_method types (e.g., ‚Äúrepeatedcv‚Äù or ‚Äúboot‚Äù), and even define custom imputation or summary functions.\nBy default, if metric is not provided, the function selects a task-appropriate metric such as accuracy for classification or RMSE for regression. The argument task = \"auto\" intelligently detects whether you‚Äôre solving a classification or regression problem.\nIn the next section, we‚Äôll explore how to compare the trained models using summary() and visualize key performance metrics."
  },
  {
    "objectID": "tutorials/fastml-classification.html#compare-model-performance",
    "href": "tutorials/fastml-classification.html#compare-model-performance",
    "title": "Training Classification Models",
    "section": "3. Compare Model Performance",
    "text": "3. Compare Model Performance\nAfter training, you can summarize and compare all models using the summary() function. This gives a clear overview of model performance across a range of evaluation metrics:\n\nsummary(result, type = \"metrics\")\n\n\n===== fastml Model Summary =====\nTask: classification \nNumber of Models Trained: 4 \nBest Model(s): rand_forest (ranger) xgboost (xgboost) (accuracy: 0.9500000) \n\nPerformance Metrics (Sorted by accuracy ):\n\n------------------------------------------------------------------------------------------------------ \nModel         Engine   Accuracy   F1 Score   Kappa      Precision  Sensitivity  Specificity  ROC AUC   \n------------------------------------------------------------------------------------------------------ \nrand_forest*  ranger   0.9500000  0.9473684  0.9000000  1.0000000  0.9000000    1.0000000    1.0000000 \nxgboost*      xgboost  0.9500000  0.9473684  0.9000000  1.0000000  0.9000000    1.0000000    1.0000000 \nlogistic_reg  glm      0.9000000  0.8888889  0.8000000  1.0000000  0.8000000    1.0000000    0.9000000 \nsvm_rbf       kernlab  0.9000000  0.8888889  0.8000000  1.0000000  0.8000000    1.0000000    1.0000000 \n------------------------------------------------------------------------------------------------------ \n(*Best model)\n\n\nEach row summarizes one model‚Äôs performance, including:\n\nAccuracy: Overall classification correctness.\nF1 Score: Harmonic mean of precision and recall.\nKappa: Agreement adjusted for chance.\nPrecision: Proportion of positive predictions that were correct.\nSensitivity (Recall): Ability to identify positive cases.\nSpecificity: Ability to identify negative cases.\nROC AUC: Area under the ROC curve; a measure of overall discriminative ability.\n\nThe best-performing models (based on the selected metric) are marked with an asterisk *. In this case, both rand_forest and xgboost achieved the highest accuracy (0.95), making them the top candidates for deployment or further analysis.\nWant to focus on just a few models? You can pass a subset using the algorithm argument in summary()."
  },
  {
    "objectID": "tutorials/fastml-classification.html#inspect-tuned-hyperparameters",
    "href": "tutorials/fastml-classification.html#inspect-tuned-hyperparameters",
    "title": "Training Classification Models",
    "section": "4. Inspect Tuned Hyperparameters",
    "text": "4. Inspect Tuned Hyperparameters\nTo understand what made the top-performing models successful, use:\n\nsummary(result, type = \"params\")\n\nBest Model hyperparameters:\n\nModel: rand_forest (ranger) \n  mtry: 2\n  trees: 500\n  min_n: 10\n\nModel: xgboost (xgboost) \n  mtry: 2\n  trees: 15\n  min_n: 2\n  tree_depth: 6\n  learn_rate: 0.1\n  loss_reduction: 1\n  sample_size: 0.5\n\n\nThis will display the best hyperparameter values selected during tuning for each model. These settings are critical for optimizing performance and can inform further refinement or deployment.\nThese values are chosen based on cross-validation performance and reflect the best configuration for each algorithm on your specific dataset. The fastml() function handles this tuning process automatically, but if desired, you can manually set or constrain parameters using the tune_params argument."
  },
  {
    "objectID": "tutorials/fastml-classification.html#confusion-matrix",
    "href": "tutorials/fastml-classification.html#confusion-matrix",
    "title": "Training Classification Models",
    "section": "5. Confusion Matrix",
    "text": "5. Confusion Matrix\nTo evaluate model performance at the prediction level, especially for classification tasks, a confusion matrix offers detailed insight. Use:\n\nsummary(result, type = \"conf_mat\")\n\n\n===========================\nConfusion Matrices by Model\n===========================\n\nModel: rand_forest (ranger) \n---------------------------\n            Truth\nPrediction   versicolor virginica\n  versicolor          9         0\n  virginica           1        10\n\nModel: xgboost (xgboost) \n---------------------------\n            Truth\nPrediction   versicolor virginica\n  versicolor          9         0\n  virginica           1        10\n\n\nThis displays the confusion matrices for each model, showing how well the predicted classes match the actual ones.\nThe result suggests both models performed very well on this binary classification task, with minimal confusion between the two classes. For datasets with imbalanced classes or more nuanced distinctions, confusion matrices can help pinpoint which types of errors are most common‚Äîfalse positives, false negatives, or both."
  },
  {
    "objectID": "tutorials/fastml-classification.html#visualize-model-performance",
    "href": "tutorials/fastml-classification.html#visualize-model-performance",
    "title": "Training Classification Models",
    "section": "6. Visualize Model Performance",
    "text": "6. Visualize Model Performance\nTo quickly compare how each model performed across different evaluation metrics, you can use the built-in plot() function with type = \"bar\":\n\nplot(result, type = \"bar\")\n\n\n\n\n\n\n\n\nThis generates a faceted bar plot, with one panel per metric (e.g., Accuracy, F1 Score, ROC AUC), and bars representing each model‚Äôs score on that metric.\nWhy it‚Äôs useful\n\nProvides a visual overview of strengths and weaknesses across models.\nMakes it easy to spot trade-offs (e.g., high accuracy but lower sensitivity).\nHighlights top-performing models for each metric.\n\nEach bar is color-coded by model, and the best-performing models stand out immediately. This is especially helpful when evaluating more than just one metric, such as both accuracy and F1 score, which can differ notably in imbalanced datasets."
  },
  {
    "objectID": "tutorials/fastml-classification.html#visualize-roc-curves",
    "href": "tutorials/fastml-classification.html#visualize-roc-curves",
    "title": "Training Classification Models",
    "section": "7. Visualize ROC Curves",
    "text": "7. Visualize ROC Curves\nTo assess the discriminative power of each model in a binary classification task, plot their Receiver Operating Characteristic (ROC) curves using:\n\nplot(result, type = \"roc\")\n\n\n\n\n\n\n\n\nThis generates a faceted plot showing the ROC curve for each trained model.\nWhat ROC curves show\n\nTrue Positive Rate (Sensitivity) vs.¬†False Positive Rate (1 - Specificity) across thresholds.\nThe area under the curve (AUC) indicates overall model performance:\n\nAUC close to 1.0 suggests excellent discrimination.\n\n-AUC of 0.5 means no better than random guessing.\n\nWhy it‚Äôs helpful\n\nUseful when dealing with imbalanced data, where accuracy may be misleading.\nAllows comparison of how each model balances sensitivity and specificity.\nHelps in selecting an optimal probability threshold for classification.\n\nIn our example, both rand_forest and xgboost achieved an AUC of 1.0‚Äîindicating perfect separation of classes on the test set.\nYou can also zoom in on specific models using:\n\nplot(result, type = \"roc\", algorithm = \"rand_forest\")"
  },
  {
    "objectID": "tutorials/fastml-classification.html#visualize-calibration-plots",
    "href": "tutorials/fastml-classification.html#visualize-calibration-plots",
    "title": "Training Classification Models",
    "section": "8. Visualize Calibration Plots",
    "text": "8. Visualize Calibration Plots\nTo evaluate how well your model‚Äôs predicted probabilities align with observed outcomes, use a calibration plot:\n\nplot(result, type = \"calibration\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA calibration plot compares predicted probabilities to actual event rates. Well-calibrated models produce probabilities that match the true likelihood of outcomes‚Äîfor example, among all observations predicted with 70% confidence, about 70% should actually belong to the positive class.\nWhat to look for\n\nDiagonal line (ideal calibration): Perfect agreement between predicted and observed probabilities.\nModel curves: Deviations from the diagonal indicate overconfidence (curve above the line) or underconfidence (curve below the line).\nRug plots: Tick marks on the x-axis show the distribution of predicted probabilities.\n\nWhy it matters\n\nHigh accuracy doesn‚Äôt guarantee good probability estimates. Calibration assesses the reliability of those estimates.\nImportant for risk prediction, medical diagnosis, or any domain where predicted probabilities are used for decision-making.\n\nFor binary classification tasks, calibration plots provide a deeper layer of model evaluation‚Äîespecially when choosing between equally accurate models."
  },
  {
    "objectID": "tutorials/fastml-classification.html#predictions-on-new-data",
    "href": "tutorials/fastml-classification.html#predictions-on-new-data",
    "title": "Training Classification Models",
    "section": "9. Predictions on New Data",
    "text": "9. Predictions on New Data\nOnce you‚Äôve identified the best-performing model, making predictions on new, unseen data is simple and consistent with the predict() function. The fastml package ensures that the appropriate preprocessing steps are automatically applied, so you can focus directly on generating and interpreting the predictions.\nAs a demonstration, let‚Äôs select a few random observations from the binary-classified iris dataset (iris_bin):\n\nnew_obs &lt;- iris_bin[-5] %&gt;% \n  slice_sample(n = 5)\nnew_obs\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1          5.5         2.5          4.0         1.3\n2          6.5         3.0          5.5         1.8\n3          7.4         2.8          6.1         1.9\n4          7.3         2.9          6.3         1.8\n5          6.9         3.1          5.4         2.1\n\n\nThis gives you 5 unseen flower samples, with measurements such as Sepal.Length, Petal.Width, etc.\nPredicting Classes\nTo predict the class (versicolor or virginica) for each observation:\n\npredict(object = result, newdata = new_obs, type = \"class\")\n\n$`rand_forest (ranger)`\n[1] versicolor versicolor versicolor versicolor versicolor\nLevels: versicolor virginica\n\n$`xgboost (xgboost)`\n[1] versicolor versicolor versicolor versicolor versicolor\nLevels: versicolor virginica\n\nattr(,\"class\")\n[1] \"fastml_prediction\"\n\n\n\nThe result is a named list with one vector per best-performing model.\nEach vector contains the predicted class (versicolor or virginica) for each observation in new_obs.\nIf multiple models tied in performance, predictions from each are shown (e.g., rand_forest and xgboost in this case).\n\nYou can access specific model predictions like this:\n\npredict(object = result, newdata = new_obs, type = \"class\", model_name = \"rand_forest (ranger)\")\n\n[1] versicolor versicolor versicolor versicolor versicolor\nLevels: versicolor virginica\n\n\nPredicting Class Probabilities\nIn addition to class labels, you can obtain probability estimates for each class. This is especially useful when you need to assess the model‚Äôs confidence in its predictions or when applying a custom decision threshold.\nUse the following command:\n\npredict(object = result, newdata = new_obs, type = \"prob\")\n\n$`rand_forest (ranger)`\n# A tibble: 5 √ó 2\n  .pred_versicolor .pred_virginica\n             &lt;dbl&gt;           &lt;dbl&gt;\n1            0.859           0.141\n2            0.859           0.141\n3            0.859           0.141\n4            0.859           0.141\n5            0.859           0.141\n\n$`xgboost (xgboost)`\n# A tibble: 5 √ó 2\n  .pred_versicolor .pred_virginica\n             &lt;dbl&gt;           &lt;dbl&gt;\n1            0.816           0.184\n2            0.816           0.184\n3            0.816           0.184\n4            0.816           0.184\n5            0.816           0.184\n\nattr(,\"class\")\n[1] \"fastml_prediction\"\n\n\nEach sublist contains a tibble with predicted probabilities for each class, corresponding to the rows in new_obs.\nThis is useful when:\n\nTo adjust thresholds for sensitivity/specificity trade-offs\nTo rank cases based on prediction confidence\nFor risk scoring or clinical triage tasks\n\nBoth class labels and probabilities are powered by the final model trained and selected via fastml()‚Äîyou don‚Äôt need to manually reapply recipes or preprocessing steps.\nIn the next step, you can use SHAP explanations to understand why the model made these predictions."
  },
  {
    "objectID": "tutorials/fastml-classification.html#model-explainability",
    "href": "tutorials/fastml-classification.html#model-explainability",
    "title": "Training Classification Models",
    "section": "10. Model Explainability",
    "text": "10. Model Explainability\nUnderstanding why a model makes a certain prediction is just as important as how accurate it is‚Äîespecially in fields like healthcare, finance, and policy. The fastexplain() function in fastml integrates with the DALEX package to provide a suite of explainability tools.\nRun the following to generate explanations for the best model(s):\n\nfastexplain(result)       \n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  rand_forest (ranger) \n  -&gt; data              :  80  rows  4  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  80  values \n  -&gt; predict function  :  predict_function \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package , ver. , task classification \n  -&gt; predicted values  :  predict function returns multiple columns:  2  (  default  ) \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  xgboost (xgboost) \n  -&gt; data              :  80  rows  4  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  80  values \n  -&gt; predict function  :  predict_function \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package , ver. , task classification \n  -&gt; predicted values  :  predict function returns multiple columns:  2  (  default  ) \n  A new explainer has been created!  \n\n=== DALEX Variable Importance (with Boxplots) ===\n\n\n\n\n\n\n\n\n\n\n=== DALEX Shapley Values (SHAP) ===\n\n\n\n\n\n\n\n\n\nThis process wraps your trained models into DALEX explainers, enabling multiple forms of model interpretation.\nWhat you get\nOnce the explainers are created, several plots and summaries are available:\n\nPermutation-based Variable Importance: Quantifies the contribution of each feature to model accuracy by measuring the drop in performance when the feature is randomly permuted.\nSHAP Values (Shapley Additive Explanations): Breaks down individual predictions into feature-level contributions, helping you understand why a particular prediction was made.\nPartial Dependence and Model Profiles: Visualizes how predictions change across a range of values for a selected feature, isolating its effect while averaging out others."
  },
  {
    "objectID": "tutorials/fastml-classification.html#advanced-options-cheatsheet",
    "href": "tutorials/fastml-classification.html#advanced-options-cheatsheet",
    "title": "Training Classification Models",
    "section": "Advanced Options Cheat‚ÄëSheet",
    "text": "Advanced Options Cheat‚ÄëSheet\nThe fastml() function supports a variety of advanced arguments to give you full control over your machine learning pipeline. Here‚Äôs a quick reference:\n\n\n\nArgument\nPurpose\nExample\n\n\n\n\nrecipe\nSupply a custom recipes object to override the automatic preprocessing pipeline\nrecipe(Species ~ ., data = iris_bin) %&gt;% step_normalize(all_numeric())\n\n\nimpute_method\nSpecify how missing values are handled. Supported methods include \"medianImpute\", \"mice\", \"missForest\", \"knnImpute\", and more\nimpute_method = \"knnImpute\"\n\n\nalgorithm_engines\nCustomize which engine is used for each algorithm\nlist(rand_forest = \"randomForest\", svm_rbf = \"kernlab\")\n\n\ntuning_strategy\nChoose hyperparameter search strategy: \"grid\" (default) or \"bayes\" (Bayesian optimization with optional early_stopping)\ntuning_strategy = \"bayes\", tuning_iterations = 25\n\n\nlearning_curve\nGenerate learning curves to visualize performance vs.¬†training size\nlearning_curve = TRUE\n\n\n\nThese options allow for deeper experimentation and optimization, especially when working with complex datasets or performance-critical tasks.\nTip: Explore ?fastml in your R console to see the full list of arguments and their descriptions.\nBy combining ease of use with advanced flexibility, fastml helps both beginners and experienced users build robust, interpretable, and high-performing machine learning models."
  }
]