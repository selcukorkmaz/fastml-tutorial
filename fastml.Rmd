---
title: "Get Started with fastml"
description: >
  A modern, hands-on introduction to training, tuning, and evaluating machine learning
  models using the fastml package.
author: "Selçuk Korkmaz"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    toc: true
    toc_float: true
    theme: default
    css: fastml_style.css
vignette: >
  %\VignetteIndexEntry{Get Started with fastml}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

[<img src="https://www.r-pkg.org/badges/version/fastml" alt="CRAN Version">](https://CRAN.R-project.org/package=fastml)
[![GitHub repo](https://img.shields.io/badge/GitHub-fastml-blue.svg)](https://github.com/selcukorkmaz/fastml)

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5,
  message = FALSE,
  warning = FALSE
)

# Load core packages
library(fastml)
library(tidyverse)
library(bslib)

# Apply a modern theme globally (works in static vignettes)
bs_global_set(
  bs_theme(
    version = 5,
    bootswatch = "cosmo",
    base_font = font_google("Source Sans Pro"),
    heading_font = font_google("Poppins"),
    code_font = font_google("Source Code Pro"),
    primary = "#4758AB"
  )
)
```

<!-- <style> body { font-family: "Source Sans Pro", "Segoe UI", sans-serif; line-height: 1.65; color: #2c3e50; background-color: #ffffff; max-width: 900px; margin: auto; font-size: 15.5px; } h1, h2, h3 { font-weight: 600; color: #34495e; border-bottom: 1px solid #e0e0e0; padding-bottom: 4px; } code, pre { background-color: #f9f9fc; border-radius: 4px; padding: 2px 4px; } .sourceCode { background: #f4f6f7; border-left: 4px solid #4758AB; padding: 10px; border-radius: 6px; } blockquote { border-left: 4px solid #4758AB; padding-left: 12px; color: #6c757d; font-style: italic; } .note { background: #f9f9fc; border-left: 4px solid #4758AB; padding: 12px; margin: 16px 0; border-radius: 6px; } a:hover { color: #CA225E; text-decoration: underline; } .note:hover { background-color: #eef1fb; transition: background 0.3s ease; } #TOC { position: fixed; top: 90px; right: 40px; width: 260px; font-size: 0.9em; background: #fafafa; border: 1px solid #e0e0e0; padding: 8px 12px; border-radius: 6px; box-shadow: 0 0 4px rgba(0,0,0,0.1); } </style> <p align="center" style="margin-top:20px;"><img src="fastml_hex.png "width="130" alt="fastml logo" style="border:none; box-shadow:none; background:none; outline:none; display:block; margin-left:auto; margin-right:auto;" /></p> -->

<!-- <style> pre.sourceCode { position: relative; background-color: #f9f9fc; border-radius: 6px; border-left: 4px solid #4758AB; padding: 10px; } .copy-button { position: absolute; top: 8px; right: 10px; background-color: #4758AB; color: #fff; border: none; border-radius: 4px; padding: 4px 8px; font-size: 12px; cursor: pointer; opacity: 0.8; } .copy-button:hover { opacity: 1; } pre.sourceCode:hover .copy-button { display: block; } </style> <script> document.addEventListener("DOMContentLoaded", function() { document.querySelectorAll('pre code').forEach(function(block) { var button = document.createElement('button'); button.className = 'copy-button'; button.type = 'button'; button.innerText = 'Copy'; button.addEventListener('click', function() { navigator.clipboard.writeText(block.innerText); button.innerText = 'Copied!'; setTimeout(function() { button.innerText = 'Copy'; }, 1500); }); var pre = block.parentNode; pre.style.position = 'relative'; pre.insertBefore(button, block); }); }); </script> -->

<p align="center" style="margin-top:20px;">
  <img src="fastml_hex.png" width="130" alt="fastml logo" style="border:none; box-shadow:none; background:none; display:block; margin:auto;" />
</p>


# Introduction

The **fastml** package provides a unified and efficient framework for training, evaluating, and comparing multiple machine learning models in R. It is designed to minimize repetitive coding and automate essential steps of a typical machine learning workflow.

With a single, consistent interface, **fastml** enables researchers and data scientists to perform end-to-end analysis — from data preprocessing to model evaluation — with minimal manual intervention.

Key features include:

- **Comprehensive Data Preprocessing:**  
  Automatically handles missing values, encodes categorical variables, and applies user-specified normalization or scaling methods.

- **Multi-Algorithm Support:**  
  Trains and compares a broad range of models such as Random Forests, XGBoost, Support Vector Machines, Neural Networks, and Generalized Linear Models with a single function call.

- **Task Auto-Detection:**  
  Detects the nature of the modeling task—classification, regression, or survival analysis—based on the provided outcome variable.

- **Flexible Hyperparameter Tuning:**  
  Supports both default and user-defined tuning strategies, including grid search, random search, and Bayesian optimization.

- **Comprehensive Evaluation and Visualization:**  
  Generates detailed performance metrics, confusion matrices, and diagnostic plots such as ROC curves, residual plots, and feature importance visualizations.

This vignette introduces the main functionality of **fastml** through practical examples. You will learn how to set up your data, train multiple models, fine-tune them, and interpret their results efficiently.


# Philosophy: The fastml Approach

The R ecosystem — particularly the **tidymodels** framework — provides a rich, modular toolkit for building sophisticated and fully customized modeling workflows.  
While this flexibility is ideal for in-depth research, it can sometimes feel cumbersome when your goal is to obtain a fast, reliable baseline model.

In many applied settings, the objective is not to engineer the most complex pipeline, but to **rapidly compare several well-established algorithms** under consistent preprocessing and evaluation procedures.  
This is where **fastml** excels.

The philosophy behind **fastml** is simple: automate the most common 80% of the modeling workflow so you can focus on interpretation, insight, and decision-making rather than boilerplate code.

The core function, `fastml()`, provides an opinionated but extensible one-stop interface that automatically performs:

- **Task Auto-Detection:**

  Automatically detects whether you are performing classification, regression, or survival analysis based on your target variable (label).

- **Data Splitting and Stratification:**  
  Creates reproducible training and testing partitions, optionally stratified by class labels.

- **Preprocessing Pipeline:**  
  Builds a robust recipe that handles missing values, encodes categorical variables, and applies standardization or scaling as needed.

- **Multi-Algorithm Training:**  
  Trains multiple algorithms in parallel — from classical GLMs to ensemble and tree-based models — with consistent cross-validation.

- **Hyperparameter Tuning and Evaluation:**  
  Supports efficient tuning strategies and delivers comprehensive performance summaries.

In essence, **fastml** brings together best practices from the **tidymodels** ecosystem into a concise and reproducible interface — allowing you to move seamlessly from a raw data frame to a transparent model comparison in a single step.


# Installation

You can install the stable version of **fastml** from CRAN:

```{r, eval=FALSE}
# Install just the core package
install.packages("fastml")

# Install with all model dependencies (recommended)
install.packages("fastml", dependencies = TRUE)
```

If you prefer to use the most recent development version with the latest updates and features, you can install it from GitHub:

```{r, eval=FALSE}
install.packages("devtools")
devtools::install_github("selcukorkmaz/fastml")
```

After installation, load the package:

```{r, eval=FALSE}
library(fastml)
```

You are now ready to begin building and evaluating machine learning models with **fastml**.

# Your First Workflow: Classification

Let’s begin with a simple **binary classification** example using the classic *iris* dataset.  
Here, we will predict whether a flower is *versicolor* or *virginica* based on its sepal and petal measurements.

We first prepare and explore the data using **tidyverse** tools before passing it to `fastml()`.

```{r}
data(iris)

iris_binary <- iris %>%
  filter(Species != "setosa") %>%
  mutate(Species = factor(Species)) # Ensure label is a factor

# Optional: quick exploratory plot
iris_binary %>%
  ggplot(aes(x = Sepal.Length, y = Petal.Length, color = Species)) +
  geom_point() +
  labs(title = "Exploring Our Data")
```

Now, we pass this clean iris_binary data frame to `fastml()`. Notice we only provide the label. fastml inspects the *Species* column and, seeing it's a factor, automatically detects a classification task. In this example, we train two algorithms — a Random Forest and a Logistic Regression model.

```{r}
model_class <- fastml(
  data = iris_binary,
  label = "Species",
  algorithms = c("rand_forest", "logistic_reg")
)
```

After training, we can inspect the performance results.
The summary() function provides an overview of all trained models, ranked by their primary evaluation metric (typically accuracy or AUC).

```{r}
summary(model_class)
```

The model marked with * in the summary output represents the best-performing model based on the selected evaluation criterion.
This quick workflow demonstrates how fastml enables rapid, consistent model benchmarking with minimal code.

# Visualizing Results

**fastml** includes built-in visualization tools that allow quick inspection and comparison of model performance.  
Depending on the analysis type, different plots are available for classification, regression, and survival tasks.

For classification problems, the `"bar"` and `"roc"` plot types are the most commonly used.

```{r}
# Plot the performance metrics
plot(model_class, type = "bar")

# Plot ROC curves
plot(model_class, type = "roc")
```

The bar plot summarizes key performance metrics (e.g., Accuracy, AUC, F1) across all trained models, helping identify the top performer at a glance.
The ROC plot illustrates how well each classifier separates the two classes across varying thresholds.

However, a high ROC AUC does not necessarily mean that the model’s predicted probabilities are well calibrated.
For example, a model may predict “80% chance of being Virginica,” but the event occurs only 60% of the time.
To assess calibration quality, you can use the "calibration" plot:

```{r}
plot(model_class, type = "calibration")
```

The calibration plot compares predicted probabilities with observed frequencies.
A perfectly calibrated model will follow the diagonal line, while deviations indicate over- or under-confidence in the model’s predictions.

# Discovering Available Models

**fastml** supports a broad set of algorithms covering **classification**, **regression**, and **survival analysis** tasks.  
These include both traditional statistical models and modern machine learning algorithms such as Random Forests, Gradient Boosting, and Neural Networks.

To view the available algorithms for each task type, use the helper function `availableMethods()`.


```{r}
# List all supported classification algorithms
availableMethods("classification")

# List all supported regression algorithms
availableMethods("regression")

# List all supported survival analysis algorithms
availableMethods("survival")
```

This function returns the algorithm names recognized by **fastml**, which can be passed directly to the algorithms argument of the `fastml()` function.

# Running All Models at Once

In many cases, you may want to benchmark multiple algorithms to identify the best-performing model for your dataset.  
Instead of manually specifying individual algorithms, you can simply set `algorithms = "all"`.

When this option is used, **fastml** automatically runs every supported model for the selected task (classification, regression, or survival analysis).  

The `summary()` output then ranks all models according to their key performance metric.

```{r}
# This process may take several minutes depending on data size and models
model_battle <- fastml(
  data = iris_binary,
  label = "Species",
  algorithms = "all",
)

# Display a ranked summary of all trained models
summary(model_battle)
```

This approach provides a quick, reproducible “model battle royale” that highlights which algorithms perform best on your data.
It is especially useful for exploratory analysis or as a first step before focusing on fine-tuning specific models.

# Workflow Example: Regression

**fastml** automatically detects a regression task when the outcome variable (`label`) is numeric.  
In this example, we use the classic `mtcars` dataset to predict **miles per gallon (mpg)** based on engine and vehicle characteristics.

The model performance will be evaluated using **Root Mean Squared Error (RMSE)** as the optimization metric.

```{r}
# 1. Prepare data
data(pbc, package = "survival")
  
# The pbc dataset has two parts; we only want the baseline data (rows 1-312)
pbc_baseline <- pbc[1:312, ]

# 2. Train regression models
# We'll compare a Random Forest and an XGBoost model
model_reg <- fastml(
    data = pbc_baseline,
    label = "albumin",
    algorithms = c("rand_forest", "xgboost"),
    metric = "rmse",            # Optimize for RMSE
    impute_method = "remove" # Remove missing values
  )

# 3. Summarize the results
summary(model_reg)
```

The summary output lists all trained models ranked by their RMSE values, where lower scores indicate better predictive accuracy.
As with classification tasks, fastml standardizes data preprocessing and cross-validation, ensuring fair model comparison.
You can visualize regression performance further using residual plot:

```{r}
# Examine residual distribution
plot(model_reg, type = "residual")
```

These plots help assess whether errors are randomly distributed and whether any model exhibits systematic bias.


# A Deeper Dive: Survival Analysis

**fastml** provides native support for **survival analysis**, enabling the use of both classical and modern flexible models.  
A survival workflow is automatically triggered when the `label` argument includes two variables: **time** and **status**.

The package seamlessly integrates multiple survival modeling approaches, including:

- **Cox Proportional-Hazards model** (`survival` package)  
- **Royston–Parmar flexible parametric model** (`rstpm2` package)  
- **General parametric survival models** (`flexsurvreg` package)

In this example, we’ll fit a **Cox Proportional-Hazards**, a **Weibull parametric**, and an **XGBoost** survival model using the `lung` dataset.

```{r}
# 1. Prepare data
library(survival)
data(lung, package = "survival")

# 2. Train survival models
model_surv <- fastml(
  data = lung,
  label = c("time", "status"),  # triggers survival analysis
  algorithms = c("cox_ph", "parametric_surv", "xgboost"),
  impute_method = "medianImpute",  # handle missing values

  # Specify the distribution for the parametric model
  engine_params = list(
    parametric_surv = list(
      flexsurvreg = list(dist = "weibull")
    )
  )
)

# 3. Summarize model performance
# Metrics include Harrell’s C-index, Uno’s C, and Integrated Brier Score (IBS)
summary(model_surv)

```

The summary output for survival models provides specialized performance metrics that evaluate both discrimination and calibration.
To inspect detailed model coefficients or distribution parameters, you can use the `type = "params"` option:

```{r}
summary(model_surv, type = "params", algorithm = "cox_ph")
```

These outputs allow detailed inspection of model components and help compare the interpretability and flexibility of different survival approaches within a unified **fastml** workflow.

# Handling Imbalanced Data

In many real-world classification problems, one class is much less frequent than the other.  
This **class imbalance** can lead to biased models that favor the majority class and underperform on minority observations.

To mitigate this, **fastml** provides the `balance_method` argument, which controls how the training data is balanced before model fitting.  
You can specify one of the following options:

- `"upsample"` — randomly duplicates minority-class samples to achieve balance.  
- `"downsample"` — randomly removes majority-class samples to achieve balance.  
- `"none"` — disables balancing and uses the data as is (default).

Let’s demonstrate this feature using the **BreastCancer** dataset from the `mlbench` package, where the goal is to predict tumor type (*benign* vs *malignant*).


```{r}
library(dplyr)
library(mlbench)

# Load and prepare data
data(BreastCancer)
bc_data <- BreastCancer %>%
  select(-Id)  # remove non-predictor column

# Examine class distribution
table(bc_data$Class)

# Preview the data structure
head(bc_data)
```

**Case 1: No Balancing (The Baseline)**

First, we train on the original, imbalanced data.

```{r}
model_none <- fastml(
    data = bc_data,
    label = "Class",
    algorithms = "logistic_reg",
    impute_method = "medianImpute", # Handle NAs in 'Bare.nuclei'
    balance_method = "none"         # Key argument!
)
  
# Show the summary
summary(model_none)
```

**Case 2: Downsampling**

Next, we train a model after downsampling the majority (benign) class to match the size of the minority (malignant) class.

```{r}
model_down <- fastml(
    data = bc_data,
    label = "Class",
    algorithms = "logistic_reg",
    impute_method = "medianImpute", # Handle NAs in 'Bare.nuclei'
    balance_method = "downsample"         # Key argument!
)
  
# Show the summary
summary(model_down)
```

**Case 3: Upsampling**

Finally, we train a model after upsampling the minority (malignant) class to match the size of the majority (benign) class.

```{r}
model_up <- fastml(
    data = bc_data,
    label = "Class",
    algorithms = "logistic_reg",
    impute_method = "medianImpute", # Handle NAs in 'Bare.nuclei'
    balance_method = "upsample"         # Key argument!
)
  
# Show the summary
summary(model_up)
```


**Comparison**

After training three models using different balancing strategies (`none`, `downsample`, and `upsample`), we can compare their performance metrics.  
Although the confusion matrices appear similar for this dataset—due to the inherent stability of logistic regression—the ROC AUC scores reveal meaningful differences:

| Method       | ROC AUC |
|---------------|---------|
| none          | 0.959   |
| downsample    | 0.962   |
| upsample      | 0.987   |

The **ROC AUC** metric evaluates how well the model ranks positive versus negative samples based on predicted probabilities.  
Even when the discrete class predictions remain the same, differences in ROC AUC confirm that three distinct models were trained.

Among these, the model trained with **upsampling** achieved the highest ROC AUC, indicating superior discriminative performance.  
This result highlights how addressing class imbalance—particularly through upsampling—can significantly improve the model’s ability to distinguish between *benign* and *malignant* cases in imbalanced medical datasets.


# Imputation

`fastml` can handle missing data using several strategies via the `impute_method` argument.  
While `"medianImpute"` is fast, you can use more powerful (but slower) methods:

- `"knnImpute"` — Uses **K-Nearest Neighbors** to impute missing values based on similarity.  
- `"mice"` — Performs **Multivariate Imputation by Chained Equations**, a flexible and statistically grounded method.  
- `"missForest"` — Employs **Random Forests** to iteratively impute missing data.

The choice depends on dataset size, computational resources, and the degree of missingness.

Below, we demonstrate the use of **MICE** imputation with the `lung` dataset from the `survival` package.

```{r}
library(survival)
data(lung, package = "survival")

# This code assumes you have the 'mice' package installed
# install.packages("mice")

model_mice <- fastml(
  data = lung,
  label = c("time", "status"),
  algorithms = "penalized_cox",
  impute_method = "mice" # Use MICE for imputation
)

summary(model_mice, type = "metrics")
```

# Hyperparameter Tuning

Most machine learning algorithms include **hyperparameters** that control model complexity, regularization, and learning dynamics.  
By default, **fastml** uses each model’s standard parameter settings (`use_default_tuning = FALSE`), which are usually adequate for quick benchmarking.  
However, you can easily enable automated or fully customized **hyperparameter tuning** to optimize model performance.

## Enabling Tuning

To activate tuning, set `use_default_tuning = TRUE`.  
When enabled, **fastml** either uses built-in tuning grids for each algorithm or applies your own custom grid if supplied through the `tune_params` argument.

## Custom Tuning Grids

You can define a tuning grid manually for fine-grained control over search values.  
The expected structure is:

```{r, eval=FALSE}
list(algorithm = list(engine = list(
  param1 = c(values), param2 = c(values)
)))
```

Here’s an example tuning grid for the **ranger** engine of a Random Forest model:

```{r}
# Define a custom grid for the 'ranger' engine of 'rand_forest'
my_tune_grid <- list(
  rand_forest = list(
    ranger = list(
      mtry = c(1, 2, 3),
      min_n = c(5, 10)
    )
  )
)

# Train model with custom tuning
model_custom_tune <- fastml(
  data = iris_binary,
  label = "Species",
  algorithms = "rand_forest",
  tune_params = my_tune_grid,
  use_default_tuning = TRUE # Must be TRUE to enable tuning
)

# Inspect tuned parameters
summary(model_custom_tune, type = "params")
```

When tuning is enabled, fastml automatically performs internal resampling and selects the parameter combination that yields the best validation performance.
This allows efficient exploration of hyperparameter space with minimal manual coding, while maintaining compatibility with all supported algorithms and engines.

## Bayesian Tuning

Grid search can be computationally expensive because it evaluates all parameter combinations.  
To improve efficiency, **fastml** supports **Bayesian optimization**, which uses past evaluation results to guide the search toward promising parameter regions.

You can enable Bayesian tuning by setting `tuning_strategy = "bayes"` and `use_default_tuning = TRUE`.  
The argument `tuning_iterations` controls how many optimization steps are performed.

```{r}
# Bayesian hyperparameter tuning example
set.seed(123)
model_bayes <- fastml(
  data = iris_binary,
  label = "Species",
  algorithms = "xgboost",
  use_default_tuning = TRUE,
  tuning_strategy = "bayes",     # enable Bayesian optimization
  tuning_iterations = 10         # number of search iterations
)

# Review the best-found parameters
summary(model_bayes, type = "params")
```

Bayesian optimization intelligently balances exploration (trying new areas of the parameter space) and exploitation (refining promising regions).

This often yields better models in fewer iterations compared to exhaustive grid or random search, making it suitable for complex models like XGBoost or neural networks where tuning spaces are large.

# The tidymodels Bridge: Using Custom recipes

By default, **fastml** applies an internal preprocessing pipeline that includes data cleaning, encoding, and scaling.  
However, in many research or production scenarios, you may need to define **custom feature engineering steps** or domain-specific transformations.  

To support this flexibility, **fastml** seamlessly integrates with the **tidymodels** ecosystem.  
You can pass your own untrained `recipes` object through the `recipe` argument, and **fastml** will use it for all models—skipping its internal preprocessing.  
This approach allows you to combine the simplicity of **fastml** with the full power of **tidymodels**’ preprocessing framework.


```{r}
library(recipes)

# 1. Define a custom tidymodels recipe
# Example: Normalize all numeric features and apply PCA
my_recipe <- recipe(Species ~ ., data = iris_binary) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_pca(all_numeric_predictors(), num_comp = 2)

# 2. Pass the custom recipe to fastml
# fastml will now use your recipe instead of its internal pipeline
model_recipe <- fastml(
  data = iris_binary,
  label = "Species",
  recipe = my_recipe,
  algorithms = c("rand_forest", "svm_rbf")
)

# 3. Summarize model performance
summary(model_recipe)
```

When a custom `recipe` is supplied:

- **fastml** disables its internal preprocessing steps, such as imputation, encoding, and scaling.  
- The provided recipe is applied consistently across all algorithms, ensuring reproducibility.  
- You gain full control over feature engineering, transformations, and variable selection — ideal for advanced users already working within the **tidymodels** framework.

This integration allows you to combine the automation and multi-model comparison strengths of **fastml** with the flexibility and transparency of **recipes**.  
It bridges quick experimentation with advanced, fully customized preprocessing workflows, preserving both reproducibility and analytical control.

## Using Custom rsample Folds

By default, **fastml** automatically creates its own training, testing, and resampling splits.  
However, in some cases — such as **stratified sampling**, **blocked time-series splits**, or **custom validation designs** — you may want full control over how the data is partitioned.  

To do this, you can pass your own **rsample** object (e.g., `vfold_cv`, `bootstraps`, or `mc_cv`) to the `resamples` argument.  
**fastml** will then use your predefined folds for training, validation, and tuning instead of generating its own.


```{r}
library(rsample)

# 1. Create custom 5-fold cross-validation splits
my_folds <- vfold_cv(iris_binary, v = 5, strata = "Species")

# 2. Use the custom folds in fastml
model_custom_folds <- fastml(
  data = iris_binary,
  label = "Species",
  algorithms = "svm_rbf",
  resamples = my_folds,       # pass custom resampling object
  use_default_tuning = TRUE   # tuning will use these folds
)

# 3. Inspect tuned parameters
summary(model_custom_folds, type = "params")
```

This approach is particularly useful when you need to:

- **Maintain consistency** across multiple modeling experiments, ensuring that identical folds and sampling strategies are reused.  
- **Apply domain-specific validation designs**, such as patient-level stratification or batch-wise splitting in biomedical or clinical datasets.  
- **Guarantee reproducibility** of resampling strategies across environments and packages, making results directly comparable between runs.

By combining **rsample** objects with **fastml**, you gain both flexibility and reproducibility while retaining a streamlined modeling workflow.  
This integration bridges the convenience of automated model benchmarking with the precision of fully customized resampling strategies.


# Advanced Engine & Parameter Control

Beyond hyperparameter tuning, **fastml** also provides direct control over the **engine** (the underlying R package used for model fitting) and any fixed engine-specific parameters.  
This allows fine-grained customization, enabling side-by-side comparisons of different implementations of the same algorithm.


## Comparing Multiple Engines

Many algorithms—such as Random Forests, Gradient Boosting, or SVMs—can be implemented using multiple back-end engines.  
The `algorithm_engines` argument lets you specify which engines to use for each algorithm.  
You can provide a vector of engine names for a single algorithm, and **fastml** will train and evaluate each engine separately under identical preprocessing and evaluation settings.


```{r}
# Compare different Random Forest engines
model_engines <- fastml(
  data = iris_binary,
  label = "Species",
  algorithms = "rand_forest",

  # Run 'rand_forest' twice: once with 'ranger', once with 'randomForest'
  algorithm_engines = list(
    rand_forest = c("ranger", "randomForest")
  )
)

# Summarize performance across engines
summary(model_engines)

# Visualize engine comparison
plot(model_engines, type = "bar")
```

Both the `summary()` and `plot()` functions automatically recognize engine-level differences, enabling direct comparison of performance metrics, runtime, and model characteristics.  
This capability is particularly valuable for **reproducibility studies**, **benchmarking different implementations**, and **evaluating performance trade-offs** between older and newer engines.

Beyond selecting engines, you can fine-tune their behavior using the `engine_params` argument, which allows you to pass fixed arguments directly to the underlying modeling functions  
(for example, `num.trees`, `max.depth`, or `kernel`).

This balance of **control** and **automation** makes **fastml** suitable for both rapid experimentation and carefully optimized research workflows.  
It enables reproducible, side-by-side evaluation of modeling engines under a unified, transparent framework.

## Setting Fixed Engine Parameters

In some situations, you may want to **set specific model parameters manually** instead of tuning them across a search grid.  
To do this, include the desired fixed parameters inside the `tune_params` argument and enable `use_default_tuning = TRUE`.  
This ensures that **fastml** uses your predefined values during training without performing a parameter search.

In the example below, we instruct the **ranger** engine of the Random Forest algorithm to build exactly 1000 trees and compute impurity-based variable importance.  

All main model arguments are specified directly under `tune_params`:

```{r}
# Set fixed engine parameters
model_fixed_params <- fastml(
  data = iris_binary,
  label = "Species",
  algorithms = "rand_forest",
  algorithm_engines = list(rand_forest = "ranger"),
  
  # Provide fixed engine arguments inside tune_params
  tune_params = list(
    rand_forest = list(
      ranger = list(
        trees = 1000,
        importance = "impurity"
      )
    )
  ),
  use_default_tuning = TRUE  # required to apply tune_params
)

# Inspect model parameters
summary(model_fixed_params, type = "params")
```

This approach ensures reproducibility and consistency across runs by keeping model hyperparameters constant. It is particularly useful for standardized benchmarking, simulation studies, or sensitivity analyses where tuning variability is undesirable.

# Using Advanced Backends: H2O

**fastml** extends beyond the standard **parsnip** modeling engines by supporting advanced, high-performance backends such as **H2O**.  
**H2O** is an open-source, in-memory, distributed machine learning platform designed for scalability and speed,  
capable of efficiently handling large datasets that exceed the limits of traditional in-memory R modeling workflows.

By integrating **H2O** directly, **fastml** enables seamless access to its optimized implementations of algorithms such as:

- **H2O Random Forest** (`rand_forest`, engine = `"h2o"`)  
- **H2O Gradient Boosting Machine (GBM)** (`boost_tree`, engine = `"h2o"`)  
- **H2O Deep Learning** (`mlp`, engine = `"h2o"`)  
- **H2O Generalized Linear Model (GLM)** (`linear_reg`, engine = `"h2o"`)

Using H2O through **fastml** provides parallelized training, automatic data distribution across CPU cores,  
and highly efficient memory management—all while maintaining the same simple interface as standard engines.


## One-Time Setup

Before using **H2O** as a backend, ensure that both the **h2o** and **agua** packages are installed.  
The `h2o` package provides access to the distributed machine learning platform,  
while the `agua` package serves as a bridge between **parsnip** (and thus **fastml**) and the H2O framework.

These packages only need to be installed once per R environment.

```{r, eval=FALSE}
# 1. Install the packages
install.packages("h2o")
install.packages("agua") # <-- This is the new, required package
```

After installation, **fastml** automatically detects available **H2O** engines and registers them for all compatible algorithms.  
No additional configuration is needed — once `h2o` and `agua` are installed, the backend is ready for use.

This integration allows you to train high-performance, distributed models using the same unified **fastml** syntax.  
Whether you are building tree ensembles, deep neural networks, or large-scale GLMs,  
the interface remains consistent, while computation is offloaded to the **H2O** engine for efficiency and scalability.


```{r}
# 2. Load and initialize the H2O cluster
library(h2o)
library(agua)  # must be loaded to enable H2O engines in parsnip/fastml

# Initialize the H2O backend
h2o.init()
```

Once initialized, **H2O** starts a local in-memory cluster that handles data storage, model training, and parallel computation.  
The console output confirms successful initialization, displaying details such as available CPU cores, total memory, cluster name, and node configuration.

After this setup, **fastml** can seamlessly communicate with the H2O environment,  
allowing you to train and evaluate large-scale models using distributed computation.  
All H2O-based algorithms can now be accessed through the same unified **fastml** interface —  
bringing scalability and performance without changing your workflow.

## Running an H2O Model

Once **H2O** is initialized and the `agua` package is loaded, you can specify `"h2o"` as the engine for any supported algorithm — for example, `rand_forest`.  
**fastml** automatically converts your input data frame into an **H2OFrame** and handles model training within the H2O cluster.  

Your helper scripts (such as `plot.fastml.R` and `train_models.R`) include logic to correctly manage H2O-specific behaviors, including different naming conventions for prediction columns.


```{r}
# This code assumes that 'iris_binary' was created in Section 4.
# If your R session was restarted, re-run this setup.

# Start the H2O cluster
h2o.init()

# Train a Random Forest model using the H2O engine
model_h2o <- fastml(
  data = iris_binary,
  label = "Species",
  algorithms = "rand_forest",
  algorithm_engines = list(rand_forest = "h2o")
)

# Summarize model performance
summary(model_h2o)

# Visualize comparison metrics
plot(model_h2o, type = "bar")

# Shut down the H2O cluster when finished
h2o.shutdown(prompt = FALSE)

```

Once initialized, **fastml** communicates directly with the **H2O** backend, preserving the same simple and consistent interface used for all other engines.  
Under the hood, data are automatically converted to H2OFrames, and training is distributed across available CPU cores for faster computation.

The `summary()` and `plot()` functions operate exactly as they do with standard engines,  
allowing you to evaluate and visualize model performance without changing your workflow.  
This seamless integration enables users to scale from small, local datasets to large, distributed environments with no additional configuration.

# Model Interpretation with fastexplain

Training a model is only the first step — understanding **why** it makes certain predictions is equally important for building trust, diagnosing issues, and ensuring responsible deployment.  

The **fastexplain** function in **fastml** serves as a convenient, automated interface for **model-agnostic interpretability**.  
It leverages the **DALEX** package to generate multiple complementary explanations in a single call, making interpretation accessible and consistent across models.

By default, `method = "dalex"` is used, which combines three essential interpretability tools:

1. **Feature Importance** — Quantifies each variable’s contribution to model performance.  
2. **Partial Dependence Profiles** — Illustrates how predictions change as a single feature varies.  
3. **SHAP (Shapley) Values** — Provides instance-level explanations showing how features influence individual predictions.

Let’s apply `fastexplain()` to the regression model (`model_reg`) we trained in **Section 6**.

> **Note:** The `DALEX` package must be installed to enable this functionality.

```{r, eval=FALSE}
install.packages("DALEX")
```

When you call `fastexplain()`, it automatically performs a sequence of interpretation steps for the best-performing model within the `fastml` object:

1. Creates a **DALEX explainer** for the top-ranked model.  
2. Computes and visualizes **Permutation-Based Variable Importance**.  
3. Calculates and plots **SHAP Value Summaries** for local interpretability.  
4. Generates **Partial Dependence Profiles** for specified features (if provided via the `features` argument).

Let’s demonstrate this on our pbc regression model, visualizing how **bilirubin (bili)** and **age** influence the predicted serum **albumin** levels.

```{r}
  model_reg <- fastml(
    data = pbc_baseline,
    label = "albumin",
    algorithms = c("xgboost"),
    metric = "rmse",
    impute_method = "medianImpute"
  )

  
explain_model =fastexplain(
    model_reg,
    method = "dalex",
    features = c("bili", "age"),  # specify variables for partial dependence
    shap_sample = 20,          # number of samples for SHAP computation
    vi_iterations = 15         # number of iterations for permutation importance
  )
```

This single call creates a complete interpretability dashboard.  
When you execute `fastexplain()`, it automatically performs and displays the interpretability workflow in your console.

The output log confirms the process step-by-step, typically including messages such as:

- `"A new explainer has been created!"` — indicating that a **DALEX explainer** has been successfully initialized for the selected model (e.g., *xgboost*).  
- `"DALEX Variable Importance"` — computation and plotting of **permutation-based feature importance**.  
- `"DALEX Model Profiles"` — generation of **partial dependence profiles** for selected features.  
- `"DALEX Shapley values (SHAP)"` — calculation and visualization of **feature-level contribution summaries**.

All plots are automatically rendered to the console for immediate inspection.  
However, `fastexplain()` also returns an object (commonly stored as `explain_model`) that contains the underlying data from each explanation step.

This allows for deeper exploration and customized visualization.

1. **Variable Importance:**  
A permutation-based importance plot shows how much each feature contributes to the degradation in model performance (loss) when its values are randomly shuffled.  
Features causing a greater increase in loss are more influential, as they disrupt the model’s predictive structure when permuted.

In this example, the `explain_model$variable_importance` data indicates that **edema**, **id**, and **stage** are the most critical predictors.  

```{r}
explain_model$variable_importance
```

These variables exhibit the largest `mean_dropout_loss` values — for instance, `edema` shows a mean dropout loss of approximately **0.730**, meaning that shuffling this feature leads to a substantial decline in predictive accuracy.

This confirms that the model relies heavily on these variables when estimating outcomes, making them key drivers of the prediction mechanism.
  


2. **SHAP Summary:**  
A bar plot of the mean absolute SHAP values quantifies the average magnitude of each feature’s contribution to model predictions.  
This provides a global view of how strongly each variable influences the output, regardless of direction.

The `explain_model$shap_values` data frame contains the underlying statistics for these contributions.  
For example, in this model, the feature **stage** exhibits a strong average **negative** contribution to the predicted albumin levels (mean ≈ -0.056), while **edema** shows a notable **positive** contribution (mean ≈ 0.013).  
   
```{r}
explain_model$shap_values
```

Together, these patterns reveal how different predictors systematically push the model’s predictions higher or lower,  
offering interpretable insight into the biological or clinical relevance of each feature.

3. **Partial Dependence:**  
 Line plots (for example, for **bili** and **age**) visualize how the model’s predicted **albumin** values change as each feature varies, while all other variables are held constant.  
These plots capture the *marginal effect* of each feature on the prediction, helping identify monotonic relationships, nonlinear trends, or thresholds.

The `explain_model$model_profiles` object contains the raw data underlying these plots.  
Within it, the `$agr_profiles` table provides the average predicted response (`_yhat_`, corresponding to albumin) for different values (`_x_`) of each feature —  
these averaged predictions form the basis of the partial dependence curves.
  
```{r}
explain_model$model_profiles
```

Inspecting this table allows for precise, data-level understanding of how each feature influences model output beyond the visual summaries.

These visualizations form a comprehensive, model-agnostic explanation suite — clarifying both **which features matter most** and **how** they influence the predictions produced by the trained model.



