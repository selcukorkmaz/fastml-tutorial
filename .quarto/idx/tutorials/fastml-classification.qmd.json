{"title":"Training Classification Models","markdown":{"yaml":{"title":"Training Classification Models","format":{"html":{"toc":true,"toc-depth":5,"code-link":false,"css":"styles.css","page-layout":"full"}}},"headingText":"1. Load Packages and Data","containsRefs":false,"markdown":"\n\nThe `fastml()` function lies at the heart of the fastml package, providing a unified pipeline for training and evaluating classification models with minimal code. It handles everything from data preprocessing to hyperparameter tuning, cross-validation, and model comparison — all in a single step.\n\nIn this tutorial, we'll walk through a complete binary classification workflow using the iris dataset. You'll learn how to prepare your data, train multiple models at once, evaluate their performance, and interpret the results. Whether you're a beginner or an experienced user, this example will show you how to streamline your classification tasks using fastml.\n\n\n------------------------------------------------------------------------\n\n\nWe begin by loading the necessary packages: `fastml` for model training and evaluation, and `dplyr` for data manipulation.\n\n```{r}\nlibrary(fastml)\nlibrary(dplyr)\n```\n\nTo keep things simple for this introductory classification tutorial, we'll use a modified version of the classic `iris` dataset. Specifically, we’ll filter out the \"`setosa`\" species to turn the problem into a binary classification task — predicting whether a sample belongs to versicolor or virginica.\n\n```{r}\niris_bin <- iris %>%\n  filter(Species != \"setosa\") %>%\n  mutate(Species = factor(Species))\n\nhead(iris_bin)\n```\n\nAlthough `fastml()` supports multi-class classification, we use a binary task here to simplify the initial visualization and performance interpretation. This approach makes it easier to follow the modeling process while still demonstrating the core capabilities of the `fastml` package.\n\n------------------------------------------------------------------------\n\n## 2. Train Many Models in One Shot\n\nOnce your data is ready, training and comparing multiple classification models is as simple as one function call with `fastml()`. Specify the dataset, the target label, and the algorithms you want to evaluate:\n\n```{r, warning=FALSE, message=FALSE}\nresult <- fastml(\ndata        = iris_bin,\nlabel       = \"Species\",\nalgorithms  = c(\"logistic_reg\",         \n                \"svm_rbf\",             \n                \"rand_forest\",        \n                \"xgboost\")              \n)\n```\n\n\nAll major components of a typical machine learning pipeline—data splitting, preprocessing, cross-validation, and hyperparameter tuning—are handled automatically, yet remain fully customizable via function arguments.\n\n**What just happened?**\n\n| Step | Automated task |\n|------------|------------------------------------------------------------|\n| 1 | **Stratified train/test split** based on `test_size = 0.2` (unless train_data/test_data are provided). |\n| 2 | **Automatic preprocessing recipe**, including dummy-encoding for categorical variables, centering and scaling of numeric predictors, and removal of zero-variance features. |\n| 3 | **10-fold cross-validation** within the training set using` resampling_method = \"cv\"` and `folds = 10`. |\n| 4 | **Model-specific hyperparameter tuning** via grid search (or Bayesian optimization if `tuning_strategy = \"bayes\"`) |\n| 5 | **Metric collection and model selection**, finalizing the best workflow for each algorithm. |\n\n------------------------------------------------------------------------\n\nEach component can be adjusted through the arguments of `fastml()` — for instance, you can specify your own `recipe`, choose from multiple `resampling_method` types (e.g., \"`repeatedcv`\" or \"`boot`\"), and even define custom imputation or summary functions.\n\nBy default, if `metric` is not provided, the function selects a task-appropriate metric such as accuracy for classification or RMSE for regression. The argument `task = \"auto\"` intelligently detects whether you’re solving a classification or regression problem.\n\nIn the next section, we'll explore how to compare the trained models using `summary()` and visualize key performance metrics.\n\n\n## 3. Compare Model Performance\n\nAfter training, you can summarize and compare all models using the `summary()` function. This gives a clear overview of model performance across a range of evaluation metrics:\n\n```{r}\nsummary(result, type = \"metrics\")\n```\nEach row summarizes one model’s performance, including:\n\n- **Accuracy:** Overall classification correctness.\n\n- **F1 Score:** Harmonic mean of precision and recall.\n\n- **Kappa:** Agreement adjusted for chance.\n\n- **Precision:** Proportion of positive predictions that were correct.\n\n- **Sensitivity (Recall):** Ability to identify positive cases.\n\n- **Specificity:** Ability to identify negative cases.\n\n- **ROC AUC:** Area under the ROC curve; a measure of overall discriminative ability.\n\nThe best-performing models (based on the selected `metric`) are marked with an asterisk `*`. In this case, both `rand_forest` and `xgboost` achieved the highest accuracy (0.95), making them the top candidates for deployment or further analysis.\n\nWant to focus on just a few models? You can pass a subset using the algorithm argument in `summary()`.\n\n\n## 4. Inspect Tuned Hyperparameters\n\nTo understand what made the top-performing models successful, use:\n\n```{r}\nsummary(result, type = \"params\")\n```\n\nThis will display the best hyperparameter values selected during tuning for each model. These settings are critical for optimizing performance and can inform further refinement or deployment.\n\nThese values are chosen based on cross-validation performance and reflect the best configuration for each algorithm on your specific dataset. The `fastml()` function handles this tuning process automatically, but if desired, you can manually set or constrain parameters using the `tune_params` argument.\n\n## 5. Confusion Matrix\n\nTo evaluate model performance at the prediction level, especially for classification tasks, a confusion matrix offers detailed insight. Use:\n\n```{r}\nsummary(result, type = \"conf_mat\")\n```\nThis displays the confusion matrices for each model, showing how well the predicted classes match the actual ones.\n\nThe result suggests both models performed very well on this binary classification task, with minimal confusion between the two classes. For datasets with imbalanced classes or more nuanced distinctions, confusion matrices can help pinpoint which types of errors are most common—false positives, false negatives, or both.\n\n------------------------------------------------------------------------\n\n## 6. Visualize Model Performance\n\nTo quickly compare how each model performed across different evaluation metrics, you can use the built-in `plot()` function with `type = \"bar\"`:\n\n```{r}\nplot(result, type = \"bar\")\n```\nThis generates a faceted bar plot, with one panel per metric (e.g., Accuracy, F1 Score, ROC AUC), and bars representing each model's score on that metric.\n\n**Why it’s useful**\n\n- Provides a visual overview of strengths and weaknesses across models.\n\n- Makes it easy to spot trade-offs (e.g., high accuracy but lower sensitivity).\n\n- Highlights top-performing models for each metric.\n\nEach bar is color-coded by model, and the best-performing models stand out immediately. This is especially helpful when evaluating more than just one metric, such as both accuracy and F1 score, which can differ notably in imbalanced datasets.\n\n## 7. Visualize ROC Curves\n\nTo assess the discriminative power of each model in a binary classification task, plot their Receiver Operating Characteristic (ROC) curves using:\n\n```{r}\nplot(result, type = \"roc\")\n```\n\nThis generates a faceted plot showing the ROC curve for each trained model.\n\n**What ROC curves show**\n\n- True Positive Rate (Sensitivity) vs. False Positive Rate (1 - Specificity) across thresholds.\n\n- The area under the curve (AUC) indicates overall model performance:\n\n  - AUC close to 1.0 suggests excellent discrimination.\n\n  -AUC of 0.5 means no better than random guessing.\n\n**Why it’s helpful**\n\n- Useful when dealing with imbalanced data, where accuracy may be misleading.\n\n- Allows comparison of how each model balances sensitivity and specificity.\n\n- Helps in selecting an optimal probability threshold for classification.\n\nIn our example, both `rand_forest` and xgboost achieved an AUC of 1.0—indicating perfect separation of classes on the test set.\n\nYou can also zoom in on specific models using:\n\n```{r}\nplot(result, type = \"roc\", algorithm = \"rand_forest\")\n```\n\n\n## 8. Visualize Calibration Plots\n\nTo evaluate how well your model's predicted probabilities align with observed outcomes, use a calibration plot:\n\n```{r}\nplot(result, type = \"calibration\")\n```\n\nA calibration plot compares predicted probabilities to actual event rates. Well-calibrated models produce probabilities that match the true likelihood of outcomes—for example, among all observations predicted with 70% confidence, about 70% should actually belong to the positive class.\n\n**What to look for**\n\n- Diagonal line (ideal calibration): Perfect agreement between predicted and observed probabilities.\n\n- Model curves: Deviations from the diagonal indicate overconfidence (curve above the line) or underconfidence (curve below the line).\n\n- Rug plots: Tick marks on the x-axis show the distribution of predicted probabilities.\n\n**Why it matters**\n\n- High accuracy doesn’t guarantee good probability estimates. Calibration assesses the reliability of those estimates.\n\n- Important for risk prediction, medical diagnosis, or any domain where predicted probabilities are used for decision-making.\n\nFor binary classification tasks, calibration plots provide a deeper layer of model evaluation—especially when choosing between equally accurate models.\n\n\n\n## 9. Predictions on New Data\n\nOnce you've identified the best-performing model, making predictions on new, unseen data is simple and consistent with the `predict()` function. The `fastml` package ensures that the appropriate preprocessing steps are automatically applied, so you can focus directly on generating and interpreting the predictions.\n\nAs a demonstration, let’s select a few random observations from the binary-classified iris dataset (`iris_bin`):\n\n```{r}\nnew_obs <- iris_bin[-5] %>% \n  slice_sample(n = 5)\nnew_obs\n```\n\nThis gives you 5 unseen flower samples, with measurements such as Sepal.Length, Petal.Width, etc.\n\n**Predicting Classes**\n\nTo predict the class (versicolor or virginica) for each observation:\n\n```{r}\npredict(object = result, newdata = new_obs, type = \"class\")\n```\n\n- The result is a named list with one vector per best-performing model.\n\n- Each vector contains the predicted class (versicolor or virginica) for each observation in new_obs.\n\n- If multiple models tied in performance, predictions from each are shown (e.g., rand_forest and xgboost in this case).\n\n\nYou can access specific model predictions like this:\n\n```{r}\npredict(object = result, newdata = new_obs, type = \"class\", model_name = \"rand_forest (ranger)\")\n```\n\n\n**Predicting Class Probabilities**\n\nIn addition to class labels, you can obtain probability estimates for each class. This is especially useful when you need to assess the model’s confidence in its predictions or when applying a custom decision threshold.\n\nUse the following command:\n\n```{r}\npredict(object = result, newdata = new_obs, type = \"prob\")\n```\n\nEach sublist contains a tibble with predicted probabilities for each class, corresponding to the rows in new_obs.\n\nThis is useful when:\n\n- To adjust thresholds for sensitivity/specificity trade-offs\n\n- To rank cases based on prediction confidence\n\n- For risk scoring or clinical triage tasks\n\nBoth class labels and probabilities are powered by the final model trained and selected via `fastml()`—you don’t need to manually reapply recipes or preprocessing steps.\n\nIn the next step, you can use SHAP explanations to understand why the model made these predictions.\n\n\n## 10. Model Explainability\n\nUnderstanding why a model makes a certain prediction is just as important as how accurate it is—especially in fields like healthcare, finance, and policy. The `fastexplain()` function in fastml integrates with the DALEX package to provide a suite of explainability tools.\n\nRun the following to generate explanations for the best model(s):\n\n```{r}\nfastexplain(result)       \n```\n\nThis process wraps your trained models into DALEX explainers, enabling multiple forms of model interpretation.\n\n**What you get**\n\nOnce the explainers are created, several plots and summaries are available:\n\n- **Permutation-based Variable Importance:** Quantifies the contribution of each feature to model accuracy by measuring the drop in performance when the feature is randomly permuted.\n\n- **SHAP Values (Shapley Additive Explanations):** Breaks down individual predictions into feature-level contributions, helping you understand why a particular prediction was made.\n\n- **Partial Dependence and Model Profiles:** Visualizes how predictions change across a range of values for a selected feature, isolating its effect while averaging out others.\n\n\n## Advanced Options Cheat‑Sheet\n\nThe `fastml()` function supports a variety of advanced arguments to give you full control over your machine learning pipeline. Here's a quick reference:\n\n| Argument            | Purpose                                                                                                                               | Example                                                                  |\n|---------------------|----------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------|\n| `recipe`            | Supply a custom [**recipes**](https://recipes.tidymodels.org) object to override the automatic preprocessing pipeline                 | `recipe(Species ~ ., data = iris_bin) %>% step_normalize(all_numeric())` |\n| `impute_method`     | Specify how missing values are handled. Supported methods include `\"medianImpute\"`, `\"mice\"`, `\"missForest\"`, `\"knnImpute\"`, and more | `impute_method = \"knnImpute\"`                                            |\n| `algorithm_engines` | Customize which engine is used for each algorithm                                                                                     | `list(rand_forest = \"randomForest\", svm_rbf = \"kernlab\")`                |\n| `tuning_strategy`   | Choose hyperparameter search strategy: `\"grid\"` (default) or `\"bayes\"` (Bayesian optimization with optional `early_stopping`)         | `tuning_strategy = \"bayes\", tuning_iterations = 25`                      |\n| `learning_curve`    | Generate learning curves to visualize performance vs. training size                                                                   | `learning_curve = TRUE`                                                  |\n\nThese options allow for deeper experimentation and optimization, especially when working with complex datasets or performance-critical tasks.\n\nTip: Explore `?fastml` in your R console to see the full list of arguments and their descriptions.\n\nBy combining ease of use with advanced flexibility, `fastml` helps both beginners and experienced users build robust, interpretable, and high-performing machine learning models.\n\n","srcMarkdownNoYaml":"\n\nThe `fastml()` function lies at the heart of the fastml package, providing a unified pipeline for training and evaluating classification models with minimal code. It handles everything from data preprocessing to hyperparameter tuning, cross-validation, and model comparison — all in a single step.\n\nIn this tutorial, we'll walk through a complete binary classification workflow using the iris dataset. You'll learn how to prepare your data, train multiple models at once, evaluate their performance, and interpret the results. Whether you're a beginner or an experienced user, this example will show you how to streamline your classification tasks using fastml.\n\n\n------------------------------------------------------------------------\n\n## 1. Load Packages and Data\n\nWe begin by loading the necessary packages: `fastml` for model training and evaluation, and `dplyr` for data manipulation.\n\n```{r}\nlibrary(fastml)\nlibrary(dplyr)\n```\n\nTo keep things simple for this introductory classification tutorial, we'll use a modified version of the classic `iris` dataset. Specifically, we’ll filter out the \"`setosa`\" species to turn the problem into a binary classification task — predicting whether a sample belongs to versicolor or virginica.\n\n```{r}\niris_bin <- iris %>%\n  filter(Species != \"setosa\") %>%\n  mutate(Species = factor(Species))\n\nhead(iris_bin)\n```\n\nAlthough `fastml()` supports multi-class classification, we use a binary task here to simplify the initial visualization and performance interpretation. This approach makes it easier to follow the modeling process while still demonstrating the core capabilities of the `fastml` package.\n\n------------------------------------------------------------------------\n\n## 2. Train Many Models in One Shot\n\nOnce your data is ready, training and comparing multiple classification models is as simple as one function call with `fastml()`. Specify the dataset, the target label, and the algorithms you want to evaluate:\n\n```{r, warning=FALSE, message=FALSE}\nresult <- fastml(\ndata        = iris_bin,\nlabel       = \"Species\",\nalgorithms  = c(\"logistic_reg\",         \n                \"svm_rbf\",             \n                \"rand_forest\",        \n                \"xgboost\")              \n)\n```\n\n\nAll major components of a typical machine learning pipeline—data splitting, preprocessing, cross-validation, and hyperparameter tuning—are handled automatically, yet remain fully customizable via function arguments.\n\n**What just happened?**\n\n| Step | Automated task |\n|------------|------------------------------------------------------------|\n| 1 | **Stratified train/test split** based on `test_size = 0.2` (unless train_data/test_data are provided). |\n| 2 | **Automatic preprocessing recipe**, including dummy-encoding for categorical variables, centering and scaling of numeric predictors, and removal of zero-variance features. |\n| 3 | **10-fold cross-validation** within the training set using` resampling_method = \"cv\"` and `folds = 10`. |\n| 4 | **Model-specific hyperparameter tuning** via grid search (or Bayesian optimization if `tuning_strategy = \"bayes\"`) |\n| 5 | **Metric collection and model selection**, finalizing the best workflow for each algorithm. |\n\n------------------------------------------------------------------------\n\nEach component can be adjusted through the arguments of `fastml()` — for instance, you can specify your own `recipe`, choose from multiple `resampling_method` types (e.g., \"`repeatedcv`\" or \"`boot`\"), and even define custom imputation or summary functions.\n\nBy default, if `metric` is not provided, the function selects a task-appropriate metric such as accuracy for classification or RMSE for regression. The argument `task = \"auto\"` intelligently detects whether you’re solving a classification or regression problem.\n\nIn the next section, we'll explore how to compare the trained models using `summary()` and visualize key performance metrics.\n\n\n## 3. Compare Model Performance\n\nAfter training, you can summarize and compare all models using the `summary()` function. This gives a clear overview of model performance across a range of evaluation metrics:\n\n```{r}\nsummary(result, type = \"metrics\")\n```\nEach row summarizes one model’s performance, including:\n\n- **Accuracy:** Overall classification correctness.\n\n- **F1 Score:** Harmonic mean of precision and recall.\n\n- **Kappa:** Agreement adjusted for chance.\n\n- **Precision:** Proportion of positive predictions that were correct.\n\n- **Sensitivity (Recall):** Ability to identify positive cases.\n\n- **Specificity:** Ability to identify negative cases.\n\n- **ROC AUC:** Area under the ROC curve; a measure of overall discriminative ability.\n\nThe best-performing models (based on the selected `metric`) are marked with an asterisk `*`. In this case, both `rand_forest` and `xgboost` achieved the highest accuracy (0.95), making them the top candidates for deployment or further analysis.\n\nWant to focus on just a few models? You can pass a subset using the algorithm argument in `summary()`.\n\n\n## 4. Inspect Tuned Hyperparameters\n\nTo understand what made the top-performing models successful, use:\n\n```{r}\nsummary(result, type = \"params\")\n```\n\nThis will display the best hyperparameter values selected during tuning for each model. These settings are critical for optimizing performance and can inform further refinement or deployment.\n\nThese values are chosen based on cross-validation performance and reflect the best configuration for each algorithm on your specific dataset. The `fastml()` function handles this tuning process automatically, but if desired, you can manually set or constrain parameters using the `tune_params` argument.\n\n## 5. Confusion Matrix\n\nTo evaluate model performance at the prediction level, especially for classification tasks, a confusion matrix offers detailed insight. Use:\n\n```{r}\nsummary(result, type = \"conf_mat\")\n```\nThis displays the confusion matrices for each model, showing how well the predicted classes match the actual ones.\n\nThe result suggests both models performed very well on this binary classification task, with minimal confusion between the two classes. For datasets with imbalanced classes or more nuanced distinctions, confusion matrices can help pinpoint which types of errors are most common—false positives, false negatives, or both.\n\n------------------------------------------------------------------------\n\n## 6. Visualize Model Performance\n\nTo quickly compare how each model performed across different evaluation metrics, you can use the built-in `plot()` function with `type = \"bar\"`:\n\n```{r}\nplot(result, type = \"bar\")\n```\nThis generates a faceted bar plot, with one panel per metric (e.g., Accuracy, F1 Score, ROC AUC), and bars representing each model's score on that metric.\n\n**Why it’s useful**\n\n- Provides a visual overview of strengths and weaknesses across models.\n\n- Makes it easy to spot trade-offs (e.g., high accuracy but lower sensitivity).\n\n- Highlights top-performing models for each metric.\n\nEach bar is color-coded by model, and the best-performing models stand out immediately. This is especially helpful when evaluating more than just one metric, such as both accuracy and F1 score, which can differ notably in imbalanced datasets.\n\n## 7. Visualize ROC Curves\n\nTo assess the discriminative power of each model in a binary classification task, plot their Receiver Operating Characteristic (ROC) curves using:\n\n```{r}\nplot(result, type = \"roc\")\n```\n\nThis generates a faceted plot showing the ROC curve for each trained model.\n\n**What ROC curves show**\n\n- True Positive Rate (Sensitivity) vs. False Positive Rate (1 - Specificity) across thresholds.\n\n- The area under the curve (AUC) indicates overall model performance:\n\n  - AUC close to 1.0 suggests excellent discrimination.\n\n  -AUC of 0.5 means no better than random guessing.\n\n**Why it’s helpful**\n\n- Useful when dealing with imbalanced data, where accuracy may be misleading.\n\n- Allows comparison of how each model balances sensitivity and specificity.\n\n- Helps in selecting an optimal probability threshold for classification.\n\nIn our example, both `rand_forest` and xgboost achieved an AUC of 1.0—indicating perfect separation of classes on the test set.\n\nYou can also zoom in on specific models using:\n\n```{r}\nplot(result, type = \"roc\", algorithm = \"rand_forest\")\n```\n\n\n## 8. Visualize Calibration Plots\n\nTo evaluate how well your model's predicted probabilities align with observed outcomes, use a calibration plot:\n\n```{r}\nplot(result, type = \"calibration\")\n```\n\nA calibration plot compares predicted probabilities to actual event rates. Well-calibrated models produce probabilities that match the true likelihood of outcomes—for example, among all observations predicted with 70% confidence, about 70% should actually belong to the positive class.\n\n**What to look for**\n\n- Diagonal line (ideal calibration): Perfect agreement between predicted and observed probabilities.\n\n- Model curves: Deviations from the diagonal indicate overconfidence (curve above the line) or underconfidence (curve below the line).\n\n- Rug plots: Tick marks on the x-axis show the distribution of predicted probabilities.\n\n**Why it matters**\n\n- High accuracy doesn’t guarantee good probability estimates. Calibration assesses the reliability of those estimates.\n\n- Important for risk prediction, medical diagnosis, or any domain where predicted probabilities are used for decision-making.\n\nFor binary classification tasks, calibration plots provide a deeper layer of model evaluation—especially when choosing between equally accurate models.\n\n\n\n## 9. Predictions on New Data\n\nOnce you've identified the best-performing model, making predictions on new, unseen data is simple and consistent with the `predict()` function. The `fastml` package ensures that the appropriate preprocessing steps are automatically applied, so you can focus directly on generating and interpreting the predictions.\n\nAs a demonstration, let’s select a few random observations from the binary-classified iris dataset (`iris_bin`):\n\n```{r}\nnew_obs <- iris_bin[-5] %>% \n  slice_sample(n = 5)\nnew_obs\n```\n\nThis gives you 5 unseen flower samples, with measurements such as Sepal.Length, Petal.Width, etc.\n\n**Predicting Classes**\n\nTo predict the class (versicolor or virginica) for each observation:\n\n```{r}\npredict(object = result, newdata = new_obs, type = \"class\")\n```\n\n- The result is a named list with one vector per best-performing model.\n\n- Each vector contains the predicted class (versicolor or virginica) for each observation in new_obs.\n\n- If multiple models tied in performance, predictions from each are shown (e.g., rand_forest and xgboost in this case).\n\n\nYou can access specific model predictions like this:\n\n```{r}\npredict(object = result, newdata = new_obs, type = \"class\", model_name = \"rand_forest (ranger)\")\n```\n\n\n**Predicting Class Probabilities**\n\nIn addition to class labels, you can obtain probability estimates for each class. This is especially useful when you need to assess the model’s confidence in its predictions or when applying a custom decision threshold.\n\nUse the following command:\n\n```{r}\npredict(object = result, newdata = new_obs, type = \"prob\")\n```\n\nEach sublist contains a tibble with predicted probabilities for each class, corresponding to the rows in new_obs.\n\nThis is useful when:\n\n- To adjust thresholds for sensitivity/specificity trade-offs\n\n- To rank cases based on prediction confidence\n\n- For risk scoring or clinical triage tasks\n\nBoth class labels and probabilities are powered by the final model trained and selected via `fastml()`—you don’t need to manually reapply recipes or preprocessing steps.\n\nIn the next step, you can use SHAP explanations to understand why the model made these predictions.\n\n\n## 10. Model Explainability\n\nUnderstanding why a model makes a certain prediction is just as important as how accurate it is—especially in fields like healthcare, finance, and policy. The `fastexplain()` function in fastml integrates with the DALEX package to provide a suite of explainability tools.\n\nRun the following to generate explanations for the best model(s):\n\n```{r}\nfastexplain(result)       \n```\n\nThis process wraps your trained models into DALEX explainers, enabling multiple forms of model interpretation.\n\n**What you get**\n\nOnce the explainers are created, several plots and summaries are available:\n\n- **Permutation-based Variable Importance:** Quantifies the contribution of each feature to model accuracy by measuring the drop in performance when the feature is randomly permuted.\n\n- **SHAP Values (Shapley Additive Explanations):** Breaks down individual predictions into feature-level contributions, helping you understand why a particular prediction was made.\n\n- **Partial Dependence and Model Profiles:** Visualizes how predictions change across a range of values for a selected feature, isolating its effect while averaging out others.\n\n\n## Advanced Options Cheat‑Sheet\n\nThe `fastml()` function supports a variety of advanced arguments to give you full control over your machine learning pipeline. Here's a quick reference:\n\n| Argument            | Purpose                                                                                                                               | Example                                                                  |\n|---------------------|----------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------|\n| `recipe`            | Supply a custom [**recipes**](https://recipes.tidymodels.org) object to override the automatic preprocessing pipeline                 | `recipe(Species ~ ., data = iris_bin) %>% step_normalize(all_numeric())` |\n| `impute_method`     | Specify how missing values are handled. Supported methods include `\"medianImpute\"`, `\"mice\"`, `\"missForest\"`, `\"knnImpute\"`, and more | `impute_method = \"knnImpute\"`                                            |\n| `algorithm_engines` | Customize which engine is used for each algorithm                                                                                     | `list(rand_forest = \"randomForest\", svm_rbf = \"kernlab\")`                |\n| `tuning_strategy`   | Choose hyperparameter search strategy: `\"grid\"` (default) or `\"bayes\"` (Bayesian optimization with optional `early_stopping`)         | `tuning_strategy = \"bayes\", tuning_iterations = 25`                      |\n| `learning_curve`    | Generate learning curves to visualize performance vs. training size                                                                   | `learning_curve = TRUE`                                                  |\n\nThese options allow for deeper experimentation and optimization, especially when working with complex datasets or performance-critical tasks.\n\nTip: Explore `?fastml` in your R console to see the full list of arguments and their descriptions.\n\nBy combining ease of use with advanced flexibility, `fastml` helps both beginners and experienced users build robust, interpretable, and high-performing machine learning models.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"css":["styles.css"],"toc-depth":5,"output-file":"fastml-classification.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.30","theme":"cosmo","code-copy":true,"title":"Training Classification Models","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}