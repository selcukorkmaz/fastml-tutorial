{"title":"Predicting Continuous Outcomes","markdown":{"yaml":{"title":"Predicting Continuous Outcomes","format":"html","editor":"visual"},"headingText":"1. Load Packages and Data","containsRefs":false,"markdown":"\n\n\nIn this tutorial, we’ll use `fastml` to tackle a regression problem—specifically, predicting house sale prices. Just like in classification, the `fastml()` function streamlines the entire modeling workflow for continuous targets with minimal setup.\n\nYou’ll see how it:\n\n- Automatically handles data splitting and preprocessing,\n\n- Applies appropriate resampling methods (like cross-validation),\n\n- Tunes hyperparameters across multiple regression algorithms,\n\n- Compares model performance using metrics such as RMSE and R².\n\nWhether you’re forecasting prices, estimating risk scores, or predicting any numeric outcome, `fastml()` makes regression modeling just as effortless and reproducible as classification. Let’s walk through a real-world example step by step.\n\n\n------------------------------------------------------------------------\n\n\nIn this regression example, we’ll use a medical dataset to predict a continuous outcome—specifically, Body Mass Index (BMI) based on various clinical features. This simulates a common scenario in healthcare: estimating a patient's health metric using routine measurements.\n\nWe’ll use the Pima Indians Diabetes dataset, available via the `mlbench` package. It contains health data from adult female patients of Pima Indian heritage.\n\n```{r, warning=FALSE}\nlibrary(fastml)\nlibrary(dplyr)\nlibrary(mlbench)\n\ndata(PimaIndiansDiabetes)\n\n# Prepare dataset: rename 'mass' to 'BMI' and drop 'diabetes'\npima_reg <- PimaIndiansDiabetes %>%\n  rename(BMI = mass) %>%\n  select(-diabetes) %>%\n  filter(BMI > 0)  # remove invalid BMI values (e.g., 0)\n\nhead(pima_reg)\n```\n\nThe resulting dataset includes 768 rows and 8 predictor variables:\n\n- **pregnant**: Number of times pregnant,\n\n- **glucose**: Plasma glucose concentration,\n\n- **pressure**:\tDiastolic blood pressure (mm Hg),\n\n- **triceps**: Triceps skin fold thickness (mm),\n\n- **insulin**: 2-hour serum insulin (μU/mL),\n\n- **BMI**: Body mass index (target variable here),\n\n- **pedigree**: Diabetes pedigree function\n\n- **age**: Age in years\n\n\nIn the next step, we'll explore the dataset and launch a full regression workflow using fastml().\n\n\n\n------------------------------------------------------------------------\n\n## 2. Train Several Regression Models\n\nWith the dataset prepared, we can now use `fastml()` to train and evaluate multiple regression models in a single step. Just like in classification, the function takes care of splitting the data, preprocessing, resampling, and hyperparameter tuning.\n\nIn this example, we'll train four common regression algorithms to predict BMI:\n\n- Linear regression (`linear_reg`)\n\n- Support Vector Machine with radial kernel (`svm_rbf`)\n\n- Random Forest (`rand_forest`)\n\n- LightGBM (`lightgbm`)\n\nLet’s run the full pipeline:\n\n```{r, message=FALSE, warning=FALSE}\nresult <- fastml(\n  data       = pima_reg,\n  label      = \"BMI\",\n  algorithms = c(\"linear_reg\", \"svm_rbf\", \"rand_forest\", \"lightgbm\")\n)\n```\n\n\n**What happens under the hood?**\n\n1.  **Recipe**: median‑imputes NAs, one‑hot‑encodes categoricals, centres & scales numerics.\n2.  **Resampling**: 10‑fold CV within the train split.\n3.  **Tuning grids**: automatically generated per algorithm.\n4.  **Finalisation**: the best hyper‑parameters are selected and the workflow refit on the full training data.\n\nYou don’t need to worry about creating resampling objects or preprocessing steps manually—everything is handled internally, while remaining fully customizable via optional arguments.\n\nIn the next section, we'll examine and compare the performance of the trained regression models using summary metrics like RMSE and R-squared.\n\n------------------------------------------------------------------------\n\n## 3. Compare Model Performance\n\nAfter training, you can evaluate how each model performed using the summary() function. This provides a comprehensive overview of model performance based on regression metrics such as:\n\n- **RMSE** (Root Mean Squared Error): Measures average prediction error magnitude.\n\n- *R-squared* (Coefficient of Determination): Indicates how well the model explains variability in the outcome.\n\n- **MAE** (Mean Absolute Error): Average absolute difference between predicted and actual values.\n\nTo view the results:\n\n```{r}\nsummary(result, type = \"metrics\")\n```\nFrom this output, we see that the lightgbm model achieved the lowest RMSE, indicating the most accurate predictions overall. It also has the highest R-squared, suggesting it explains a greater portion of variance in BMI compared to other models.\n\nYou can also plot the performance of all models across metrics using:\n\n```{r}\nplot(result, type = \"bar\")\n```\nThis generates a faceted bar plot, making it easy to compare RMSE, R-squared, and MAE visually.\n\nIn the next step, we’ll take a closer look at the best model's tuned hyperparameters.\n\n\n------------------------------------------------------------------------\n\n## 4. Inspect the Best Model\n\nOnce the models have been trained and evaluated, `fastml()` automatically identifies the best-performing model based on the optimization metric you specified (e.g., RMSE). You can inspect which model was selected and view its internal details using:\n\n```{r}\nresult$best_model_name\n```\n\nThis tells us that the best-performing model is a LightGBM gradient boosting machine.\n\nTo view the trained model’s workflow—including the preprocessing steps, model specification, and tuning results—use:\n\n```{r}\nresult$best_model\n```\n\nThis reveals:\n\n- The full tidymodels workflow, including the recipe and the fitted model.\n\n- The model engine and its finalized hyperparameters.\n\n- A summary of the training process and resampling results.\n\nThis confirms that:\n\n- Preprocessing included zero-variance filtering, dummy encoding (if applicable), centering, and scaling.\n\n- The model is a boosted tree trained with LightGBM, using 100 trees.\n\nYou can also retrieve the exact hyperparameters selected during tuning using:\n\n```{r}\nsummary(result, type = \"params\")\n```\n\nThis level of detail is helpful for understanding model complexity, reproducibility, and for future deployment.\n\nIn the next section, we’ll use the selected model to make predictions on new, unseen data.\n\n## 5. Predict on New Observations\n\nOnce you've identified and reviewed the best model, you can use it to make predictions on new, unseen data. \n\nStart by sampling a few observations from the dataset (or use an external dataset if available):\n\n```{r}\n# Sample 5 new observations\nnew_obs <- pima_reg %>% \n  slice_sample(n = 5) %>% \n  dplyr::select(-BMI) \n\nnew_obs\n```\n\nUse the predict() function to generate predicted BMI values:\n\n```{r}\npredict(result, new_obs)\n```\n\n------------------------------------------------------------------------\n\n## 6. Variable Importance & SHAP Values\n\nIn medical and clinical applications, understanding how a model arrives at its prediction is often just as crucial as the prediction itself. For this reason, `fastml` provides SHAP (Shapley Additive Explanations) support via the `fastexplain()` function—allowing you to interpret individual-level predictions from your best model.\n\nTo compute and visualize SHAP values:\n\n```{r}\nfastexplain(result)\n```\n\nThis produces two key plots:\n\n**Feature Importance (Permutation-Based)**\n\n- This plot shows how much the model’s RMSE increases when each feature is permuted (i.e., randomly shuffled).\n\n- The larger the increase in error, the more important the feature is to the model's performance.\n\n- Triceps skinfold thickness is by far the most important feature, followed by glucose, blood pressure, and age.\n\n**SHAP Values (Local Explainability)**\n\n- SHAP values break down each individual prediction into feature-level contributions.\n\n- Positive SHAP values increase the predicted BMI; negative ones decrease it.\n\n- In this example:\n\n  - Triceps and pressure have strong positive contributions.\n\n  - Pregnancy count and age tend to lower BMI predictions for certain individuals.\n\n  - Glucose and pedigree show more mixed or subtle effects.\n  \n**Interpretation Highlights**\n\n- Triceps consistently emerges as the most influential variable, both globally and locally.\n\n- Some variables (like glucose) may be important for prediction accuracy (as seen in the permutation plot) but may not contribute uniformly across all patients (as reflected in SHAP values).\n\n- SHAP plots also help identify non-linear or interaction effects—for example, how pregnant affects predictions differently depending on other inputs.\n\n------------------------------------------------------------------------\n\n\n\n\n","srcMarkdownNoYaml":"\n\n\nIn this tutorial, we’ll use `fastml` to tackle a regression problem—specifically, predicting house sale prices. Just like in classification, the `fastml()` function streamlines the entire modeling workflow for continuous targets with minimal setup.\n\nYou’ll see how it:\n\n- Automatically handles data splitting and preprocessing,\n\n- Applies appropriate resampling methods (like cross-validation),\n\n- Tunes hyperparameters across multiple regression algorithms,\n\n- Compares model performance using metrics such as RMSE and R².\n\nWhether you’re forecasting prices, estimating risk scores, or predicting any numeric outcome, `fastml()` makes regression modeling just as effortless and reproducible as classification. Let’s walk through a real-world example step by step.\n\n\n------------------------------------------------------------------------\n\n## 1. Load Packages and Data\n\nIn this regression example, we’ll use a medical dataset to predict a continuous outcome—specifically, Body Mass Index (BMI) based on various clinical features. This simulates a common scenario in healthcare: estimating a patient's health metric using routine measurements.\n\nWe’ll use the Pima Indians Diabetes dataset, available via the `mlbench` package. It contains health data from adult female patients of Pima Indian heritage.\n\n```{r, warning=FALSE}\nlibrary(fastml)\nlibrary(dplyr)\nlibrary(mlbench)\n\ndata(PimaIndiansDiabetes)\n\n# Prepare dataset: rename 'mass' to 'BMI' and drop 'diabetes'\npima_reg <- PimaIndiansDiabetes %>%\n  rename(BMI = mass) %>%\n  select(-diabetes) %>%\n  filter(BMI > 0)  # remove invalid BMI values (e.g., 0)\n\nhead(pima_reg)\n```\n\nThe resulting dataset includes 768 rows and 8 predictor variables:\n\n- **pregnant**: Number of times pregnant,\n\n- **glucose**: Plasma glucose concentration,\n\n- **pressure**:\tDiastolic blood pressure (mm Hg),\n\n- **triceps**: Triceps skin fold thickness (mm),\n\n- **insulin**: 2-hour serum insulin (μU/mL),\n\n- **BMI**: Body mass index (target variable here),\n\n- **pedigree**: Diabetes pedigree function\n\n- **age**: Age in years\n\n\nIn the next step, we'll explore the dataset and launch a full regression workflow using fastml().\n\n\n\n------------------------------------------------------------------------\n\n## 2. Train Several Regression Models\n\nWith the dataset prepared, we can now use `fastml()` to train and evaluate multiple regression models in a single step. Just like in classification, the function takes care of splitting the data, preprocessing, resampling, and hyperparameter tuning.\n\nIn this example, we'll train four common regression algorithms to predict BMI:\n\n- Linear regression (`linear_reg`)\n\n- Support Vector Machine with radial kernel (`svm_rbf`)\n\n- Random Forest (`rand_forest`)\n\n- LightGBM (`lightgbm`)\n\nLet’s run the full pipeline:\n\n```{r, message=FALSE, warning=FALSE}\nresult <- fastml(\n  data       = pima_reg,\n  label      = \"BMI\",\n  algorithms = c(\"linear_reg\", \"svm_rbf\", \"rand_forest\", \"lightgbm\")\n)\n```\n\n\n**What happens under the hood?**\n\n1.  **Recipe**: median‑imputes NAs, one‑hot‑encodes categoricals, centres & scales numerics.\n2.  **Resampling**: 10‑fold CV within the train split.\n3.  **Tuning grids**: automatically generated per algorithm.\n4.  **Finalisation**: the best hyper‑parameters are selected and the workflow refit on the full training data.\n\nYou don’t need to worry about creating resampling objects or preprocessing steps manually—everything is handled internally, while remaining fully customizable via optional arguments.\n\nIn the next section, we'll examine and compare the performance of the trained regression models using summary metrics like RMSE and R-squared.\n\n------------------------------------------------------------------------\n\n## 3. Compare Model Performance\n\nAfter training, you can evaluate how each model performed using the summary() function. This provides a comprehensive overview of model performance based on regression metrics such as:\n\n- **RMSE** (Root Mean Squared Error): Measures average prediction error magnitude.\n\n- *R-squared* (Coefficient of Determination): Indicates how well the model explains variability in the outcome.\n\n- **MAE** (Mean Absolute Error): Average absolute difference between predicted and actual values.\n\nTo view the results:\n\n```{r}\nsummary(result, type = \"metrics\")\n```\nFrom this output, we see that the lightgbm model achieved the lowest RMSE, indicating the most accurate predictions overall. It also has the highest R-squared, suggesting it explains a greater portion of variance in BMI compared to other models.\n\nYou can also plot the performance of all models across metrics using:\n\n```{r}\nplot(result, type = \"bar\")\n```\nThis generates a faceted bar plot, making it easy to compare RMSE, R-squared, and MAE visually.\n\nIn the next step, we’ll take a closer look at the best model's tuned hyperparameters.\n\n\n------------------------------------------------------------------------\n\n## 4. Inspect the Best Model\n\nOnce the models have been trained and evaluated, `fastml()` automatically identifies the best-performing model based on the optimization metric you specified (e.g., RMSE). You can inspect which model was selected and view its internal details using:\n\n```{r}\nresult$best_model_name\n```\n\nThis tells us that the best-performing model is a LightGBM gradient boosting machine.\n\nTo view the trained model’s workflow—including the preprocessing steps, model specification, and tuning results—use:\n\n```{r}\nresult$best_model\n```\n\nThis reveals:\n\n- The full tidymodels workflow, including the recipe and the fitted model.\n\n- The model engine and its finalized hyperparameters.\n\n- A summary of the training process and resampling results.\n\nThis confirms that:\n\n- Preprocessing included zero-variance filtering, dummy encoding (if applicable), centering, and scaling.\n\n- The model is a boosted tree trained with LightGBM, using 100 trees.\n\nYou can also retrieve the exact hyperparameters selected during tuning using:\n\n```{r}\nsummary(result, type = \"params\")\n```\n\nThis level of detail is helpful for understanding model complexity, reproducibility, and for future deployment.\n\nIn the next section, we’ll use the selected model to make predictions on new, unseen data.\n\n## 5. Predict on New Observations\n\nOnce you've identified and reviewed the best model, you can use it to make predictions on new, unseen data. \n\nStart by sampling a few observations from the dataset (or use an external dataset if available):\n\n```{r}\n# Sample 5 new observations\nnew_obs <- pima_reg %>% \n  slice_sample(n = 5) %>% \n  dplyr::select(-BMI) \n\nnew_obs\n```\n\nUse the predict() function to generate predicted BMI values:\n\n```{r}\npredict(result, new_obs)\n```\n\n------------------------------------------------------------------------\n\n## 6. Variable Importance & SHAP Values\n\nIn medical and clinical applications, understanding how a model arrives at its prediction is often just as crucial as the prediction itself. For this reason, `fastml` provides SHAP (Shapley Additive Explanations) support via the `fastexplain()` function—allowing you to interpret individual-level predictions from your best model.\n\nTo compute and visualize SHAP values:\n\n```{r}\nfastexplain(result)\n```\n\nThis produces two key plots:\n\n**Feature Importance (Permutation-Based)**\n\n- This plot shows how much the model’s RMSE increases when each feature is permuted (i.e., randomly shuffled).\n\n- The larger the increase in error, the more important the feature is to the model's performance.\n\n- Triceps skinfold thickness is by far the most important feature, followed by glucose, blood pressure, and age.\n\n**SHAP Values (Local Explainability)**\n\n- SHAP values break down each individual prediction into feature-level contributions.\n\n- Positive SHAP values increase the predicted BMI; negative ones decrease it.\n\n- In this example:\n\n  - Triceps and pressure have strong positive contributions.\n\n  - Pregnancy count and age tend to lower BMI predictions for certain individuals.\n\n  - Glucose and pedigree show more mixed or subtle effects.\n  \n**Interpretation Highlights**\n\n- Triceps consistently emerges as the most influential variable, both globally and locally.\n\n- Some variables (like glucose) may be important for prediction accuracy (as seen in the permutation plot) but may not contribute uniformly across all patients (as reflected in SHAP values).\n\n- SHAP plots also help identify non-linear or interaction effects—for example, how pregnant affects predictions differently depending on other inputs.\n\n------------------------------------------------------------------------\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"css":["styles.css"],"output-file":"fastml-regression.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.30","theme":"cosmo","code-copy":true,"title":"Predicting Continuous Outcomes","editor":"visual"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}