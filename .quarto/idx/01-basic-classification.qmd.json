{"title":"01. Basic Classification","markdown":{"yaml":{"title":"01. Basic Classification","execute":{"warning":false,"message":false}},"headingText":"Before you start","containsRefs":false,"markdown":"\n\n\nThis tutorial builds on the following conceptual material:\n\n-   **[C1. What is Data Leakage](C1-what-is-data-leakage.qmd)**\n-   **[C2. Why Most ML Pipelines Are Unsafe by Default](C2-why-most-ml-pipelines-are-unsafe-by-default.qmd)**\n-   **[C3. Guarded Resampling](C3-guarded-resampling.qmd)**\n-   **[C4. What fastml Deliberately Does Not Allow](C4-what-fastml-does-not-allow.qmd)**\n\nReaders are expected to be familiar with these concepts before proceeding.\n\nThe workflow demonstrated here uses a deliberately constrained interface. These constraints are intentional and reflect the design philosophy of fastml: to reduce common sources of evaluation error by limiting user-facing degrees of freedom along the default execution path.\n\n## The problem we are solving\n\nThe goal of this tutorial is to estimate the **out-of-sample performance** of a binary classifier.\n\nThe focus is not on maximizing predictive accuracy, tuning hyperparameters, or examining model internals. Instead, the objective is narrowly defined:\n\n**What level of predictive performance can reasonably be expected on new, unseen data, given a fixed modeling specification?**\n\nIn this setting, model fitting itself is straightforward. The primary challenge lies in performance evaluation.\n\nSpecifically, the difficulty is to obtain an estimate that is not biased by information leakage or other forms of contamination arising from reuse of the data during training and evaluation.\n\n## Why this is harder than it sounds\n\nFrom **C1** and **C2**, recall the following points:\n\n-   Leakage does not require obvious mistakes.\n-   Pipelines that appear valid can still yield biased performance estimates.\n-   Most machine learning frameworks allow such pipelines to run without warnings.\n\nFrom **C3**, recall:\n\n-   Correct evaluation requires resampling to *enclose* preprocessing and model fitting.\n-   This enclosure must be structural, not procedural.\n\nFrom **C4**, recall:\n\n-   fastml enforces correctness by removing degrees of freedom rather than relying on user discipline.\n\nThis tutorial demonstrates what that enforcement looks like in practice.\n\n## The data\n\nThis tutorial uses a simple binary classification dataset from the **modeldata** package.\n\nThe dataset is characterized by:\n\n- a binary outcome variable (`Class`),\n- a small number of continuous predictors (`A` and `B`),\n- the absence of missing values.\n\nThis choice is intentional. The dataset is deliberately low-dimensional and clean, so that attention can remain on the evaluation procedure rather than on feature engineering, preprocessing decisions, or data-quality issues.\n\n\n```{r}\nlibrary(modeldata)\ndata(two_class_dat)\nhead(two_class_dat)\n```\n\nThe purpose of this example is to illustrate evaluation mechanics rather than data preprocessing challenges. Although the dataset itself is simple, the evaluation principles demonstrated here extend to more complex datasets and other supported tasks in fastml, subject to their respective assumptions and constraints.\n\n## What you do not need to do in fastml\n\nBefore showing the workflow, it is important to be explicit.\n\nIn **fastml**, users are not required to:\n\n-   manually assemble train–test splits,\n-   explicitly construct preprocessing recipes,\n-   apply scaling or imputation outside the resampling loop,\n-   compose workflows from loosely coupled components,\n-   control when preprocessing is trained relative to resampling,\n-   directly manipulate resampling objects during model execution.\n\nThese steps are common entry points for data leakage.\n\nBy default, fastml executes preprocessing, model fitting, and evaluation within a single, resampling-aware structure. While advanced users may override specific components, the standard execution path is designed to preserve training–assessment isolation without relying on user discipline.\n\n## Declaring intent\n\nIn **fastml**, the user specifies *what is to be evaluated* rather than manually assembling the individual components of a modeling pipeline.\n\nAt a minimum, this specification includes:\n\n-   the dataset,\n-   the outcome variable,\n-   the set of algorithms to be evaluated,\n-   the intended resampling strategy.\n\n```{r}\nlibrary(fastml)\n\nfit <- fastml(\n  data       = two_class_dat,\n  label      = \"Class\",\n  algorithms = c(\"rand_forest\", \"xgboost\"),\n  resampling = \"cv\",\n  folds      = 5,\n)\n```\n\nThis call defines the full evaluation setup under the default execution path. Model fitting, preprocessing, and performance estimation are carried out internally according to the declared intent and the constraints imposed by fastml.\n\n## What happens internally\n\nThe behavior described below reflects the default execution path in **fastml** and is not exposed for routine user configuration.\n\nFor each resampling split:\n\n-   training data are defined according to the resampling specification,\n-   any preprocessing steps are estimated using the training data only,\n-   models are fitted on the resulting training set,\n-   predictions are generated for the corresponding assessment set,\n-   performance metrics are computed exclusively on assessment data.\n\nAs an additional safety measure, fastml performs internal checks on the resampling structure. If a resample is detected in which the training set coincides with the full dataset, execution is halted and the run is flagged as unsafe.\n\n## Why this differs from typical workflows\n\nIn many machine learning frameworks:\n\n-   pipelines that violate evaluation assumptions can be constructed,\n-   correct evaluation depends largely on user discipline and correct assembly of components,\n-   violations such as preprocessing leakage may execute without explicit warnings.\n\nIn **fastml**:\n\n-   common classes of incorrect evaluation pipelines are restricted along the default execution path,\n-   methodological correctness is less dependent on user assembly decisions,\n-   key evaluation invariants are checked and enforced during execution.\n\nThis reflects a deliberate design trade-off: reducing flexibility in pipeline construction in order to lower the risk of undetected evaluation errors.\n\n## Inspecting results\n\nOnce execution is complete, performance estimates can be accessed directly from the fitted object.\n\n```{r}\nfit$performance$rand_forest$ranger\nfit$performance$xgboost$xgboost\n```\n\nThe reported metrics summarize performance estimates obtained under 5-fold cross-validation for each model.\n\nFor both the random forest and xgboost models, these estimates are computed on assessment data that are held out from model fitting within each resampling split. They therefore differ from training-set performance and are not derived from post-hoc adjustments or recalibration.\n\nIn this example, the random forest model attains slightly higher average performance across most metrics, including accuracy, Cohen’s kappa, F1 score, and ROC AUC. However, the differences relative to xgboost are modest, and both models display a similar balance between sensitivity and specificity. These results indicate comparable overall performance with small differences in error structure rather than a clear dominance of one model over the other.\n\nAll reported values arise directly from the resampling-based evaluation procedure used by fastml, in which preprocessing, model fitting, and performance estimation are executed within a single, resampling-aware structure. The metrics therefore represent cross-validated performance summaries under the declared evaluation setup, subject to the usual variability and limitations inherent to resampling-based estimates.\n\n## Fold-Level variability\n\n```{r}\nfit$resampling_results$`rand_forest (ranger)`$folds\nfit$resampling_results$`xgboost (xgboost)`$folds\n```\n\nPerformance estimates vary across resampling folds, across metrics, and across models.\n\nIn this example, both the random forest and xgboost models exhibit noticeable but moderate fold-to-fold variability. Accuracy, sensitivity, specificity, and agreement-based measures such as Cohen’s kappa fluctuate across folds for both models, reflecting differences in class composition and difficulty across resampling splits.\n\nAcross folds, the two models show broadly comparable behavior. The random forest model attains slightly higher values in some folds, while the xgboost model matches or closely tracks its performance in others. For both models, sensitivity and specificity remain relatively balanced across folds, and no systematic metric asymmetry or degenerate behavior is observed. Variation in kappa and accuracy remains within a range consistent with expected resampling variability rather than indicating instability.\n\nThis pattern illustrates that, even when aggregated summaries suggest similar overall performance, fold-level inspection remains necessary to assess stability, class-wise trade-offs, and the influence of individual resampling splits. Such variability is an inherent feature of resampling-based evaluation and cannot be fully characterized by a single averaged estimate.\n\n## Model comparison\n\n```{r}\nsummary(fit)\n```\n\nThe summary output compares models evaluated under an identical resampling specification.\n\nIn this example, the random forest model attains slightly higher average performance across all reported metrics, including accuracy, F1 score, Cohen’s kappa, sensitivity, specificity, and ROC AUC. The xgboost model shows closely comparable performance, with modestly lower values across the same metrics. The differences between models are small and reflect incremental variations in error rates rather than qualitatively distinct error profiles.\n\nBecause all models are evaluated using the same resampling splits, observed performance differences can be attributed to the modeling approaches rather than to variation in data partitioning. This consistency is not merely a convenience; it is a methodological requirement for meaningful model comparison under resampling-based evaluation.\n\n## What is guaranteed here\n\nSubject to the constraints accepted along the default execution path, **fastml** provides the following assurances:\n\n- preprocessing steps are estimated separately within each resampling split, preventing information flow across folds,\n- model training and evaluation are carried out on disjoint data subsets within each split,\n- all algorithms are evaluated using an identical resampling structure,\n- reported performance metrics correspond to resampling-based estimates of out-of-sample performance.\n\nThese properties arise from the architectural design of fastml rather than from user-enforced conventions. They reflect enforced evaluation invariants under standard usage, not informal best-practice recommendations.\n\n## What fastml cannot guarantee\n\nAs emphasized in the accompanying manuscript, **fastml** does not and cannot guarantee:\n\n- that outcome variables are correctly defined or scientifically meaningful,\n- that the raw features are free from prior leakage, measurement artifacts, or target contamination,\n- that the learning task itself addresses a scientifically relevant question,\n- that the chosen performance metrics are appropriate for the scientific or clinical context.\n\nMethodological safeguards can reduce certain classes of technical error, but they cannot substitute for domain expertise, sound study design, or scientific judgment.\n\n## Summary\n\nThis tutorial did not focus on constructing highly flexible modeling pipelines.\n\nInstead, it demonstrated how fastml limits common pathways to invalid evaluation by constraining how models are trained, evaluated, and compared along the default execution path.\n\nThe distinguishing characteristic of fastml is therefore not automation for its own sake, but the enforcement of evaluation invariants intended to reduce methodological errors in performance estimation.\n\n## What comes next\n\n**[02. Multiple Models and Fair Comparison](02-multiple-models-fair-comparison.qmd)**  \nWhy comparing models is statistically invalid without shared resampling.\n","srcMarkdownNoYaml":"\n\n## Before you start\n\nThis tutorial builds on the following conceptual material:\n\n-   **[C1. What is Data Leakage](C1-what-is-data-leakage.qmd)**\n-   **[C2. Why Most ML Pipelines Are Unsafe by Default](C2-why-most-ml-pipelines-are-unsafe-by-default.qmd)**\n-   **[C3. Guarded Resampling](C3-guarded-resampling.qmd)**\n-   **[C4. What fastml Deliberately Does Not Allow](C4-what-fastml-does-not-allow.qmd)**\n\nReaders are expected to be familiar with these concepts before proceeding.\n\nThe workflow demonstrated here uses a deliberately constrained interface. These constraints are intentional and reflect the design philosophy of fastml: to reduce common sources of evaluation error by limiting user-facing degrees of freedom along the default execution path.\n\n## The problem we are solving\n\nThe goal of this tutorial is to estimate the **out-of-sample performance** of a binary classifier.\n\nThe focus is not on maximizing predictive accuracy, tuning hyperparameters, or examining model internals. Instead, the objective is narrowly defined:\n\n**What level of predictive performance can reasonably be expected on new, unseen data, given a fixed modeling specification?**\n\nIn this setting, model fitting itself is straightforward. The primary challenge lies in performance evaluation.\n\nSpecifically, the difficulty is to obtain an estimate that is not biased by information leakage or other forms of contamination arising from reuse of the data during training and evaluation.\n\n## Why this is harder than it sounds\n\nFrom **C1** and **C2**, recall the following points:\n\n-   Leakage does not require obvious mistakes.\n-   Pipelines that appear valid can still yield biased performance estimates.\n-   Most machine learning frameworks allow such pipelines to run without warnings.\n\nFrom **C3**, recall:\n\n-   Correct evaluation requires resampling to *enclose* preprocessing and model fitting.\n-   This enclosure must be structural, not procedural.\n\nFrom **C4**, recall:\n\n-   fastml enforces correctness by removing degrees of freedom rather than relying on user discipline.\n\nThis tutorial demonstrates what that enforcement looks like in practice.\n\n## The data\n\nThis tutorial uses a simple binary classification dataset from the **modeldata** package.\n\nThe dataset is characterized by:\n\n- a binary outcome variable (`Class`),\n- a small number of continuous predictors (`A` and `B`),\n- the absence of missing values.\n\nThis choice is intentional. The dataset is deliberately low-dimensional and clean, so that attention can remain on the evaluation procedure rather than on feature engineering, preprocessing decisions, or data-quality issues.\n\n\n```{r}\nlibrary(modeldata)\ndata(two_class_dat)\nhead(two_class_dat)\n```\n\nThe purpose of this example is to illustrate evaluation mechanics rather than data preprocessing challenges. Although the dataset itself is simple, the evaluation principles demonstrated here extend to more complex datasets and other supported tasks in fastml, subject to their respective assumptions and constraints.\n\n## What you do not need to do in fastml\n\nBefore showing the workflow, it is important to be explicit.\n\nIn **fastml**, users are not required to:\n\n-   manually assemble train–test splits,\n-   explicitly construct preprocessing recipes,\n-   apply scaling or imputation outside the resampling loop,\n-   compose workflows from loosely coupled components,\n-   control when preprocessing is trained relative to resampling,\n-   directly manipulate resampling objects during model execution.\n\nThese steps are common entry points for data leakage.\n\nBy default, fastml executes preprocessing, model fitting, and evaluation within a single, resampling-aware structure. While advanced users may override specific components, the standard execution path is designed to preserve training–assessment isolation without relying on user discipline.\n\n## Declaring intent\n\nIn **fastml**, the user specifies *what is to be evaluated* rather than manually assembling the individual components of a modeling pipeline.\n\nAt a minimum, this specification includes:\n\n-   the dataset,\n-   the outcome variable,\n-   the set of algorithms to be evaluated,\n-   the intended resampling strategy.\n\n```{r}\nlibrary(fastml)\n\nfit <- fastml(\n  data       = two_class_dat,\n  label      = \"Class\",\n  algorithms = c(\"rand_forest\", \"xgboost\"),\n  resampling = \"cv\",\n  folds      = 5,\n)\n```\n\nThis call defines the full evaluation setup under the default execution path. Model fitting, preprocessing, and performance estimation are carried out internally according to the declared intent and the constraints imposed by fastml.\n\n## What happens internally\n\nThe behavior described below reflects the default execution path in **fastml** and is not exposed for routine user configuration.\n\nFor each resampling split:\n\n-   training data are defined according to the resampling specification,\n-   any preprocessing steps are estimated using the training data only,\n-   models are fitted on the resulting training set,\n-   predictions are generated for the corresponding assessment set,\n-   performance metrics are computed exclusively on assessment data.\n\nAs an additional safety measure, fastml performs internal checks on the resampling structure. If a resample is detected in which the training set coincides with the full dataset, execution is halted and the run is flagged as unsafe.\n\n## Why this differs from typical workflows\n\nIn many machine learning frameworks:\n\n-   pipelines that violate evaluation assumptions can be constructed,\n-   correct evaluation depends largely on user discipline and correct assembly of components,\n-   violations such as preprocessing leakage may execute without explicit warnings.\n\nIn **fastml**:\n\n-   common classes of incorrect evaluation pipelines are restricted along the default execution path,\n-   methodological correctness is less dependent on user assembly decisions,\n-   key evaluation invariants are checked and enforced during execution.\n\nThis reflects a deliberate design trade-off: reducing flexibility in pipeline construction in order to lower the risk of undetected evaluation errors.\n\n## Inspecting results\n\nOnce execution is complete, performance estimates can be accessed directly from the fitted object.\n\n```{r}\nfit$performance$rand_forest$ranger\nfit$performance$xgboost$xgboost\n```\n\nThe reported metrics summarize performance estimates obtained under 5-fold cross-validation for each model.\n\nFor both the random forest and xgboost models, these estimates are computed on assessment data that are held out from model fitting within each resampling split. They therefore differ from training-set performance and are not derived from post-hoc adjustments or recalibration.\n\nIn this example, the random forest model attains slightly higher average performance across most metrics, including accuracy, Cohen’s kappa, F1 score, and ROC AUC. However, the differences relative to xgboost are modest, and both models display a similar balance between sensitivity and specificity. These results indicate comparable overall performance with small differences in error structure rather than a clear dominance of one model over the other.\n\nAll reported values arise directly from the resampling-based evaluation procedure used by fastml, in which preprocessing, model fitting, and performance estimation are executed within a single, resampling-aware structure. The metrics therefore represent cross-validated performance summaries under the declared evaluation setup, subject to the usual variability and limitations inherent to resampling-based estimates.\n\n## Fold-Level variability\n\n```{r}\nfit$resampling_results$`rand_forest (ranger)`$folds\nfit$resampling_results$`xgboost (xgboost)`$folds\n```\n\nPerformance estimates vary across resampling folds, across metrics, and across models.\n\nIn this example, both the random forest and xgboost models exhibit noticeable but moderate fold-to-fold variability. Accuracy, sensitivity, specificity, and agreement-based measures such as Cohen’s kappa fluctuate across folds for both models, reflecting differences in class composition and difficulty across resampling splits.\n\nAcross folds, the two models show broadly comparable behavior. The random forest model attains slightly higher values in some folds, while the xgboost model matches or closely tracks its performance in others. For both models, sensitivity and specificity remain relatively balanced across folds, and no systematic metric asymmetry or degenerate behavior is observed. Variation in kappa and accuracy remains within a range consistent with expected resampling variability rather than indicating instability.\n\nThis pattern illustrates that, even when aggregated summaries suggest similar overall performance, fold-level inspection remains necessary to assess stability, class-wise trade-offs, and the influence of individual resampling splits. Such variability is an inherent feature of resampling-based evaluation and cannot be fully characterized by a single averaged estimate.\n\n## Model comparison\n\n```{r}\nsummary(fit)\n```\n\nThe summary output compares models evaluated under an identical resampling specification.\n\nIn this example, the random forest model attains slightly higher average performance across all reported metrics, including accuracy, F1 score, Cohen’s kappa, sensitivity, specificity, and ROC AUC. The xgboost model shows closely comparable performance, with modestly lower values across the same metrics. The differences between models are small and reflect incremental variations in error rates rather than qualitatively distinct error profiles.\n\nBecause all models are evaluated using the same resampling splits, observed performance differences can be attributed to the modeling approaches rather than to variation in data partitioning. This consistency is not merely a convenience; it is a methodological requirement for meaningful model comparison under resampling-based evaluation.\n\n## What is guaranteed here\n\nSubject to the constraints accepted along the default execution path, **fastml** provides the following assurances:\n\n- preprocessing steps are estimated separately within each resampling split, preventing information flow across folds,\n- model training and evaluation are carried out on disjoint data subsets within each split,\n- all algorithms are evaluated using an identical resampling structure,\n- reported performance metrics correspond to resampling-based estimates of out-of-sample performance.\n\nThese properties arise from the architectural design of fastml rather than from user-enforced conventions. They reflect enforced evaluation invariants under standard usage, not informal best-practice recommendations.\n\n## What fastml cannot guarantee\n\nAs emphasized in the accompanying manuscript, **fastml** does not and cannot guarantee:\n\n- that outcome variables are correctly defined or scientifically meaningful,\n- that the raw features are free from prior leakage, measurement artifacts, or target contamination,\n- that the learning task itself addresses a scientifically relevant question,\n- that the chosen performance metrics are appropriate for the scientific or clinical context.\n\nMethodological safeguards can reduce certain classes of technical error, but they cannot substitute for domain expertise, sound study design, or scientific judgment.\n\n## Summary\n\nThis tutorial did not focus on constructing highly flexible modeling pipelines.\n\nInstead, it demonstrated how fastml limits common pathways to invalid evaluation by constraining how models are trained, evaluated, and compared along the default execution path.\n\nThe distinguishing characteristic of fastml is therefore not automation for its own sake, but the enforcement of evaluation invariants intended to reduce methodological errors in performance estimation.\n\n## What comes next\n\n**[02. Multiple Models and Fair Comparison](02-multiple-models-fair-comparison.qmd)**  \nWhy comparing models is statistically invalid without shared resampling.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"01-basic-classification.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","theme":"cosmo","title":"01. Basic Classification"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}