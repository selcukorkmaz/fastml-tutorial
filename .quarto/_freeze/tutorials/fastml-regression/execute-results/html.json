{
  "hash": "16cea4f3f0044f7d3288ea61f0600256",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting Continuous Outcomes\"\nformat: html\neditor: visual\n---\n\n\nIn this tutorial, we’ll use `fastml` to tackle a regression problem—specifically, predicting house sale prices. Just like in classification, the `fastml()` function streamlines the entire modeling workflow for continuous targets with minimal setup.\n\nYou’ll see how it:\n\n- Automatically handles data splitting and preprocessing,\n\n- Applies appropriate resampling methods (like cross-validation),\n\n- Tunes hyperparameters across multiple regression algorithms,\n\n- Compares model performance using metrics such as RMSE and R².\n\nWhether you’re forecasting prices, estimating risk scores, or predicting any numeric outcome, `fastml()` makes regression modeling just as effortless and reproducible as classification. Let’s walk through a real-world example step by step.\n\n\n------------------------------------------------------------------------\n\n## 1. Load Packages and Data\n\nIn this regression example, we’ll use a medical dataset to predict a continuous outcome—specifically, Body Mass Index (BMI) based on various clinical features. This simulates a common scenario in healthcare: estimating a patient's health metric using routine measurements.\n\nWe’ll use the Pima Indians Diabetes dataset, available via the `mlbench` package. It contains health data from adult female patients of Pima Indian heritage.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fastml)\nlibrary(dplyr)\nlibrary(mlbench)\n\ndata(PimaIndiansDiabetes)\n\n# Prepare dataset: rename 'mass' to 'BMI' and drop 'diabetes'\npima_reg <- PimaIndiansDiabetes %>%\n  rename(BMI = mass) %>%\n  select(-diabetes) %>%\n  filter(BMI > 0)  # remove invalid BMI values (e.g., 0)\n\nhead(pima_reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  pregnant glucose pressure triceps insulin  BMI pedigree age\n1        6     148       72      35       0 33.6    0.627  50\n2        1      85       66      29       0 26.6    0.351  31\n3        8     183       64       0       0 23.3    0.672  32\n4        1      89       66      23      94 28.1    0.167  21\n5        0     137       40      35     168 43.1    2.288  33\n6        5     116       74       0       0 25.6    0.201  30\n```\n\n\n:::\n:::\n\n\nThe resulting dataset includes 768 rows and 8 predictor variables:\n\n- **pregnant**: Number of times pregnant,\n\n- **glucose**: Plasma glucose concentration,\n\n- **pressure**:\tDiastolic blood pressure (mm Hg),\n\n- **triceps**: Triceps skin fold thickness (mm),\n\n- **insulin**: 2-hour serum insulin (μU/mL),\n\n- **BMI**: Body mass index (target variable here),\n\n- **pedigree**: Diabetes pedigree function\n\n- **age**: Age in years\n\n\nIn the next step, we'll explore the dataset and launch a full regression workflow using fastml().\n\n\n\n------------------------------------------------------------------------\n\n## 2. Train Several Regression Models\n\nWith the dataset prepared, we can now use `fastml()` to train and evaluate multiple regression models in a single step. Just like in classification, the function takes care of splitting the data, preprocessing, resampling, and hyperparameter tuning.\n\nIn this example, we'll train four common regression algorithms to predict BMI:\n\n- Linear regression (`linear_reg`)\n\n- Support Vector Machine with radial kernel (`svm_rbf`)\n\n- Random Forest (`rand_forest`)\n\n- LightGBM (`lightgbm`)\n\nLet’s run the full pipeline:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult <- fastml(\n  data       = pima_reg,\n  label      = \"BMI\",\n  algorithms = c(\"linear_reg\", \"svm_rbf\", \"rand_forest\", \"lightgbm\")\n)\n```\n:::\n\n\n\n**What happens under the hood?**\n\n1.  **Recipe**: median‑imputes NAs, one‑hot‑encodes categoricals, centres & scales numerics.\n2.  **Resampling**: 10‑fold CV within the train split.\n3.  **Tuning grids**: automatically generated per algorithm.\n4.  **Finalisation**: the best hyper‑parameters are selected and the workflow refit on the full training data.\n\nYou don’t need to worry about creating resampling objects or preprocessing steps manually—everything is handled internally, while remaining fully customizable via optional arguments.\n\nIn the next section, we'll examine and compare the performance of the trained regression models using summary metrics like RMSE and R-squared.\n\n------------------------------------------------------------------------\n\n## 3. Compare Model Performance\n\nAfter training, you can evaluate how each model performed using the summary() function. This provides a comprehensive overview of model performance based on regression metrics such as:\n\n- **RMSE** (Root Mean Squared Error): Measures average prediction error magnitude.\n\n- *R-squared* (Coefficient of Determination): Indicates how well the model explains variability in the outcome.\n\n- **MAE** (Mean Absolute Error): Average absolute difference between predicted and actual values.\n\nTo view the results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(result, type = \"metrics\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n===== fastml Model Summary =====\nTask: regression \nNumber of Models Trained: 4 \nBest Model(s): lightgbm (lightgbm) (rmse: 5.5434100) \n\nPerformance Metrics (Sorted by rmse ):\n\n------------------------------------------------------ \nModel        Engine    RMSE       R-squared  MAE       \n------------------------------------------------------ \nlightgbm*    lightgbm  5.5434100  0.4006765  4.0925296 \nrand_forest  ranger    5.7083442  0.3679268  4.2930432 \nsvm_rbf      kernlab   5.8889871  0.3302711  4.3937137 \nlinear_reg   lm        6.5682857  0.1621334  5.1909364 \n------------------------------------------------------ \n(*Best model)\n```\n\n\n:::\n:::\n\nFrom this output, we see that the lightgbm model achieved the lowest RMSE, indicating the most accurate predictions overall. It also has the highest R-squared, suggesting it explains a greater portion of variance in BMI compared to other models.\n\nYou can also plot the performance of all models across metrics using:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(result, type = \"bar\")\n```\n\n::: {.cell-output-display}\n![](fastml-regression_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\nThis generates a faceted bar plot, making it easy to compare RMSE, R-squared, and MAE visually.\n\nIn the next step, we’ll take a closer look at the best model's tuned hyperparameters.\n\n\n------------------------------------------------------------------------\n\n## 4. Inspect the Best Model\n\nOnce the models have been trained and evaluated, `fastml()` automatically identifies the best-performing model based on the optimization metric you specified (e.g., RMSE). You can inspect which model was selected and view its internal details using:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult$best_model_name\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  lightgbm \n\"lightgbm\" \n```\n\n\n:::\n:::\n\n\nThis tells us that the best-performing model is a LightGBM gradient boosting machine.\n\nTo view the trained model’s workflow—including the preprocessing steps, model specification, and tuning results—use:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult$best_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$`lightgbm (lightgbm)`\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_zv()\n• step_dummy()\n• step_center()\n• step_scale()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLightGBM Model (100 trees)\nObjective: regression\nFitted to dataset with 7 columns\n```\n\n\n:::\n:::\n\n\nThis reveals:\n\n- The full tidymodels workflow, including the recipe and the fitted model.\n\n- The model engine and its finalized hyperparameters.\n\n- A summary of the training process and resampling results.\n\nThis confirms that:\n\n- Preprocessing included zero-variance filtering, dummy encoding (if applicable), centering, and scaling.\n\n- The model is a boosted tree trained with LightGBM, using 100 trees.\n\nYou can also retrieve the exact hyperparameters selected during tuning using:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(result, type = \"params\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBest Model hyperparameters:\n\nModel: lightgbm (lightgbm) \n  mtry: 2\n  trees: 100\n  min_n: 5\n  tree_depth: 3\n  learn_rate: 0.1\n  loss_reduction: 1\n  sample_size: 0.5\n```\n\n\n:::\n:::\n\n\nThis level of detail is helpful for understanding model complexity, reproducibility, and for future deployment.\n\nIn the next section, we’ll use the selected model to make predictions on new, unseen data.\n\n## 5. Predict on New Observations\n\nOnce you've identified and reviewed the best model, you can use it to make predictions on new, unseen data. \n\nStart by sampling a few observations from the dataset (or use an external dataset if available):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample 5 new observations\nnew_obs <- pima_reg %>% \n  slice_sample(n = 5) %>% \n  dplyr::select(-BMI) \n\nnew_obs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    pregnant glucose pressure triceps insulin pedigree age\n430        1      95       82      25     180    0.233  43\n76         1       0       48      20       0    0.140  22\n714        0     134       58      20     291    0.352  21\n389        5     144       82      26     285    0.452  58\n646        2     157       74      35     440    0.134  30\n```\n\n\n:::\n:::\n\n\nUse the predict() function to generate predicted BMI values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(result, new_obs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 22.82529 22.82529 22.82529 22.82529 22.82529\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## 6. Variable Importance & SHAP Values\n\nIn medical and clinical applications, understanding how a model arrives at its prediction is often just as crucial as the prediction itself. For this reason, `fastml` provides SHAP (Shapley Additive Explanations) support via the `fastexplain()` function—allowing you to interpret individual-level predictions from your best model.\n\nTo compute and visualize SHAP values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfastexplain(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPreparation of a new explainer is initiated\n  -> model label       :  lightgbm \n  -> data              :  605  rows  7  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  605  values \n  -> predict function  :  predict_function \n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package , ver. , task regression \n  -> predicted values  :  numerical, min =  21.50534 , mean =  32.58835 , max =  50.39042  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  -13.91933 , mean =  0.002388819 , max =  15.90049  \n  A new explainer has been created!  \n\n=== DALEX Variable Importance (with Boxplots) ===\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](fastml-regression_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== DALEX Shapley Values (SHAP) ===\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](fastml-regression_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n:::\n\n\nThis produces two key plots:\n\n**Feature Importance (Permutation-Based)**\n\n- This plot shows how much the model’s RMSE increases when each feature is permuted (i.e., randomly shuffled).\n\n- The larger the increase in error, the more important the feature is to the model's performance.\n\n- Triceps skinfold thickness is by far the most important feature, followed by glucose, blood pressure, and age.\n\n**SHAP Values (Local Explainability)**\n\n- SHAP values break down each individual prediction into feature-level contributions.\n\n- Positive SHAP values increase the predicted BMI; negative ones decrease it.\n\n- In this example:\n\n  - Triceps and pressure have strong positive contributions.\n\n  - Pregnancy count and age tend to lower BMI predictions for certain individuals.\n\n  - Glucose and pedigree show more mixed or subtle effects.\n  \n**Interpretation Highlights**\n\n- Triceps consistently emerges as the most influential variable, both globally and locally.\n\n- Some variables (like glucose) may be important for prediction accuracy (as seen in the permutation plot) but may not contribute uniformly across all patients (as reflected in SHAP values).\n\n- SHAP plots also help identify non-linear or interaction effects—for example, how pregnant affects predictions differently depending on other inputs.\n\n------------------------------------------------------------------------\n\n\n\n\n",
    "supporting": [
      "fastml-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}